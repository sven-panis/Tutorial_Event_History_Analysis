---
title: "manuscript_notes"
author: "Rich"
date: "2024-08-06"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file is a way to provide feedback on draft versions of the manuscript. 
I can edit the manuscript directly of course and I have done in a few places (which git should track), but this is a space to provide comments.

I'm not sure of this is the best way to go. But so be it. 

At the end we can add this folder to .gitignore, so we don't need to post this online.


# Aug 6th 2024 #

## Abstract ##

You say: To move beyond mean performance measures, various distributional analyses have been proposed.

This makes complete sense. But I would say it differently. I would emphasise in a sentence or two why mean performance measures are limited, and thus highlight the benefit of non-mean measures. 

And maybe try to find simpler language for distributional measures. Again, it is completely precise and accurate, but I worry that most folks will read it and think "that aint anything I care about" when in actual fact it might be extremely relevant. 

So, in other words, I think the abstract needs a clearer hook for the generalist experimental psych reader. Something like ... mean avg measures (across trials) hide/obscure a lot of information about psychological processes, such as when psychological states occur in time. In contrast, blah blah approaches reveal how psych processes occur in specific time windows (etc. etc.). And this changes key conclusions about psych processes. 

Here we provide a tutorial on how to do this [[and then return to the details of the tutorial as you already state]]. Even though I have written a lot here, I expect the changes to the abstract to be small. I just wanted to try to make my point clear here to you. I hope that makes sense.


## Introduction ##

I would slightly re-structure the introduction. I would use some subsections in the intro maybe also to organise ideas. 

The main change that I propose is to have a first section that provides the motivation in simple terms (non-modelling terms), 

as well as a roadmap for what is to come next in the rest of the paper, so that folks know what to expect. This is especially important for reviews/tutorials etc., since it doesn't follow the standard empirical paper approach i.e., intro, methods, results ... 

Then I would move all of the modelling stuff that is currently in the intro into an entirely new section. Here are some ideas on structure with example sections numbered...

1. motivation = means vs distribution/timewindow approach. Benefits etc. 

I would have a plot in this part of the intro. Just real or simulated raw data - like we discussed in our meeting on Monday for your future talk - bar plots of means between conditions juxtaposed with a distributional analysis across time. 

I think the visually hits home the basic point about how timing is important. Then all the modelling and lifetable stuff  that comes later is important as a method, but we get in there nice and early with a clear message on why it is so ubiquitous in exp psych (e.g., almost everyone plots mean RT, for example). So we make the benefit/value really clear, really early. 

2. Roadmap of the paper, which outlines what to expect. e.g., We first provide a brief overview of hazard models to orient the reader to the basic concepts and ideas that we will use through the paper. However, this will remain relatively short, and for detailed treatment, see blah blah REFs. 

And we *must* provide some figures here. Figures really help with stats stuff, because it brings more people in. Can you think of some figures to illustrate key modelling concepts? Or do you think the mean vs distribution figure above in point (1) will be enough? 

We then provide 4 different tutorials, each of which is written is R code and publicly available. The tutorials provide hands-on, concrete examples of key parts of the analytical process, so that others can also analyse time to event data.... 
Tutorial 1 blah. 2 blah. 3 blah. 4 blah. Provide a sentence or two on each tutorial. 


## Background on EHA analyses ##

Here is where you move the modelling stuff into a separate section. 
This is where you get into the more technical stuff.
keep this short and refer to other, more detailed reviews (including your own papers).
The reason for keeping this section short is that we want to focus on a worked examples of how to do the analyses, rather than justify why you should use these techniques. Loads of papers have focussed on why before. And let's say this explicitly at the start of this section, so that if anyone complains that we need more background info., we have already explained why we kept it short.


## Tutorial 1 ##

## Tutorial 2 ##


...



# September 2024 #

I received your draft and made a bunch of edits and the comments are below.
I added my edits to git and made a commit, then pushed them back.

## general stuff ##

### functions ###

maybe a function directory in supps?
Each of your user-defined functions can be listed with a description of what it does. Like in an R package?

-> Our custom functions are now located in the file custom_functions.R.
The functions are also listed in the supplementary material.

### references ###

I know why you have the issue with Bengtsson 2021 reference. For some reason, it is simply in your .bib file more than once. So there is some typo/bug that needs fixing so that the ref is only listed once.

At least most of the the code works fine, it is just your .bib file that needs fixing.

-> The .bib file is generated automatically. I have solved the problem by adding the argument "append=F" to the function r_refs().

More generally, I'm not sure whether it is better to have the R package REFs in a footnote or in the main text as a paragraph. We can decide later on what works best.

-> ok.

### terms ###

we use the terms hazard and EHA interchangeably. We should pick one, define it and then stick to it. 
For example, the first line of the section "overview of hazard analysis" starts with "To apply event history analysis"

-> I agree, but we are also discussing SAT analyses. I had a reviewer once who insisted that SAT is different from EHA. So we could talk about EHA/SAT analyses perhaps?

### formulas ###

I think we should go "formula free" in the main text, and instead report formulas in supplementary materials. The reason is as I now make clear at the start of sectino 2:

>For a comprehensive background context to hazard analysis, we recommend several excellent textbooks (REFs). Likewise, for general introduction to understanding regression equations, we recommend several introductory level textbooks (REFs). Our focus here is not on providing a detailed account of the underlying regression equations, since this has been done many times before. Instead, we want to provide an intuition to how EHA works in general and in the context of experimental psychology. As such, we supply relevant equations in supplementary materials and refer to them in the text whenever relevant.

I would also add as a general commwent that whenever we do insert formulas in text (in the future or in supplementary material here) we should number them and we should clearly define every part of the equation. 

-> all equations and formulas are moved to the supplementary material. Equations are numbered.

### big computational chunks in the manuscript ###

I know one of the benefits of using Rmarkdown and papaja is that you can have seamless integration of code chunks and prose. However, I also value some efficiency and simplicity. So, for me, if a code chunk is simple and short e.g., simulate some easy data and plot, then I can see why it is nice to have it in. But when the code chunks get really big like the 'load-functions' chunk around line 256, I don't think it works as nicely. The reason I say this is at least twofold. 1) since we are providing the tutorials as separate .Rmd files and .html files, then the underlying code chunks and functions are of course available to everyone. So the main text can be the front-end window dressing to get folks to want to jump into and use the code. That means, we can just insert images from the tutorial folders, rather than have massive code chunks, which then make the manuscript.Rmd file unnecessarily long. 2) the second reason is that unless we use 'cache' function or eval=F, then it slows down the knit process considerably as a whole bunch of computations have to be performed every time we generate the pdf. And as a process of editing and revision, I like to be able to see how my edits change the output and I like to read the pdf as a doc to see how it flows. So, for that, I'd like to keep the manuscript file lightweight and easy to render. That means we keep all code chunks that actually compute stuff in the tutorials and then mainly just refer to .jpegs in the main text.

Does that make sense?

-> Yes, the big code chunks are removed.

### tables ###

tables 1 and 2, which I added, would be better and easier to read if they were either placed side-by-side or incorporated into one table. I'll leave that for you to figure out. You're welcome!!

-> Still to do... It does not seem straightforward to place apa tables side by side. For now, I kept them separate and placed text between them.  

### summary of the steps involved ###

for each tutorial, I think it would be nice to have a summary of the steps involved, as a roadmap.
e.g., a list of the steps that need to be taken or a flowchart. I think it would be nice to see which chunky bits of stuff need to happen. Just an idea to ease the burden of information and give people a guide.

-> I added a toc for each tutorial.

## abstract ##

I need to shorten it.

## Tutorial 1 ##

no need to cite research gate, just cite the paper and put the data on our GitHub project / OSF, so that it is available.

-> ok.

### figure 4 - h(t), S(t), ca(t) ###

the figure that shows all three panels.

-> I added the probability mass function P(t) for each condition.

Can we add shading that highlights particular time-windows that we refer to in the text?
I think that would help.

And as we disucssed before, we need to be mindful that folks know exactly what eaxh panels add on its own and/or in combination with the other panels. 

-> I think shading is not necessary.

### unpack some statements a little more ###

e.g., you say

>Qualitatively similar results were obtained for the other five participants. These results go against the (often implicit) assumption that all observed responses are primed responses to the target stimulus.

Extend this paragraph to make it crystal clear what we mean. Can we unpack the theoretical relevance and bring it back into how EHA is revealing?

-> I added the following text:
These results go against the (often implicit) assumption that all observed responses are primed responses to the target stimulus. Instead, the distributional data show that early responses are triggered exclusively by the prime stimulus, while only later responses reflect primed responses to the target stimulus.

And why don't we show the group average data, rather than just PID 6? It seems odd to me without an explanation/justification. 

-> I added the following text:
It is important to visually inspect the functions first for each participant, in order to identify possible cheaters (e.g., a flat conditional accuracy function at .5 indicates (s)he was only guessing), outlying individuals, and/or different groups with qualitatively different behavior. 

When participants show qualitatively the same distributional patterns, on might consider to aggregate their data and make one plot (see Tutorial_1a.Rmd).

### I removed the following text from this section ###

>Also, in their second Experiment, @panisWhatShapingRT2016 showed that the negative compatibility effect in the mask-present conditions (see Tutorial 4) is time-locked to mask onset. This example shows that a simple difference between two means fails to reveal the dynamic behavior people display in many experimental paradigms [@panisHowCanWe2020; @panisNeuropsychologicalEvidenceTemporal2017; @panisStudyingDynamicsVisual2020; @panisTimecourseContingenciesPerceptual2009; @panisWhenDoesInhibition2022; @schmidtResponseInhibitionNegative2022]. In other words, statistically controlling for the passage of time during data analysis is equally important as experimental control during the design of an experiment, to better understand human behavior in experimental paradigms. 

This is really interesting, but it doesn't seem to fit in this section. In this section, we just want to tell people how to wrangle and look at the data.

-> ok.

Maybe in a later section we can add in this text to explain the significance or implications.

-> I added the following text to the conclusion:
Statistically controlling for the passage of time during data analysis is thus equally important as experimental control during the design of an experiment, to better understand human behavior in experimental paradigms.

It also read a little bit like we were making inferential claims from descriptive stats, so it shoudl wait until we get to a later tutorial maybe.

-> I agree.

### link function plot ###

this can go in supps, as we are not writing a regression textbook.

-> Done.

### this losing people and we need a way to simplify ###

You say this...

>An example (single-level) discrete-time hazard model with three predictors (TIME, X~1~, X~2~), the cloglog link function, and a third-order polynomial specification for TIME can be written as follows:

>cloglog[h(t)] = ln(-ln[1-h(t)]) =  [$\alpha$~1~ONE + $\alpha$~2~(TIME-9) + $\alpha$~3~(TIME-9)$^2$] +                                    [$\beta$~1~X~1~ + $\beta$~2~X~2~ + $\beta$~3~X~2~(TIME-9)].

>The main predictor variable TIME is the time bin index t that is centered on value 9 in this example. The first set of terms within brackets, the alpha parameters multiplied by their polynomial specifications of (centered) time, represents the shape of the baseline cloglog-hazard function (i.e., when all predictors X~i~ take on a value of zero). The second set of terms (the beta parameters) represents the vertical shift in the baseline cloglog-hazard for a 1 unit increase in the respective predictor variable. Predictors can be discrete, continuous, and time-varying or time-invariant. For example, the effect of a 1 unit increase in X~1~ is to vertically shift the whole baseline cloglog-hazard function by $\beta$~1~ cloglog-hazard units. However, if the predictor interacts linearly with TIME (see X~2~ in the example), then the effect of a 1 unit increase in X~2~ is to vertically shift the predicted cloglog-hazard in bin 9 by $\beta$~2~ cloglog-hazard units (when TIME-9 = 0), in bin 10 by $\beta$~2~ + $\beta$~3~ cloglog-hazard units (when TIME-9 = 1), and so forth. To interpret the effects of a predictor,its $\beta$ parameter is exponentiated, resulting in a hazard ratio (due to the use of the cloglog link). When using the logit link, exponentiating a $\beta$ parameter results in an odds ratio.

>An example (single-level) discrete-time hazard model with a general specification for TIME (separate intercepts for each of six bins, where D1 to D6 are binary variables identifying each bin) and a single predictor (X~1~) can be written as follows:

>cloglog[h(t)] = ln(-ln[1-h(t)]) =  [$\alpha$~1~D1 + $\alpha$~2~D2 + $\alpha$~3~D3 + $\alpha$~4~D4 + $\alpha$~5~D5 + $\alpha$~6~D6] + [$\beta$~1~X~1~].

But we need to bring people along, as that is our stated aim here, so this needs to go in supps. Then we can keep the main text accessible, without compromising on the underlying regression equations. But these have all been stated in introductory texts so it should not be front and centre for us. But it should be there for the interested reader and hence why supps is perfects.

-> The example regression equations are moved to the supplementary material.

I also suggest we discuss how to interpret the results when we get to the results.

-> Done.

### prior plot ###

again, move this plot to supps. It is a beautiful plot and hard-won by you to get this right. But move it to supps and let people get on with their lives! If they want to see it, then they can. But we need to keep the flow and momentum in the main text.

-> Done.

>To gain a sense of what prior *logit* values would approximate a uniform distribution on the probability scale, @kurzAppliedLongitudinalDataAnalysis2023 simulated a large number of draws from the Uniform(0,1) distribution, converted those draws to the log-odds metric, and fitted a Student's t distribution. Row C in Figure 4 shows that using a t-distribution with 7.61 degrees of freedom and a scale parameter of 1.57 as a prior on the logit scale, approximates a uniform distribution on the probability scale. According to @kurzAppliedLongitudinalDataAnalysis2023, such a prior might be a good prior for the intercept(s) in a logit-hazard model, while the N(0,1) prior in row D might be a good prior for the non-intercept parameters in a logit-hazard model, as it gently regularizes p towards .5 (i.e., a zero effect on the logit scale).

>To gain a sense of what prior *cloglog* values would approximate a uniform distribution on the hazard probability scale, we followed Kurz's approach and simulated a large number of draws from the Uniform(0,1) distribution, converted them to the cloglog metric, and fitted a skew-normal model (due to the asymmetry of the cloglog link function). Row E shows that using a skew-normal distribution with a mean of -0.59, a standard deviation of 1.26, and a skewness of -4.22 as a prior on the cloglog scale, approximates a uniform distribution on the probability scale. 
However, because hazard values below .5 are more likely in RT studies, using a skew-normal distribution with a mean of -1, a standard deviation of 1, and a skewness of -2 as a prior on the cloglog scale (row F), might be a good weakly informative prior for the intercept(s) in a cloglog-hazard model. 
A skew-normal distribution with a mean of -0.2, a standard deviation of 0.71, and a skewness of -2.2 might be a good weakly informative prior for the non-intercept parameters in a cloglog-hazard model as it gently regularizes p towards .6321 (i.e., a zero effect on the cloglog scale). 


### no need to output code for priors and such like ###

people are not learning anything by you showing them the code that lists priors. Instead, give them an intuition about the general approach and then they can see the implementation in the raw tutorial code. e.g., explain the justification and approach to priors in simple, overview terms, and then let them read the code if they want to know how to set them.

-> I removed the code that lists priors. I kept the model code for M1, and the model formulas for models M2 to M4.

### avoid repetition in the model building steps ###

for model 1 in Tutorial 2 - the first inferential model - we should outline the key steps. e.g., read in the data, set priors, build the model, evaluate the model via model comparison and parameter estimates. But, as I said in the previous subsection, there is no need to show code chunks for most fo these steps. Instead, I would focus on the general steps and procedures and let the tutorial .Rmd and .html files present the code. 

Then, as we move to Model 2, we only mention what differs between models 1 and 2. And the same for the rest. This keeps it short. 

-> I agree.

And when we move on to the frequentist models with lme4, we should only say something like... the general process is similar, except there are no priors to set.

-> Done.

Again, in the main text, we do not want to drowned people in implementation detail when we have a set of tutorials to do exactly that.


### model building sections ###

rather than output the code, which I've removed for the majority of instances, give some intuition on why we would want to specify formulas and models in this way. 

-> Done.

e.g., why have time in a certain form and why build from model 1 through to 4. When you add or change something, say why that might be useful to explore. But don't make it overly complicated! 

e.g., if time effects are not linear, which seems likely, then one may want to try different parameterisations of time...

### model comparison ###

you say:
>Clearly, both weighting schemes prefer model M4.

How is this clear? I don't think there are any plots or tables in the main text.

-> I added the output of the model_weights() functions to section 4.3.7. 

### parameter plots ###

let's have a paragraph on how we might interpret these plots.
Can we have a table with summary data and quantiles, just to show how they might be interpreted and reported?

-> I added that table in Tutorials 2a and 2b.

### frequentist versus Bayesian plot ###

I like the idea of comparing. That's nice. But let's improve the figure and include intervals as well as points.

-> Done.


## Tutorial 4 ##

>Note the negative compatibility effect in the hazard and conditional accuracy functions when a (relevant, irrelevant, or lines) mask is present.

This needs explaining or removing.

-> I added the following text:
In the no-mask condition (column 1 in Figure 5), we observe a positive compatibility effect in the hazard and ca(t) functions, as congruent primes temporarily generate higher values for hazard and conditional accuracy compared to incongruent primes. However, when a (relevant, irrelevant, or lines) mask is present (columns 2-4), there is a negative compatibility effect in the hazard and conditional accuracy functions, as congruent primes temporarily generate *lower* values for hazard and conditional accuracy compared to incongruent primes.

## Disucssion ##

[[This para needs to be way shorter and easier to read or we get rid of it]]

>Second, because RT distributions may differ from one another in multiple ways, @townsendTruthConsequencesOrdinal1990 developed a dominance hierarchy of statistical differences between two arbitrary distributions A and B. For example, if F~A~(t) > F~B~(t) for all t, then both cumulative distribution functions are said to show a complete ordering. Townsend (1990) showed that a complete ordering on the hazard functions —$\lambda$~A~(t) > $\lambda$~B~(t) for all t— implies a complete ordering on both the cumulative distribution and survivor functions —F~A~(t) > F~B~(t) and S~A~(t) < S~B~(t)— which in turn implies an ordering on the mean latencies —mean A < mean B. In contrast, an ordering on two means does *not* imply a complete ordering on the corresponding F(t) and S(t) functions, and a complete ordering on these latter functions does *not* imply a complete ordering on the corresponding hazard functions. This means that stronger conclusions can be drawn from data when comparing the hazard functions using EHA. For example, when mean A < mean B, the hazard functions might show a complete ordering (i.e., for all t), a partial ordering (e.g., only for t > 300 ms, or only for t < 500 ms), or they may cross each other one or more times.
As a result, instead of using delta-plots for RT -- differences in quantiles from F(t)$^-1$ -- one can simply plot delta-h(t) functions [see @panisHowCanWe2020].

-> I simplified the text to this:
Second, because RT distributions may differ from one another in multiple ways, @townsendTruthConsequencesOrdinal1990 developed a dominance hierarchy of statistical differences between two arbitrary distributions A and B. For example, if h~A~(t) > h~B~(t) for all t, then both hazard functions are said to show a complete ordering. Townsend (1990) concluded that stronger conclusions can be drawn from data when comparing the hazard functions using EHA. For example, when mean A < mean B, the hazard functions might show a complete ordering (i.e., for all t), a partial ordering (e.g., only for t > 300 ms, or only for t < 500 ms), or they may cross each other one or more times.


# 23 oktober 2024

Still to do:
- combine tables showing person-trial and person-trial-period data sets
- REF for time-to-death (line 127 in ms.) : Done
- REF for reviews and tutorials on EHA (line 140) : Done
- REF sampling bias (line 332): Instead of adding a REF, I rephrased the sentence.
- REF for package MASS (line 665) : Done
- REF for prevalence (line 710) : Done
-frequentist versus Bayesian plot with intervals: Done
- summary of steps involved in tutorials: Done.
- check odds ratio rounding : Done 
(added to apa_table(): format.args = list(
       digits = c(0, 2,2,2,2,2,5),
        decimal.mark = ".", big.mark = ""))


# October 29, 2024 #

## minor comments ##

- I removed the repetition of the title on the intro page. This always annoys me!

-> great!

- Minor point. Response time vs reaction time? Line 1 of the intro, we say response. 
But don't we want to say reaction time, as most people use that term?

-> I replaced each reaction with response, as reaction is often used for detection tasks only.

- Makeham reference. Shouldn't it just be Makeham, 1860, rather than first and middle names?

-> Yes, I will check the bib file... correct version is in extrareferences.bib (backup for extrareferences2.bib)

- I would explicitly insert a reference in the main text for papaja(). See the citation subsection of this webpage: https://github.com/crsh/papaja. And see page 6 in the manuscript for the text that I added. I feel it is worth highlighting / advertising since the developers can get more recognition and maybe others will start using it too.

-> ref papaja is in extrareferences.bib

- I would remove reference to specific equations throughout the manuscript. Instead, I would just say it once, like you do at the start of section 2. e.g., we only supply regression equations in supps. then don't mention it again because it disrupts the flow in my view. Folks who want more can easily flick to the supps and read all they want.

-> removed equation numbers in sections 2 and 2..

- section 2.1. we use an abbreviation for EHA. and we also define it as hazard. Since we have already given the other terms for EHA, let's just use EHA. I hope that makes sense. And shouldn't we abbreviate to EHA the first time we mention it, rather than in section 2.1?

-> yes, done.

- section 2.1. You say...

>Continuous RT data is treated here as interval-censored data.

But in the context of this paragraph, this makes no sense and just seems tacked on and disconnected. Can we explain or remove?

-> I removed it, and added it with explanation in the supplementary material - part A.


- Also in section 2.1, you say in the middle of a paragraph...

>This conditionality in the definition of hazard is what makes the hazard function so diagnostic for studying event occurrence, as an event can physically not occur when it has already occurred before.

Two things here. 1) At this stage in proceedings, I'm not even sure we need to give the benefits of using a hazard, but instead, just give the definition. So we might just remove it.
2) If we do include it, then this sentence does not communicate how or why it is more diagnostic, just that it is more diagnostic. And we provide no support for this claim. Later on in the discussion (I think) there is more detail regarding the greater diagnosticity. But here, we get caught between two posts and don't provide sufficient detail. So, I would just remove it from here for now, to make things shorter and simpler, at least for the time being.

-> Deleted and moved to supplementary material.


- Also in section 2.1, you say ...

>As explained in part A of the supplementary material, the survivor function provides a context for interpreting the hazard function.

Again, I'm not sure we can leave people hanging like this. Instead, I would maybe wrap some of this contextualisation into the figure legend of Figure 2. And delete this sentence. Or include 1 or 2 sentences that explain how the survivor can help contextualise the hazard, but make it clear and relevant.

[[As I went back to the manuscript, I saw an old paragrpah and re-inlcuded it, as I think it is relevant. e.g., if we make the hazard and survivor functions a big deal by inlcuding them in a figure, then we really should explain what to make of them and why they are important.]]

-> Sentence deleted, but wrapped with old paragraph in figure legend of Figure 2.



- section 4.1.2. I don't understand why it is -187ms. This needs explaining because it is not obvious.

-> The prime is presented *before* the target. I removed the minus and made it explicit.


- I really like it when you mention the use of "our" custom functions on page 19. That seems exactly what we should be doing. i.e., present a workflow and functions that make life easier for others to do it.

-> great.



- what does the following mean?

>duplicate terms in the model formula are ignored

This should be clearly explained or removed. 

-> changed to: Note that duplicate terms across the interaction terms in the model formula (e.g., condition) are ignored. 




- I like the walkthrough of the parameter estimates in section 4.3.8. That's nice and clear. 

-> ok.



- can we say why we have the following sentence?

>For illustration purposes, we only fitted the effects from model M3 (see Tutorial 2a) using the function glmer() from the R package lme4.

Were they too complex to fit or something else?

-> I expected problems with fitting the very complex model M4. 
I want to keep this tutorial short.


## regressions references ##

Gelman, A., Hill, J., & Vehtari, A. (2021). Regression and Other Stories. Cambridge University Press.

Winter, B. (2019). Statistics for Linguists: An Introduction Using R. Routledge.

-> done.

## table 1 and table 2 ##

Personally, I think having Tables 1 and 2 separately is fine. Let's leave it for now.

-> ok.

## figure 1 ##

I changed the layout of figure 1 as I thought it was easier to see.
I also changed the way RT data were generated to a uniform distribution.

-> ok.

## figure 3 ##

I tried to insert a little more explanation as to why the joint consideration of hazard plus ca is psychologically interesting.
One problem I had, however, is that if I calculate mean average RT from these hazards, then there is no difference in RT. I guess if we want to visually illustrate the differences it might be nice to have an insert plot showing the typical interferece measure of mean RT and then the distribution plots.
However, it might also be better just to keep this super simple and make the point that we do at the moment, which is that both together help guide inferences. And let's worry less about plotting mean RT in this figure.

-> I prefer the last option. I created a new figure 3 and added info to its caption.


## life tables ##

section 4.1.2
We mention life tables, but have we defined them before this point? If not, we need to define them first. And mention why they are important. If not, they just come out of nowhere.
indeed, maybe the below quoted text should come earlier to explain life tables

>Table 3 shows the life table for condition "blank" (no prime stimulus presented) for participant 6. A life table includes for each time bin, the risk set (i.e., the number of trials that are event-free at the start of the bin), the number of observed events, and the estimates of h(t), S(t), P(t), possibly ca(t), and their estimated standard errors (se). At time point zero, no events can occur and therefore h(t) and ca(t) are undefined.

-> Done.  

## figure 5 ##

Move this to the Tutorial. It is way too complicated for an already long paper.
Folks who are interested can look at the tutorial.

-> Done.


## code can be more compact ##

e.g., this 

```{r}
# load person-trial-bin oriented data set
ptb_data <- read_csv("../Tutorial_1_descriptive_stats/data/inputfile_hazard_modeling.csv")
# select analysis time window: (200,600] with 10 bins (time bin ranks 6 to 15)
ptb_data <- ptb_data %>% filter(period > 5)
# create factor condition, with "blank" as the reference level
ptb_data <- ptb_data %>% mutate(condition = factor(condition, labels = c("blank", "congruent","incongruent")))
# center TIME (variable period) on bin 9, and variable trial on number 1000 and rescale; Add dummy variables for each bin.
ptb_data <- ptb_data %>% 
        mutate(period_9 = period - 9,
               trial_c = (trial - 1000)/1000,
               d6  = if_else(period == 6, 1, 0),
               d7  = if_else(period == 7, 1, 0),
               d8  = if_else(period == 8, 1, 0),
               d9  = if_else(period == 9, 1, 0),
               d10 = if_else(period == 10, 1, 0),
               d11 = if_else(period == 11, 1, 0),
               d12 = if_else(period == 12, 1, 0),
               d13 = if_else(period == 13, 1, 0),
               d14 = if_else(period == 14, 1, 0),
               d15 = if_else(period == 15, 1, 0))
```

can instead be...

```{r}
# load person-trial-bin oriented data set
ptb_data <- read_csv("../Tutorial_1_descriptive_stats/data/inputfile_hazard_modeling.csv") %>% 
# select analysis time window: (200,600] with 10 bins (time bin ranks 6 to 15)
filter(period > 5) %>% 
# create factor condition, with "blank" as the reference level
mutate(condition = factor(condition, labels = c("blank", "congruent","incongruent"))) %>% 
# center TIME (variable period) on bin 9, and variable trial on number 1000 and rescale; Add dummy variables for each bin.
mutate(period_9 = period - 9,
       trial_c = (trial - 1000)/1000,
       d6  = if_else(period == 6, 1, 0),
       d7  = if_else(period == 7, 1, 0),
       d8  = if_else(period == 8, 1, 0),
       d9  = if_else(period == 9, 1, 0),
       d10 = if_else(period == 10, 1, 0),
       d11 = if_else(period == 11, 1, 0),
       d12 = if_else(period == 12, 1, 0),
       d13 = if_else(period == 13, 1, 0),
       d14 = if_else(period == 14, 1, 0),
       d15 = if_else(period == 15, 1, 0))
```

-> Done.

And I also wonder. Should this even be in the main text, given that it is in the tutorial?
I'm not sure. But it doesn't strike me as overly important to have the repetition or redundancy, and we have a long paper already. So folks might just get tired of having to figure out more stuff when they don't need to if they only want to read along. What do you think?

-> I think it is important for naive readers to get an idea of how the variables in the model formulas presented later are calculated. 


## model building ##

adapt_delta seems high. I rarely go above 0.99.
And why not init_r = 0.1 or similar instead init = aexactly zero every time.
0.1 means that the starting point varies a bit between -0.1 and 0.1 (i think)

-> adapt-delta of 0.999 is not unusual for generalized linear mixed models.

-> init="0" avoids the chain initialization problems I experienced.


## table 4 and 5 ##

These are way too long. 
My suggestions as follows:

1. just show con and inc 500 for the purposes of the manuscript. then in the main text and legend we can explain that it is just a selection of data for the purposes of understanding.
The full tables can be in SUpps if you feel it is necessary.

2. remove the .width column. just say in the legend 95% quantiles.

->  Done.

3. split c500 and i500 into two columns. e.g, trial =500, condition = con or inc but in two separate columns.

->  Table has changed...

4. Round to two dp.

-> Done.

5. And what's going on with some of those hazard values and odds ratio values? Is it just because one value could be so small that the ratio can be absolutely massive? It just seems odd without any explanation? e.g., take a look at the odds ratio column in table 5.

For now, I just removed them to make the doc easier to manage.

-> Tables have changed in the new ms.


## model formulas ##

some things are not clear, such as:

- model_M1. 

--The use of 0 and Intercept needs explaining.

-> Done.

--adapt_delta is high. I rarely have to go above 0.99. Does the model not look right if this is set lower? This could be a sign that the model is not a suitable structure.

-> .999 is not unusual for generalized linear mixed models.

--step_size - I have never set this. Can you send me some info on this parameter?
-- init set at exactly zero. This should be avoided whenever possible. Try init_r = 0.1. This means that the start points will vary between -0.1 and 0.1, rather than always being the same value (zero).


-- Although condition is defined earlier. I would make it clear in this section what condition means, as it is super important.

-> Done.

- model_M2
-- clearly define what period_9 means.

-> Done.

-- It seems to me that you have quadratic and 3rd order terms, but you only mention 3rd order in the text.

-> I thought a "third order polynomial" implies a constant, linear, quadratic and cubic component... 

## loo plots ##

They seem to be missing from section 4.3.7. 
They would be useful in combinartion with model weights.

-> todo!!!!!!!

## psychological / cognitive interpretation ##

You say ...

>The fact that congruent primes generate negative effects for 80 ms (compared to no-prime trials) towards the end of the experiment, while incongruent primes generate negative effects for 120 ms throughout the experiment, suggests the involvement of separate cognitive processes. 

Note - I removed the word "strongly" from here, as I'm just not sure it is appropriate and it adds nothing anyway.

And my question: We argue the above, but couldn't an equally plausible account be that it reflects the exact same cognitive process, but for different durations? i.e., different onsets and offsets? Is there some aspect to the argument that I am missing? If so, we need to provide those details or give the alternative interpretation. It is still interesting, but I think we need to be clearer here and a little more careful.


And a more general comment on this section, which I don't, as yet, have a perfect solution to.
On page 37, the paragraph starts with Panis and Schmidt (2016) distinguished between automatic ...
I like the aim of this paragraph because it brings folks back to cognitive mechanisms and cog psych, which many readers may be interested in, since that is who the article is aimed at.
In other words, it aims to show how all this fancy modelling and effort can bring folks to a meaningfully different interpretation of the operation of the underlying system.
With that said, after reading quite a bit of stats stuff, modelling stuff and code chunks, parameters etc., to them dump on top a bunch of new processes and terms that are not previously outlined, just feels too much for readers. Like they are punch-drunk at this point and we can tell them anything because they stopped listening or caring. 

So, what to do? I outline some options below.

1. create a new subsection 4.3.9 Relevance for theory (or a similar title) where we place the paragraph and explain why and how such results matter for theory development. By doing so, we accept that it is long and complicated, but we think it is important enough to give it the space it needs.

2. We largely remove the paragraph and instead point readers towards Panis and Schmidt 2016. e.g., To see how such results matter for theory development and the distinction between competing theories, see Panis and Schmidt 2016.

-> I added a new subsection.


## posterior predictions? ##

You say...

>To conclude this Tutorial 2a, Figure 7 shows the model-based hazard functions for each prime type for participant 6, in trials 500, 1000, and 1500.

Are these posterior predictions? If so, they need a little explanation.

-> Done.

## figure settings ##

The below figure settings are in the manuscript file.

```{r plot-settings}
## theme settings for ggplot
theme_set(
  theme_bw() +
    theme(text = element_text(size = 18), 
          title = element_text(size = 18),
          legend.position = "bottom")
)

## Set the amount of dodge in figures
pd <- position_dodge(0.7)
pd2 <- position_dodge(1)
```

This was confusing initially, because your figures definitely do not follow these settings.

Then I realised that you are just taking figures from files. 
So, the files used to generate the figures need this section adding to the top of the script, so that they are generated in a consistent manner.

-> Done.

Also, I would be explicit with sizing when saving figures, if you don't so this already.
If not, the size of the saved figure is determined by the plot window in RStudio and that can change depending on the screen and size of RStudio at the time.
And also set the figure dpi higher for a crisper resolution e.g., dpi=800.. 

https://ggplot2.tidyverse.org/reference/ggsave.html

e.g.,

```{r}
ggsave("sims/figures/haz_surv.jpeg",
       width = 7, height = 5, dpi=800)
```

-> Done.

One more important thing - colours/aesthetics etc. 
I think we can improve our data vis game. 
e.g., Figure 6 and 8 look great, but Figure 7 and 9 need some work.
I think we can pick better colour palettes e.g., https://r-graph-gallery.com/38-rcolorbrewers-palettes.html

I often go with the following palette ("Dark2"), see text and chunk below as an example. use colour and fill depending on whether you want the particular palette applied to colour or fill aesthetics or both.

  scale_colour_brewer(palette = "Dark2") + 
  scale_fill_brewer(palette = "Dark2") + 

```{r}
p9.7 <-ggplot(data_hs, aes(x = timebin, y = probability,
                         colour = dv)) +
  geom_line(linewidth = 1) +
  geom_point(size=3) +
  scale_colour_brewer(palette = "Dark2") + 
  scale_x_continuous(breaks = 1:10) +
  scale_y_continuous(limits = c(0,1)) +
  ggtitle("hazard and survivor functions") 
p9.7
```

This is important for several reasons.
1. Data vis is just important and can really make a paper stand out.
2. We need to be consistent throughout and we don't want some tables/figures looking great and others looking half-finished or an after thought.

-> Done.

## figure legends ##

in some cases, much more information is needed. e.g., Figure 7 and 9. Folks need a guide.

-> I have to check for all Figures... todo



## figure layout ##

have you considered 1 column instead of 3 for figure 7/9?
does it look better/easier to understand? Maybe try it and see?

Figure 9 looks odd. Can we re-szie and re-colour as sggested above. and maybe try one column for the facet rather than 3. To set this, just insert "ncol=1" into the facet argument. e.g., facet_wrap(~condition, ncol=1).

-> Figure 9 has changed.

Figure 10 can be flipped such that the axes are opposite. e.g., flip x and y. then the text will be easier to read and plots easier to understand, in terms of numerical values. 

-> Done


## Discussion ##

I really liked reading the discussion. 
For the most part, it was compact and efficient and covered some big-ticket items.
The one exception was the 5.1 advantages section. It was all very clear, but it read like a background to the topic, which should come in the intro, rather than at the end. e.g., we've just been through all of this text, code, figures and model building and then we decide to say there are advantages?? It just seems out of sync.
So, I moved the text to Supplementary Material and created a new sub-section in supps.
There are several benefits of doing so. 

1) we can keep the discussion focussed on things that are relevant to exp psych folks, rather than more general stats topics, which would be relevant for textbooks and the like. 

2) we can link to these benefits earlier on when we give a background to EHA in section 2.1.
I've created a new sub-section in sectino 2 and added a sumamry paragraph that points the reader to the supplementary materials for the details.

3) We can add something else into the discussion that is more relevant to our aim here. I have tried to add in a section on common use-cases for exp psych folks. It might not quite work or fit in yet, but the motivation was something like this. You may want to use it if you are interested in the timeline of effects. But you may also need to use it from a counfound/technical/method perspective because it might qualify inferences made from mean-average comparisons. The reason is just to make it clear that you need not be interested in temporal states to find this useful on a methodological/analytical perspective.

# comments Sven 6 nov

1. Figure 1: connect the points per subject?

# comments Sven 26 november

Still to do:

- check legends of all Figures, and add more info if needed

- make loo plots (I did not find code to do that. Do you have a reference for such code?)

- add info on simulation and power analysis (section 4.4 + summary in section 4)

- delete code in Tutorial_2a about brand new hypothetical subject?

- use timebin or time bins?

- use multi-level or multilevel ?


## December 2nd 2024 ##

I read your comments above. 

On first glance, I really like the new plots!

I'll dig in now with some comments.

### intro ###

I think the Halley Ref should be 1693 and not 1997? We have a copy of the original manuscriopt, so it seems reasonable to cite the original?

-> Done.

### section 3.1 ###

maybe add a sentence to highlights how we add our own custom functions to make the wrangling and plotting sections quicker and more efficient. e.g., we add to this approach by writing our own custom functions.

### section 3.2 ###

first sentence, the McElreath reference should be 2020 for the textbook and not 2018.



### 4.3.4 Model M1d ###

I'm not convinced we need this model to be reported in the main text. It is complex and deviates from the prior model structure with index coding.
Folks need to get their head around polynomial specifications of time. And we don't really need them to and we don't really give them much chance to understand it. It is kind of like "trust me".
Instead, I would have this in the tutorial with a qualifier saying...you can also build models that specify time differently...

Then in the main text just compare M0i and M1i, for the purposes of demonstrating how to do it.
The basic conclusion being that adding the factor condition improves predictive accuracy compare to timebin only. 
That's maybe quite basic, but also quite important too for our conclusion - congruency matters.

Or maybe have these models, all with index coding:

- grand intercept only (no predictors)
- timebin only (M0i)
- congruency only (without timebin)
- congruency:timebin (M1i)

Or just stick with two models, as you have above (M0i and M1i).

The main thing we want to communicate is how and why to do model comparison. So let's keep it as simple as possible, at least in the main text.

### section 4.3.3 ###

we need to define index coding.

-> McElreath book... def


maybe somewhere earlier we could have a subsection that explains different coding schemes for categorical variables? e.g., dummy, reference and contrast it with index. Then say we focus on index in the main text, as we think it is easier to interpret means per condition.

### section 4.3.6 ###

Tables 4 and 5 can be moved to the tutorial only (or Supps) whichever makes most sense. 

->

Figure 5 y-axis. Should it just be cloglog? Rather than cloglog-hazard?

Another way to plot Figure 5 would be to avoid a facet and instead fill by prime. That way, you might benefit from the principle of putting things close together that you want to compare e.g., the effects of prime should be easier to see if prime comparisons are close together. Maybe at least plot it the other way to have a look and see what you think? I'd like to see it at least.

->

There is no need to commit the space needed when they are redundant with the figures. e.g., we can just say in the main text, the point estimates and quantile intervals can be reported in a table (see the tutorial for details). That saves loads of space and time scanning values when the figures are beautiful and standalone as they are.

also, the title of this sub-section should be about evaluating parameter estimates (rather than interpreting model ...). This is because you can use model comparison to interpret the model. So we should make the subsection distinctive.

This figures in this section are wonderful. Nice work!

I suggest two changes:

1. Combine Figures 6 and 7 into one figure (so that the group average is at the top and the individual PIDs are below. Patchwork can do this for you. And if you include + plot_annotation(tag_levels = 'A') then you'll get a nice A and B to label the different parts of the plot.)

2. As (1) combine Figures 8 and 9.

->

on page 38, you say:

>Note that we could also have calculated hazard ratios instead of hazard differences.

This feels a little disjointed from what comes before in the paragraph.
Either remove it or provide an extra sentence that distinguishes between the absolute hazard difference and the hazard ratio.

-> I will remove it.

Figure 6. Change the title from "Average participant" as it is a bit misleading. Maybe something like "group average" or "grand average" or even remove the title completely once you combine it with the individual pid plots because it will probably become obvious, especially with a figure legend explaining things.

->

Figure 9. spell out AMEs in full to avoid jargon.

->

### section 4.4 ###

Combine Figures 11 and 12 like I suggest above.

Put Table 6 in the tutorial and remove it from the main text.

Figure 14. Do a facet_wrap by prime type. Put timebin on the x axis and cloglog on the y-axis. This makes the orientation similar to the prior plots and will therefore be easier to follow for the reader.

