---
title: "Tutorial_2a"
author: "Sven Panis"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: united
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Tutorial 2a shows various ways to fit Bayesian binary regression models to discrete time-to-event data. The data are taken from the first experiment of Panis and Schmidt (2016), but using only the no-mask trials (prime = blank, congruent, or incongruent). We then compare models using WAIC and LOO, and interpret parameter estimates for one model. 
We also plot the logit and cloglog link functions, and illustrate how various prior distributions on the logit and cloglog scales look on the original probability (i.e., hazard) scale. 

# 1. Load the libraries that we will be using.

```{r load-pkg, results='hide'}
pkg <- c("cmdstanr", "standist", "tidyverse", "RColorBrewer", "patchwork", 
         "brms", "tidybayes", "bayesplot", "future", "parallel", "modelr")

lapply(pkg, library, character.only = TRUE)
```

Set options. 

```{r set-options, results='hide'}
options(brms.backend = "cmdstanr",
        mc.cores = parallel::detectCores(),
        future.fork.enable = TRUE,
        future.rng.onMisuse = "ignore") ## automatically set in RStudio

supportsMulticore()
detectCores()
```

```{r check-info, results='hide'}
packageVersion("cmdstanr")
devtools::session_info("rstan")
```

theme settings for ggplot

```{r plot-settings, results='hide'}
theme_set(
  theme_bw() +
    theme(text = element_text(size = 22, face = "bold"), 
          title = element_text(size = 22, face = "bold"),
          legend.position = "bottom")
)

## Set the amount of dodge in figures
pd <- position_dodge(0.7)
pd2 <- position_dodge(1)
```

# 2. Load and wrangle the person-trial-bin data set that we saved in Tutorial 1a.

```{r load-data}
ptb_data <- read_csv("Tutorial_1_descriptive_stats/data/inputfile_hazard_modeling.csv")
print(ptb_data,n=30)
summary(ptb_data) # 26602 rows: 6 participants, trial (experiment-wise), 3 conditions, 15 periods, and event indicator (0/1)
```

Wrangle the data set. 

```{r wrangle-data}
ptb_data <- ptb_data %>% 
# select analysis time range: (200,600] with 10 bins (time bin ranks 6 to 15)
  filter(period > 5) %>%
  
# create categorical predictor for TIME named "timebin" with index coding
  mutate(timebin = factor(period,levels=c(6:15)),
# create continuous predictor for TIME named "period_9", centered on bin 9,
         period_9 = period - 9,
# create binary variables "dummys" to indicate each bin in the analysis
         d6  = if_else(period == 6, 1, 0),
         d7  = if_else(period == 7, 1, 0),
         d8  = if_else(period == 8, 1, 0),
         d9  = if_else(period == 9, 1, 0),
         d10 = if_else(period == 10, 1, 0),
         d11 = if_else(period == 11, 1, 0),
         d12 = if_else(period == 12, 1, 0),
         d13 = if_else(period == 13, 1, 0),
         d14 = if_else(period == 14, 1, 0),
         d15 = if_else(period == 15, 1, 0),

# create continuous predictor for trial number named "trial_c", centered on bin 1000 and rescaled
         trial_c = (trial - 1000)/1000,
# create categorical predictor for trial number named "stage" (early,middle,late) with index coding
         stage = ifelse(trial <= 500, 1, ifelse(trial > 1000, 3, 2)),
         stage = factor(stage, levels=c(1,2,3)),

# create factor "condition", with "blank" as the reference level
         condition = factor(condition, labels = c("blank", "congruent","incongruent")),
# create categorical predictor "prime" with index-coding
         prime = ifelse(condition=="blank",1, ifelse(condition=="congruent",2,3)),
         prime = factor(prime,levels=c(1,2,3))) %>%
  select(pid,event,trial,trial_c,stage,condition,prime,period,period_9,timebin,d6:d15)

head(ptb_data,n=17)
summary(ptb_data)
```

# 3. Fit hazard models.

## Model M0: A null model without experimental predictors

The first model we fit is a "random intercepts" model, where we fit a single intercept for each timebin and allow each intercept to vary between participants.
This is a general specification of TIME, and can be used if we do not want to make assumptions about how cloglog-hazard changes over time (within a trial). 
There are two ways to implement such a model. First, we can use the index-coding approach, which supplies an intercept for each level of TIME.

Prepare the data file, and specify priors.

```{r data-priors-M0}
M0_data <- ptb_data %>% select(pid, event, timebin)

priors_M0 <- c(
  set_prior("skew_normal(-0.59,1.26,-4.22)", class = "b"), # See suppl. material 
  set_prior("normal(0, 1)", class = "sd"),                    
  set_prior("lkj(2)", class = "cor")                        
)
```

Fit model M0.

```{r fit-model-M0, eval=F} 
plan(multicore)

model_M0 <-                     # wrong data file name
   brm(data = M0_data,
       family = binomial(link="cloglog"),
       event|trials(1) ~ 0 + timebin + (0 + timebin | pid),
       prior = priors_M0,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M0")
```

This took 43 minutes on a MacBook Pro (Sonoma 14.6.1 OS, 18GB Memory, M3 Pro Chip).

```{r}
model_M0 <- readRDS("Tutorial_2_Bayesian/models/model_M0.rds")
summary(model_M0)
formula(model_M0)
plot(model_M0)
```

Second, we can fit the same model using a dummy coding approach, as follows.

Prepare the data file, and specify priors.

```{r data-priors-M0d}
M0d_data <- ptb_data %>% select(pid, event, d6:d8, d10:d15)

priors_M0d <- c(
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d6"),
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d7"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d8"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d10"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d11"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d12"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d13"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d14"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d15"),
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "Intercept"),
  set_prior("normal(0, 1)", class = "sd"),  
  set_prior("lkj(2)", class = "cor")  
)
```

Fit the model with dummy coding (M0d).

```{r fit-model-M0d, eval=F}
plan(multicore)

model_M0d <-            # bernouilli.  :(
   brm(data = M0d_data,
       family = bernoulli(link="cloglog"),
       event ~ 0 + d6 + d7 + d8 + Intercept + d10 + d11 + d12 + d13 + d14 + d15 + 
                   (d6 + d7 + d8 + 1 + d10 + d11 + d12 + d13 + d14 + d15  | pid),
       prior = priors_M0d,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M0d")
```

M0d took 77 minutes.

```{r}
model_M0d <- readRDS("Tutorial_2_Bayesian/models/model_M0d.rds")
summary(model_M0d)
formula(model_M0d)
```

### Make assumptions to simplify the model

Instead of using a general specification of TIME, we can treat TIME as a continuous variable and make assumptions about how hazard changes over time within a trial. For example, we might assume that hazard changes in a linear + quadratic fashion over time bins within a trial.

Prepare the data file, and specify priors.

```{r data-priors-M0c}
M0c_data <- ptb_data %>% select(pid, event, period_9) %>%
  mutate(period_9_sq = period_9^2,
         period_9_cu = period_9^3)

priors_M0c <- c(
  set_prior("skew_normal(-0.2,0.71,-2.2)", class = "b"),      
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "Intercept"),  
  set_prior("normal(0, 1)", class = "sd"),  
  set_prior("lkj(2)", class = "cor")  
)
```

Fit the model with a continuous predictor (M0c).

```{r fit-model-M0c, eval=F}
plan(multicore)

model_M0c <-
   brm(data = M0c_data,
       family = binomial(link="cloglog"),
       event | trials(1) ~ 0 + Intercept + period_9 + period_9_sq + period_9_cu + 
                          (0 + Intercept + period_9 + period_9_sq + period_9_cu  | pid),
       prior = priors_M0c,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M0c")
```

Model_M0c took about 144 minutes to run.

```{r}
model_M0c <- readRDS("Tutorial_2_Bayesian/models/model_M0c.rds")
summary(model_M0c)
formula(model_M0c)
```

## Model M1. Adding our experimental manipulation.

The next model we fit also includes our predictor variable prime type.
First, we can use index coding for the categorical predictor.

Prepare the data file, and specify priors.

```{r data-priors-M1}
M1_data <- ptb_data %>% select(pid, event, timebin, prime) 

priors_M1 <- c(
  set_prior("skew_normal(-0.59,1.26,-4.22)", class = "b"), 
  set_prior("normal(0, 1)", class = "sd"),                    
  set_prior("lkj(2)", class = "cor")                        
)
```

Fit model M1.

```{r fit-model-M1, eval=F}
plan(multicore)

model_M1 <-
   brm(data = M1_data,
       family = binomial(link="cloglog"),
       event|trials(1) ~ 0 + timebin:prime  +
                        (0 + timebin:prime | pid),
       prior = priors_M1,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M1")
```

Model M1 took about 126 minutes to fit.

```{r}
model_M1 <- readRDS("Tutorial_2_Bayesian/models/model_M1.rds")
summary(model_M1)
formula(model_M1)
```

Second, if we want to make assumptions about (1) how hazard changes over TIME in the reference condition (blank prime), and (2) how the effect of congruent and incongruent primes change over TIME (relax the proportionality assumption), then we can switch to a dummy coding approach and treat TIME as a continuous variable.
For example, we may assume that hazard can change in a linear + quadratic + cubic fashion over time for a blank prime, and that the effects of congruent and incongruent primes relative to blank change in a linear + quadratic + cubic fashion, and fit the following model (M1d).

```{r data-priors-M1d}
M1d_data <- ptb_data %>% select(pid, event, period_9, condition) %>%
  
priors_M1d <- c(
  set_prior("skew_normal(-0.2,0.71,-2.2)", class = "b"),
  set_prior("skew_normal(-1,1,-2)", class = "b", coef="Intercept"), 
  set_prior("normal(0, 1)", class = "sd"),            
  set_prior("lkj(2)", class = "cor")                  
)
```

```{r fit-model-M1d, eval=F}
plan(multicore)

model_M1d <- 
   brm(data = M1d_data,
       family = binomial(link="cloglog"),
       event | trials(1) ~ 0 + Intercept + 
                           condition*period_9 + 
                           condition*period_9_sq + 
                           condition*period_9_cu +
                           (1 + condition*period_9 +
                           condition*period_9_sq +
                           condition*period_9_cu | pid),
       prior = priors_M1d,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M1d")
```

Model_M1d took about 268 minutes to run. Note that duplicate terms in the model formula (e.g., condition) are ignored.

```{r}
model_M1d <- readRDS("Tutorial_2_Bayesian/models/model_M1d.rds")
summary(model_M1d)
formula(model_M1d)
```

One could also fit a model with prime and period_9. However, to include interactions between a index coded categorical variable and a continuous variable in brm(), one has to switch to the non-linear syntax, as illustrated in the following model formula.

```{r}
bf(event|trials(1) ~ 0 + a + b * period_9 + c * period_9_sq,
                 a ~ 0 + prime + (0 + prime |i| pid),
                 b ~ 0 + prime + (0 + prime |i| pid),
                 c ~ 0 + prime + (0 + prime |i| pid),
                 nl = TRUE)
```

The priors can be set as follows.

```{r priors-nonlinearsyntax}
priors <- c(
  prior(skew_normal(-0.59,1.26,-4.22), class = b, nlpar = a), 
  prior(skew_normal(-0.2,.71,-2.2), class = b, nlpar = b), 
  prior(skew_normal(-0.2,.71,-2.2), class = b, nlpar = c), 
  prior(exponential(1), class = sd, group = pid, nlpar = a),
  prior(exponential(1), class = sd, group = pid, nlpar = b),
  prior(lkj(2), class = cor, group = pid)
)
```

## Model M2. Including effects of trial number.

Up until now, we have been working with one time scale, TIME or bin rank within a trial. While many cognitive processes play out on this short time scale (milliseconds to seconds), some play out on longer time scales (minutes to hours), e.g., learning processes.

When we are interested in studying how the hazard of response occurence in our priming experiment also changes on a longer time scale, we can add trial number into the model formula. Here we simply illustrate some possibilities with index and reference coding.

First, we can categorize the predictor trial number by grouping trials in one of three "stages" of the experiment (stage 1 = trials 1 to 500; stage 2 = trials 501 to 1000; stage 3 = trials 1001 and later) and use index coding (variable "stage" in ptb_data).

```{r}
event|trials(1) ~ 0 + timebin:prime:stage  + (0 + timebin:prime:stage | pid)
```

Second, we can treat trial number as a continuous variable, and make assumptions about the way hazard changes with trial number (e.g., linear, quadratic, etc.) for each combination of timebin and prime.

```{r}
bf(event|trials(1) ~ 0 + a + b * trial_c + c * trial_c_sq,
                 a ~ 0 + timebin:prime + (0 + timebin:prime |i| pid),
                 b ~ 0 + timebin:prime + (0 + timebin:prime |i| pid),
                 c ~ 0 + timebin:prime + (0 + timebin:prime |i| pid),
                 nl = TRUE)
```

Third, one can use dummy coding for prime type (variable "condition" in ptb_data), and treat TIME and trial number as continous variables.

```{r}
 bf(event | trials(1) ~ 0 + Intercept + 
                          condition*period_9*trial_c + 
                          condition*period_9*trial_c_sq + 
                          condition*period_9_sq*trial_c +
                          condition*period_9_sq*trial_c_sq +
                          (1 +  condition*period_9*trial_c +
                                condition*period_9*trial_c^2) + 
                                condition*period_9_sq*trial_c +
                                condition*period_9_sq*trial_c_sq  | pid)
```

# Compare models using loo and waic.

The predictive accuracy of a set of models can be compared using WAIC and LOO.

```{r compare-models}
model_M0  <- readRDS("Tutorial_2_Bayesian/models/model_M0.rds")
model_M0d <- readRDS("Tutorial_2_Bayesian/models/model_M0d.rds")
model_M0c <- readRDS("Tutorial_2_Bayesian/models/model_M0c.rds")
model_M1  <- readRDS("Tutorial_2_Bayesian/models/model_M1.rds")
model_M1d <- readRDS("Tutorial_2_Bayesian/models/model_M1d.rds")
```

Using WAIC and LOO for comparing nonnested models.

```{r add-criterion, eval=F}
model_M0  <- add_criterion(model_M0, c("loo", "waic"))
model_M0d <- add_criterion(model_M0d, c("loo", "waic"))
model_M0c <- add_criterion(model_M0c, c("loo", "waic"))
model_M1  <- add_criterion(model_M1, c("loo", "waic"))
model_M1d <- add_criterion(model_M1d, c("loo", "waic"))
```

Compare all three models.

```{r loo-compare}
loo_compare(model_M0, model_M0d, model_M0c, model_M1, model_M1d, criterion = "loo") %>% print(simplify = F)
loo_compare(model_M0, model_M0d, model_M0c, model_M1, model_M1d, criterion = "waic") %>% print(simplify = F)
```

```{r model-weights}
model_weights(model_M0, model_M0d, model_M0c, model_M1, model_M1d, weights = "loo") %>% round(digits = 3)

model_weights(model_M0, model_M0d, model_M0c, model_M1, model_M1d, weights = "waic") %>% round(digits = 3)
```

# Model inference for model M1.

## Diagnostics

The Pareto k estimates can be displayed as follows.

```{r pareto}
loo(model_M1)$diagnostics %>% 
  data.frame() %>% 
  # attach the `id` values
  bind_cols(M1_data) %>% 
  mutate(id = 1:n()) %>%
  
  ggplot(aes(x = id, y = pareto_k)) +
  geom_point(alpha = 3/4) + 
  geom_text(data = . %>% filter(pareto_k > .2),
            aes(x = id + 2, label = id),
            size = 3, hjust = 0) +
  theme(panel.grid = element_blank())
```

## Posterior distributions of the population-level parameters of M1

Wrangle the posterior draws.

```{r}
post <- as_draws_df(model_M1) %>% 
  select(-lp__) %>% 
  as_tibble()

post_summary <- posterior_summary(model_M1, robust = TRUE)
post_summary[1:30,]

post_qi_b <- post %>%
  select(starts_with("b_")) %>%
  pivot_longer(everything()) %>%
  group_by(name) %>% 
  median_qi(value) %>% 
  arrange(name)
head(post_qi_b) # 30 "fixed" effects 
```

Visualise fixed effects.

```{r}
tidy_fixed <- post %>% 
  select(starts_with("b_"), .chain, .iteration, .draw) %>% 
  rename(chain=.chain, iter=.iteration, draw=.draw) %>% 
  pivot_longer(-c(chain, draw, iter)) %>% 
  mutate(timebin = str_sub(name,10,11),
         timebin = factor(str_remove(timebin,":"),levels=c(6:15)),
         condition = str_sub(name,17,18),
         condition = factor(str_remove(condition,"e"),
                            levels=c(1,2,3),
                            labels=c("blank","congruent","incongruent")))
head(tidy_fixed)
tail(tidy_fixed)

# plot
p_tidy_fixed <- ggplot(tidy_fixed, aes(x = timebin, y = value, fill=condition)) +  
  stat_halfeye(alpha=0.7) +
  labs(title = 'Posterior distributions for "fixed" effects\nin Model M1',
       x = "time bin", y = "cloglog-hazard") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
   theme(axis.text.x = element_text(angle=90)) +
  facet_wrap(~condition)
p_tidy_fixed
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_postdistr.png", width = 10, height = 8, dpi = 800)
```

## Expected value of the posterior predictive distribution

We can plot the expected value of the posterior predictive distribution -- the predicted hazard values -- for the average participant, for each participant in the data set, and for a brand new hypothetical participant.

For the average participant, using add_epred_draws().

```{r}
dat_M1 <- as_tibble(model_M1$data)

epreds_grand <- dat_M1 %>% 
  data_grid(timebin, prime)  %>% 
  add_epred_draws(model_M1, re_formula = NA) %>% 
  mutate(prime = factor(prime, levels=c(1,2,3),labels=c("blank","congruent","incongruent"))) %>%
  ungroup()
```

Summarize and plot predicted hazard values.

```{r}
epreds_grand %>% group_by(timebin,prime) %>% mean_qi()
```

```{r}
 ggplot(epreds_grand, aes(x=timebin, y=.epred, 
                       fill=prime, color=prime)) +
    stat_lineribbon(alpha = 0.5, .width = .95) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    theme(axis.text.x = element_text(angle=90)) +
    scale_y_continuous(limits=c(0,.6)) +
    labs(y = "predicted hazard") +
    ggtitle("Average participant")
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_pred_grand.png", width = 10, height = 8, dpi = 800)
```

Again, for the average participant, but using posterior draws.

```{r}
tidy_fixed %>%
  mutate(haz = 1-exp((-1)*exp(value))) %>% # inverse cloglog
  
ggplot(aes(x=timebin, y=haz, 
                       fill=condition, color=condition)) +
    stat_lineribbon(alpha = 0.5, .width = .95) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    theme(axis.text.x = element_text(angle=90)) +
    scale_y_continuous(limits=c(0,.6)) +
    labs(y = "predicted hazard") +
    ggtitle("Average participant")
```

Second, for each participant in the data set.

```{r}
epreds_pid <- dat_M1 %>% 
  data_grid(pid,timebin, prime)  %>% 
  add_epred_draws(model_M1, re_formula = NULL) %>% 
  mutate(prime = factor(prime, levels=c(1,2,3),labels=c("blank","congruent","incongruent"))) %>%
  ungroup()
```

Summarize and plot predicted hazard values.

```{r}
epreds_pid %>% group_by(pid,timebin,prime) %>% mean_qi()
```

```{r}
 ggplot(epreds_pid, aes(x=timebin, y=.epred, 
                       fill=prime, color=prime)) +
    stat_lineribbon(alpha = 0.5, .width = .95) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    theme(axis.text.x = element_text(angle=90)) +
    scale_y_continuous(limits=c(0,1)) +
    labs(y = "predicted hazard") +
    ggtitle("Each participant (N = 6)") +
    facet_wrap(~pid, ncol=3)
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_pred_pid.png", width = 10, height = 8, dpi = 800)
```

Third, for a completely new hypothetical participant.

```{r}
epreds_pid_new <- 
  posterior_epred(model_M1, 
                  newdata = expand_grid(timebin=6:15,prime=1:3,pid=7), 
                  re_formula = NULL, 
                  allow_new_levels=T, 
                  sample_new_levels = "gaussian") %>% 
  as_tibble %>%
  mutate(draw = 1:n()) %>%
  pivot_longer(V1:V30,values_to = ".epred", names_to = "condition") %>%
  
  mutate(prime = factor(rep(c(1,2,3), 10*8000), levels=c(1,2,3),labels=c("blank","congruent","incongruent")),
         timebin = factor(rep(rep(c(6:15),each=3),8000),levels=c(6:15)  )) %>%
  ungroup()
```

```{r}
epreds_pid_new <- dat_M1 %>% 
  data_grid(timebin, prime)  %>%
  mutate(pid = 7) %>%
  add_epred_draws(model_M1, re_formula = NULL, allow_new_levels=T,    sample_new_levels = "gaussian") %>% 
  mutate(prime = factor(prime, levels=c(1,2,3),labels=c("blank","congruent","incongruent"))) %>%
  ungroup()
```

Summarize and plot predicted hazard values.

```{r}
epreds_pid_new %>% group_by(pid,timebin,prime) %>% mean_qi()
```

```{r}
 ggplot(epreds_pid_new, aes(x=timebin, y=.epred, 
                       fill=prime, color=prime)) +
    stat_lineribbon(alpha = 0.5, .width = .95) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    theme(axis.text.x = element_text(angle=90)) +
    scale_y_continuous(limits=c(0,1)) +
    labs(y = "predicted hazard") +
    ggtitle("A brand new hypothetical participant") 
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_pred_pid_new.png", width = 10, height = 8, dpi = 800)
```


### Grand average marginal effects

We are actually interested in the difference in predicted hazard between congruent and blank primes on the one hand, and between incongruent and blank primes on the other hand, for each time bin. When based on the grand mean, this is known as the grand average marginal effect (AME).

```{r}
epreds_grand_diffs <- epreds_grand %>%
  pivot_wider(id_cols = c(timebin, .draw),
              names_from = "prime",
              values_from = ".epred") %>% #80000 x 5
  mutate(`congruent minus blank` = congruent - blank,
         `incongruent minus blank` = incongruent -  blank) %>% 
  select(-c(blank,congruent,incongruent)) %>% 
  pivot_longer(cols = c(`congruent minus blank`,`incongruent minus blank`),
               names_to = "contrast",
               values_to = "pred_haz") 

epreds_grand_diffs # 160 000 rows 
```

Summarize and plot.

```{r}
table <- epreds_grand_diffs %>%
  group_by(contrast,timebin) %>%
    mean_qi(.width = c(.95)) %>%
  arrange(contrast,timebin,.width) %>%
  select(-c(.width,.point,.interval))
```

```{r save-table-grand-AMEs, eval=F}
write_csv(table, file="Tutorial_2_Bayesian/tables/grand_AMEs.csv")
```

```{r}
 ggplot(epreds_grand_diffs, aes(x=timebin, y=pred_haz, 
                       fill=contrast)) +
  stat_lineribbon(alpha = 0.5, point_interval = "mean_qi",.width=c(.8,.95)) +
 # stat_halfeye(point_interval = "mean_qi",.width=c(.8,.95))+
  geom_hline(yintercept = 0, color = "red", lty = 3) +
  scale_y_continuous(limits=c(-0.5,0.3)) +
  scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  theme(axis.text.x = element_text(angle=90)) +
  labs(y = "difference in predicted hazard") +
  ggtitle("Grand average marginal effect") +
  facet_wrap(~contrast)
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_grand_AME.png", width = 10, height = 8, dpi = 800)
```

Note that these grand AMEs can also be calculated based on the posterior draws.

```{r}
post %>% # 8000 x 30 = 240000
  mutate_all(function(x){1-exp((-1)*exp(x))}) %>%
  mutate(bin6_cb = `b_timebin6:prime2` - `b_timebin6:prime1`,
         bin7_cb = `b_timebin7:prime2` - `b_timebin7:prime1`,
         bin8_cb = `b_timebin8:prime2` - `b_timebin8:prime1`,
         bin9_cb = `b_timebin9:prime2` - `b_timebin9:prime1`,
         bin10_cb = `b_timebin10:prime2` - `b_timebin10:prime1`,
         bin11_cb = `b_timebin11:prime2` - `b_timebin11:prime1`,
         bin12_cb = `b_timebin12:prime2` - `b_timebin12:prime1`,
         bin13_cb = `b_timebin13:prime2` - `b_timebin13:prime1`,
         bin14_cb = `b_timebin14:prime2` - `b_timebin14:prime1`,
         bin15_cb = `b_timebin15:prime2` - `b_timebin15:prime1`,
         bin6_ib = `b_timebin6:prime3` - `b_timebin6:prime1`,
         bin7_ib = `b_timebin7:prime3` - `b_timebin7:prime1`,
         bin8_ib = `b_timebin8:prime3` - `b_timebin8:prime1`,
         bin9_ib = `b_timebin9:prime3` - `b_timebin9:prime1`,
         bin10_ib = `b_timebin10:prime3` - `b_timebin10:prime1`,
         bin11_ib = `b_timebin11:prime3` - `b_timebin11:prime1`,
         bin12_ib = `b_timebin12:prime3` - `b_timebin12:prime1`,
         bin13_ib = `b_timebin13:prime3` - `b_timebin13:prime1`,
         bin14_ib = `b_timebin14:prime3` - `b_timebin14:prime1`,
         bin15_ib = `b_timebin15:prime3` - `b_timebin15:prime1`) %>%
  select(starts_with("bin"))  %>% # 8000 x 20
  
  pivot_longer(cols = bin6_cb:bin15_ib, names_to = "condition", values_to = "haz_diff") %>%
  mutate(bin = str_sub(condition,4,5),
         bin = str_remove(bin,"_"),
         bin = factor(bin, levels=c(6:15)),
         comp = str_sub(condition,6,8),
         comp = str_remove(comp,"_"),
         contrast = ifelse(comp == "cb", "congruent minus blank", "incongruent minus blank")) %>% 
  
  ggplot(aes(x=bin, y = haz_diff, fill = contrast)) +
    stat_halfeye(point_interval = "mean_qi",.width=c(.8,.95)) +
    geom_hline(yintercept = 0, color = "red", lty = 3) +
    scale_y_continuous(limits=c(-0.5,0.3)) +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    theme(axis.text.x = element_text(angle=90)) +
    labs(y = "difference in predicted hazard", x = "timebin") +
    ggtitle("Grand average marginal effect") +
    facet_wrap(~contrast)
```

### Subject-specific average marginal effects

To calculate the subject-specific AMEs for each time bin, we create contrasts between the conditional means.

```{r}
epreds_pid_diffs <- epreds_pid %>%
  pivot_wider(id_cols = c(pid,timebin, .draw),
              names_from = "prime",
              values_from = ".epred") %>% #80000 x 5
  mutate(`congruent minus blank` = congruent - blank,
         `incongruent minus blank` = incongruent -  blank) %>% 
  select(-c(blank,congruent,incongruent)) %>% 
  pivot_longer(cols = c(`congruent minus blank`,`incongruent minus blank`),
               names_to = "contrast",
               values_to = "pred_haz") 
epreds_pid_diffs # 160 000 rows 
```

Summarize and plot the contrasts in predicted hazard values.

```{r}
epreds_pid_diffs %>% group_by(pid,timebin,contrast) %>% mean_qi()
```

```{r}
 ggplot(epreds_pid_diffs, aes(x=timebin, y=pred_haz, 
                       fill=contrast, color=contrast)) +
    stat_lineribbon(alpha = 0.5, point_interval = "mean_qi",.width=c(.95)) +
    #stat_halfeye(point_interval = "mean_qi",.width=c(.8,.95)) +
    geom_hline(yintercept = 0, color = "red", lty = 3) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    theme(axis.text.x = element_text(angle=90)) +
    scale_y_continuous(limits=c(-0.6,0.6)) +
    labs(y = "difference in predicted hazard") +
    ggtitle("Subject-specific AMEs") +
    facet_wrap(~pid, ncol=3)
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_pid_AMEs.png", width = 10, height = 8, dpi = 800)
```


[[remove next section....]]
And check whether an average of subject-specific AMEs is equal to the grand AME... NO !!!

```{r}
epreds_pid_diffs_av <- epreds_pid_diffs %>% group_by(timebin,contrast,.draw) %>%
  mutate(average_condmeans = mean(pred_haz))
```

```{r}
 ggplot(epreds_pid_diffs_av, aes(x=timebin, y=pred_haz, 
                       fill=contrast)) +
  stat_lineribbon(alpha = 0.5, point_interval = "mean_qi",.width=c(.8,.95)) +
 # stat_halfeye(point_interval = "mean_qi",.width=c(.8,.95))+
  geom_hline(yintercept = 0, color = "red", lty = 3) +
  scale_y_continuous(limits=c(-0.5,0.3)) +
  scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
  scale_fill_brewer(palette = "Dark2") +
  scale_color_brewer(palette = "Dark2") +
  theme(axis.text.x = element_text(angle=90)) +
  labs(y = "difference in predicted hazard") +
  ggtitle("Average of subject-specific AMEs") +
  facet_wrap(~contrast)
```

### AMEs for a brand new hypothetical participant

```{r}
epreds_pid_new_diffs <- epreds_pid_new %>%
  pivot_wider(id_cols = c(pid,timebin, .draw),
              names_from = "prime",
              values_from = ".epred") %>% #80000 x 5
  mutate(`congruent minus blank` = congruent - blank,
         `incongruent minus blank` = incongruent -  blank) %>% 
  select(-c(blank,congruent,incongruent)) %>% 
  pivot_longer(cols = c(`congruent minus blank`,`incongruent minus blank`),
               names_to = "contrast",
               values_to = "pred_haz") 
epreds_pid_new_diffs # 160 000 rows 
```

Summarize and plot the contrasts in predicted hazard values.

```{r}
epreds_pid_new_diffs %>% group_by(pid,timebin,contrast) %>% mean_qi()
```

```{r}
 ggplot(epreds_pid_new_diffs, aes(x=timebin, y=pred_haz, 
                       fill=contrast, color=contrast)) +
    stat_lineribbon(alpha = 0.5, point_interval = "mean_qi",.width=c(.95)) +
    #stat_halfeye(point_interval = "mean_qi",.width=c(.8,.95)) +
    geom_hline(yintercept = 0, color = "red", lty = 3) +
    scale_fill_brewer(palette = "Dark2") +
    scale_color_brewer(palette = "Dark2") +
    scale_x_discrete(labels = str_c("(",(c(6:15)-1)*40,",",c(6:15)*40,"]"), breaks = 6:15) +
    theme(axis.text.x = element_text(angle=90)) +
    scale_y_continuous(limits=c(-0.7,0.7)) +
    labs(y = "difference in predicted hazard") +
    ggtitle("AMEs for a brand new\nhypothetical participant") +
    facet_wrap(~contrast)
```

```{r save-post, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M1_pid_new_AMEs.png", width = 10, height = 8, dpi = 800)
```

## Example conclusions for model M1.

What can we conclude from model M1 about our research question, i.e., what is the temporal dynamics of the effect of prime-target congruency on RT? In other words, in which of the 40-ms time bins between 200 and 600 ms after target onset does changing the prime from blank to congruent or incongruent affect the hazard of response occurrence (for a prime-target SOA of xxx ms)?

We can base our conclusion on the grand AMEs of congruent and incongruent primes in each bin. The contrast "congruent minus blank" was estimated to be 0.0925 hazard units in bin 6 (95% CrI = [0.0411, 0.183]), and 0.152 hazard units in bin 7 (95% CrI = [0.0489, 0.262]). For the other bins, the 95% credible interval contained zero.
The contrast "incongruent minus blank" was estimated to be 0.0943 hazard units in bin 6 (95% CrI = [0.0322, 0.208]), -0.204 hazard units in bin 9 (95% CrI = [-0.331, -0.0769]), -0.275 hazard units in bin 10 (95% CrI = [-0.45, -0.0513]), and -0.213 hazard units in bin 11 (95% CrI = [-0.377, -0.0137]). For the other bins, the 95% credible interval contained zero.

There are thus two phases of performance for the average subject between 200 and 600 ms after target onset. In the first phase, the addition of a congruent or incongruent prime stimulus increases the hazard of response occurrence compared to blank prime trials in the time period (200, 240]. In the second phase, only the incongruent prime decreases the hazard of response occurrence compared to blank primes, in the time period (320,440]. The effect of an incongruent prime thus depends on how much waiting time has passed since target onset.

The posterior distribution of each contrast can also be summarized by considering its proportion below or above some value, like zero.
For example, here are the proportions that each contrast is larger and smaller than 0:

```{r}
pabove <- epreds_grand_diffs %>% group_by(timebin,contrast) %>%
    summarize(prop_above = mean(pred_haz > 0)) %>% arrange(contrast,timebin)
pbelow <- epreds_grand_diffs %>% group_by(timebin,contrast) %>%
    summarize(prop_below = mean(pred_haz < 0)) %>% arrange(contrast,timebin)
pabove %>% inner_join(pbelow, by = c("timebin","contrast"))
```

Thus the probability that the contrast "congruent minus blank" is larger than 0, is larger than .80 in bins 6 to 9. And the probability that the contrast "incongruent minus blank" is smaller than 0, is larger than .80 in bins 8 to 13.

#####################

### Prepare data for M1

```{r data-M1}
# remove unnecessary columns before fitting a model
M1_data <- ptb_data %>% select(-c(bl,tr,trial,period, period_9,d9)) # 12840 obs. 
head(M1_data)
summary(M1_data)
```

### Set up priors for M1

```{r priors-cloglog-M1}
priors_M1 <- c(
  set_prior("skew_normal(-0.2,0.71,-2.2)", class = "b"),       # weakly informative prior for beta parameters when using cloglog link
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d6"),# weakly informative prior for alpha parameters when using cloglog link 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d7"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d8"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d10"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d11"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d12"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d13"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d14"), 
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "d15"),
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "Intercept"),
  set_prior("normal(0, 1)", class = "sd"),                     # prior for standard deviation of random effects
  set_prior("lkj(2)", class = "cor")                           # prior for correlations between random effects
)
```

### Fit model M1.

```{r fit-model-M1, eval=F}
plan(multicore)

model_M1 <-
   brm(data = M1_data,
       family = binomial(link="cloglog"),
       event | trials(1) ~ 0 + d6 + d7 + d8 + Intercept + d10 + d11 + d12 + d13 + d14 + d15 + 
         condition + trial_c +
       
                   (d6 + d7 + d8 + 1 + d10 + d11 + d12 + d13 + d14 + d15 + condition + trial_c | pid),
       prior = priors_M1,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M1")
```

Model_M1 took about 70 minutes on a MacBook Pro (Sonoma 14.6.1 OS, 18GB Memory, M3 Pro Chip).

```{r check-model-M1, eval=F}
model_M1 <- readRDS("Tutorial_2_Bayesian/models/model_M1.rds")
summary(model_M1)
fixef(model_M1)
```

## Model M2: third-order polynomical specification of TIME in the baseline condition (blank prime), and main effects of prime types, and trial number.

### Prepare data for M2.

```{r data-M2}
# remove unnecessary columns before fitting a model
M2_data <- ptb_data %>% select(-c(bl,tr,trial,period, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15)) # 12840 obs. of 14 variables
head(M2_data)
```

### Set up priors for M2.

```{r priors-cloglog-M2}
priors_M2 <- c(
  set_prior("skew_normal(-0.2,0.71,-2.2)", class = "b"),       # weakly informative prior for beta parameters when using cloglog link
  set_prior("skew_normal(-1,1,-2)", class = "b", coef = "Intercept"),      # weakly informative prior for Intercept when using cloglog link
  set_prior("normal(0, 1)", class = "sd"),                     # prior for standard deviation of random effects
  set_prior("lkj(2)", class = "cor")                           # prior for correlations between random effects
)
```

### Fit model M2.

```{r fit-model-M2, eval=F}
plan(multicore)

model_M2 <-
   brm(data = M2_data,
       family = binomial(link="cloglog"),
       event | trials(1) ~ 0 + Intercept + period_9 + I(period_9^2) + I(period_9^3) + 
                          condition + trial_c +
                          (1 + period_9 + I(period_9^2) + I(period_9^3) + 
                          condition + trial_c | pid),
       prior = priors_M2,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M2")
```

Model_M2 took about 144 minutes to run.

```{r check-model-M2, eval=F}
model_M2 <- readRDS("Tutorial_2_Bayesian/models/model_M2.rds")
summary(model_M2)
fixef(model_M2)
```

## Model M3: third-order polynomical specification of TIME in the baseline condition (blank prime), and relax proportionality assumption for prime types, and trial number.

### Prepare data for M3.

```{r data-M3}
# remove unnecessary columns before fitting a model
M3_data <- ptb_data %>% select(-c(bl,tr,trial,period, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15)) # 12840 obs. of 14 variables
head(M3_data)
```

### Set up priors for M3.

```{r priors-cloglog-M3}
priors_M3 <- c(
  set_prior("skew_normal(-0.2,0.71,-2.2)", class = "b"),       # weakly informative prior for beta parameters when using cloglog link
  set_prior("skew_normal(-1,1,-2)", class = "b", coef="Intercept"),      # weakly informative prior for Intercept when using cloglog link
  set_prior("normal(0, 1)", class = "sd"),                     # for standard deviation of random effects
  set_prior("lkj(2)", class = "cor")                           # for correlations between random effects
)
```

### Fit model M3.

```{r fit-model-M3, eval=F}
plan(multicore)

model_M3 <- 
   brm(data = M3_data,
       family = binomial(link="cloglog"),
       event | trials(1) ~ 0 + Intercept + # Note that duplicate terms in the model formula are ignored
                           condition*period_9 + 
                           condition*I(period_9^2) + 
                           condition*I(period_9^3) +
                           trial_c*period_9 + 
                           trial_c*I(period_9^2) + 
                           trial_c*I(period_9^3) +
                           (1 + condition*period_9 +
                           condition*I(period_9^2) +
                           condition*I(period_9^3) +
                           trial_c*period_9 + 
                           trial_c*I(period_9^2) + 
                           trial_c*I(period_9^3) | pid),
       prior = priors_M3,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M3")
```

Model_M3 took about 268 minutes to run.

```{r check-model-M3, eval=F}
model_M3 <- readRDS("Tutorial_2_Bayesian/models/model_M3.rds")

summary(model_M3)
```

## Model M4: third-order polynomical specification of TIME in the baseline condition (blank prime), and relax all assumptions for prime types, and trial number.

### Prepare data for M4.

```{r data-M4}
# remove unnecessary columns before fitting a model
M4_data <- ptb_data %>% select(-c(bl,tr,trial,period, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15)) # 12840 obs. of 14 variables
head(M4_data)
```

### Set up priors for M4.

```{r priors-cloglog-M4}
priors_M4 <- c(
  set_prior("skew_normal(-0.2,0.71,-2.2)", class = "b"),       # weakly informative prior for beta parameters when using cloglog link
  set_prior("skew_normal(-1,1,-2)", class = "b", coef="Intercept"),      # weakly informative prior for Intercept when using cloglog link
  set_prior("normal(0, 1)", class = "sd"),                     # for standard deviation of random effects
  set_prior("lkj(2)", class = "cor")                           # for correlations between random effects
)
```

### Fit model M4.

```{r fit-model-M4, eval=F}
plan(multicore)

model_M4 <- 
   brm(data = M4_data,
       family = binomial(link="cloglog"),
       event | trials(1) ~ 0 + Intercept + # Note that duplicate terms in the model formula are ignored
                          condition*period_9*trial_c + 
                          condition*period_9*I(trial_c^2) + 
                          condition*I(period_9^2)*trial_c +
                          condition*I(period_9^2)*I(trial_c^2) +
                          condition*I(period_9^3) +
                          trial_c*I(period_9^3) +
                          (1 +  condition*period_9*trial_c +
                                condition*period_9*I(trial_c^2) + 
                                condition*I(period_9^2)*trial_c +
                                condition*I(period_9^2)*I(trial_c^2)  +
                                condition*I(period_9^3) +
                                trial_c*I(period_9^3) | pid),
       prior = priors_M4,
       chains = 4, cores = 4, iter = 3000, warmup = 1000,
       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
       seed = 12, init = "0",
       file = "Tutorial_2_Bayesian/models/model_M4")
```

Model_M4 took about 8 hours to run.

```{r check-model-M4, eval=F}
model_M4 <- readRDS("Tutorial_2_Bayesian/models/model_M4.rds")
summary(model_M4)
```

# 6. Compare models using loo and waic.

```{r compare-models}
model_M1 <- readRDS("Tutorial_2_Bayesian/models/model_M1.rds")
model_M2 <- readRDS("Tutorial_2_Bayesian/models/model_M2.rds")
model_M3 <- readRDS("Tutorial_2_Bayesian/models/model_M3.rds")
model_M4 <- readRDS("Tutorial_2_Bayesian/models/model_M4.rds")
```

Using WAIC and LOO for comparing nonnested models.

```{r add-criterion, eval=F}
model_M1  <- add_criterion(model_M1, c("loo", "waic"))
model_M2  <- add_criterion(model_M2, c("loo", "waic"))
model_M3  <- add_criterion(model_M3, c("loo", "waic"))
model_M4  <- add_criterion(model_M4, c("loo", "waic"))
```

Compare all three models.

```{r loo-compare}
loo_compare(model_M1, model_M2, model_M3, model_M4, criterion = "loo") %>% print(simplify = F)
loo_compare(model_M1, model_M2, model_M3, model_M4, criterion = "waic") %>% print(simplify = F)
```

```{r model-weights}
model_weights(model_M1, model_M2, model_M3, model_M4, weights = "loo") %>% round(digits = 3)

model_weights(model_M1, model_M2, model_M3, model_M4, weights = "waic") %>% round(digits = 3)
```

# 7. Display effects for the selected model (model_M4).

## Pareto k estimates.

```{r pareto}
loo(model_M4)$diagnostics %>% 
  data.frame() %>% 
  # attach the `id` values
  bind_cols(M4_data) %>% 
  mutate(id = 1:n()) %>%
  
  ggplot(aes(x = id, y = pareto_k)) +
  geom_point(alpha = 3/4) + 
  geom_text(data = . %>% filter(pareto_k > .2),
            aes(x = id + 2, label = id),
            size = 3, hjust = 0) +
  theme(panel.grid = element_blank())
```

## Visualize posterior distributions of the effects of congruent and incongruent primes on cloglog-hazard relative to blank prime, for each time bin in trials 500, 1000, and 1500.

```{r post-distr}
#get_variables(model_M4)[1:31]

post <-as_draws_df(model_M4) %>% # 8000 draws x 715 variables
   select(starts_with("b_")) %>%       # 8000 x 31
   expand_grid(period_9 = -3:6) %>%   
   mutate(period_9sq = period_9^2,
          period_9cu = period_9^3,
          trial500 = -500/1000,
          trial500sq = trial500^2,
          trial1500 = 500/1000,
          trial1500sq = trial1500^2)  

# effects for trials 500, 1000, and 1500
effects <- post %>% 
  mutate(`congruent trial 1500` = b_conditioncongruent + period_9 * `b_conditioncongruent:period_9` + 
            period_9sq * `b_conditioncongruent:Iperiod_9E2` + period_9cu * `b_conditioncongruent:Iperiod_9E3` +
            trial1500 * `b_conditioncongruent:trial_c` + trial1500sq * `b_conditioncongruent:Itrial_cE2` +
            period_9 * trial1500 * `b_conditioncongruent:period_9:trial_c` +
            period_9sq * trial1500 * `b_conditioncongruent:Iperiod_9E2:trial_c` +
            period_9 * trial1500sq * `b_conditioncongruent:period_9:Itrial_cE2` +
            period_9sq * trial1500sq * `b_conditioncongruent:Iperiod_9E2:Itrial_cE2`, 
         
         `incongruent trial 1500` = b_conditionincongruent + period_9 * `b_conditionincongruent:period_9` + 
            period_9sq * `b_conditionincongruent:Iperiod_9E2` + period_9cu * `b_conditionincongruent:Iperiod_9E3` +
            trial1500 * `b_conditionincongruent:trial_c` + trial1500sq * `b_conditionincongruent:Itrial_cE2` +
            period_9 * trial1500 * `b_conditionincongruent:period_9:trial_c` +
            period_9sq * trial1500 * `b_conditionincongruent:Iperiod_9E2:trial_c` +
            period_9 * trial1500sq * `b_conditionincongruent:period_9:Itrial_cE2` +
            period_9sq * trial1500sq * `b_conditionincongruent:Iperiod_9E2:Itrial_cE2`,
    
         `congruent trial 500` = b_conditioncongruent + period_9 * `b_conditioncongruent:period_9` + 
            period_9sq * `b_conditioncongruent:Iperiod_9E2` + period_9cu * `b_conditioncongruent:Iperiod_9E3` +
            trial500 * `b_conditioncongruent:trial_c` + trial500sq * `b_conditioncongruent:Itrial_cE2` +
            period_9 * trial500 * `b_conditioncongruent:period_9:trial_c` +
            period_9sq * trial500 * `b_conditioncongruent:Iperiod_9E2:trial_c` +
            period_9 * trial500sq * `b_conditioncongruent:period_9:Itrial_cE2` +
            period_9sq * trial500sq * `b_conditioncongruent:Iperiod_9E2:Itrial_cE2`, 
         
         `incongruent trial 500` = b_conditionincongruent + period_9 * `b_conditionincongruent:period_9` +
            period_9sq * `b_conditionincongruent:Iperiod_9E2` + period_9cu * `b_conditionincongruent:Iperiod_9E3` +
            trial500 * `b_conditionincongruent:trial_c` + trial500sq * `b_conditionincongruent:Itrial_cE2` +
            period_9 * trial500 * `b_conditionincongruent:period_9:trial_c` +
            period_9sq * trial500 * `b_conditionincongruent:Iperiod_9E2:trial_c` +
            period_9 * trial500sq * `b_conditionincongruent:period_9:Itrial_cE2` +
            period_9sq * trial500sq * `b_conditionincongruent:Iperiod_9E2:Itrial_cE2`, 
    
         `congruent trial 1000` = b_conditioncongruent + period_9 * `b_conditioncongruent:period_9` + 
            period_9sq * `b_conditioncongruent:Iperiod_9E2` + period_9cu * `b_conditioncongruent:Iperiod_9E3`,
          
         `incongruent trial 1000` = b_conditionincongruent + period_9 * `b_conditionincongruent:period_9` +
            period_9sq * `b_conditionincongruent:Iperiod_9E2` + period_9cu * `b_conditionincongruent:Iperiod_9E3`) 
```

```{r plot-effects}
plot_facet <- effects %>%
  pivot_longer(cols=`congruent trial 1500`:`incongruent trial 1000`, names_to = "condition" ) %>%
  select(condition, period_9, value) %>%
  group_by(period_9) %>%
  
  ggplot(aes(x = period_9, y = value)) +
  stat_lineribbon(point_interval = mean_hdi,.width = c(0.5, 0.8, 0.95),show.legend=T) +
  scale_fill_brewer() +
  geom_hline(yintercept=0, linetype="dashed", color = "red") +
  scale_x_continuous(breaks = c(-3:6), labels=c(((-3:6)+9)*40),
                     limits = c(-3,6)) +
  scale_y_continuous(breaks = c(-12,-10,-8,-6,-4,-2,0,2,4,6,8,10,12), limits = c(-12,12)) +
  labs(x = "time bin endpoint", y = "cloglog-hazard") +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle=90),
        panel.background = element_rect(fill="white", color="black"),
        legend.position = "top") +
  facet_wrap(~ factor(condition,levels=c("congruent trial 500","congruent trial 1000","congruent trial 1500","incongruent trial 500","incongruent trial 1000","incongruent trial 1500")))
  
plot_facet
```

```{r save-plot-effects, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M4effects_con_incon_3trials.png", width = 8, height = 8, dpi = 300)
```

## calculate point and interval estimates, and hazard ratios.

```{r table-interval-estimates}
int_c_500 <- effects %>%
  select(`congruent trial 500`, period_9) %>%
  group_by(period_9) %>%
  mean_hdi() %>%
  mutate(bin_endpoint = (period_9 + 9)*40,
        `hazard ratio` = exp(`congruent trial 500`),
         mean = `congruent trial 500`,
         condition = "c500") %>%
  select(bin_endpoint,condition, mean, .lower, .upper, .width, `hazard ratio`)

int_i_500 <- effects %>%
  select(`incongruent trial 500`, period_9) %>%
  group_by(period_9) %>%
  mean_hdi() %>%
  mutate(bin_endpoint = (period_9 + 9)*40,
        `hazard ratio` = exp(`incongruent trial 500`),
         mean = `incongruent trial 500`,
         condition = "i500") %>%
  select(bin_endpoint, condition,mean, .lower, .upper, .width, `hazard ratio`)

int_c_1000 <- effects %>%
  select(`congruent trial 1000`, period_9) %>%
  group_by(period_9) %>%
  mean_hdi() %>%
  mutate(bin_endpoint = (period_9 + 9)*40,
        `hazard ratio` = exp(`congruent trial 1000`),
         mean = `congruent trial 1000`,
         condition = "c1000" ) %>%
  select(bin_endpoint,condition,mean, .lower, .upper, .width, `hazard ratio`)

int_i_1000 <- effects %>%
  select(`incongruent trial 1000`, period_9) %>%
  group_by(period_9) %>%
  mean_hdi() %>%
  mutate(bin_endpoint = (period_9 + 9)*40,
        `hazard ratio` = exp(`incongruent trial 1000`),
         mean = `incongruent trial 1000`,
         condition = "i1000") %>%
  select(bin_endpoint, condition,mean, .lower, .upper, .width,`hazard ratio`)

int_c_1500 <- effects %>%
  select(`congruent trial 1500`, period_9) %>%
  group_by(period_9) %>%
  mean_hdi() %>%
  mutate(bin_endpoint = (period_9 + 9)*40,
        `hazard ratio` = exp(`congruent trial 1500`),
         mean = `congruent trial 1500`,
         condition = "c1500" ) %>%
  select(bin_endpoint, condition, mean, .lower, .upper, .width, `hazard ratio`)

int_i_1500 <- effects %>%
  select(`incongruent trial 1500`, period_9) %>%
  group_by(period_9) %>%
  mean_hdi() %>%
  mutate(bin_endpoint = (period_9 + 9)*40,
        `hazard ratio` = exp(`incongruent trial 1500`),
         mean = `incongruent trial 1500`,
         condition = "i1500") %>%
  select(bin_endpoint, condition,mean, .lower, .upper, .width,  `hazard ratio`)

test <- rbind(int_c_500, int_c_1000, int_c_1500,int_i_500,int_i_1000, int_i_1500)
test %>% print(n=60)
```

```{r save-table-interval-estimates, eval=F}
write_csv(test, file="Tutorial_2_Bayesian/tables/effects_intervals_table.csv")
```

## Plot model-based predicted cloglog-hazard and hazard functions, for a single subject.

```{r select-pid}
selected_subject = 6 # select pid here (1:6) manually
```

```{r help-function}
make_fitted <- function(fit, scale, ...) {
  
  fitted(fit,
         newdata = nd,
         scale = scale,
         ...) %>% 
    data.frame() %>% 
    bind_cols(nd)
}
```

### Set up new data.

```{r new-data}
# define the `newdata`
nd <- tibble(pid = 1:6) %>%
  expand_grid(incon = 0:1,
                con  = 0:1,
                trial_c = c(-0.5,0,0.5),
                period_9 = -3:6) %>%
  filter(!(con ==1 & incon == 1)) %>%
  mutate(Iperiod_9E2 = period_9^2,
         Iperiod_9E3 = period_9^3,
         Itrial_cE2 = trial_c^2,
         condition = ifelse(con==1 & incon==0,"congruent",
                     ifelse(con==0 & incon==1,"incongruent", "blank")),
         condition = factor(condition, levels=c("blank" ,"congruent","incongruent"))) 
```

### First, on the cloglog-scale.

```{r subject-cloglog}
make_fitted(model_M4, scale = "linear") %>% 
  mutate(cond   = rep(rep(c("N","C","I"),each=30),6),
         trial  = factor(trial_c,levels=c(-0.5,0.0,0.5),labels=c("trial 500","trial 1000","trial 1500")),
         period = period_9+9) %>% 
  filter(pid==selected_subject) %>% 
  # plot
  ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5,
             fill = condition, color = condition)) +
  geom_ribbon(alpha = 1/5, linewidth = 0) +
  geom_line() +
  scale_fill_viridis_d(NULL, option = "A", end = .6, direction = -1) +
  scale_color_viridis_d(NULL, option = "A", end = .6, direction = -1) +
  scale_x_continuous("time bin endpoint", breaks = 6:15, limits = c(6, 15), labels=c(6:15)*40) +
  ylab("fitted cloglog(hazard)") +
  coord_cartesian(ylim = c(-5, 1)) +
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill="white",color="black"),
        legend.position = "top",
        axis.text.x = element_text(angle=90)) +
  facet_wrap(~ trial)
```

### Second, on hazard scale.

```{r subject-hazard}
make_fitted(model_M4, scale = "response") %>% 
  mutate(cond   = rep(rep(c("N","C","I"),each=30),6),
         trial  = factor(trial_c,levels=c(-0.5,0.0,0.5),labels=c("trial 500","trial 1000","trial 1500")),
         period = period_9+9) %>% 
  filter(pid==selected_subject) %>% 
  # plot
  ggplot(aes(x = period, y = Estimate, ymin = Q2.5, ymax = Q97.5,
             fill = condition, color = condition)) +
  geom_ribbon(alpha = 1/5, linewidth = 0) +
  geom_line() +
  scale_fill_viridis_d(NULL, option = "A", end = .6, direction = -1) +
  scale_color_viridis_d(NULL, option = "A", end = .6, direction = -1) +
  scale_x_continuous("time bin endpoint", breaks = 6:15, limits = c(6, 15), labels=c(6:15)*40) +
  ylab("fitted hazard") +
  coord_cartesian(ylim = c(0, 1)) +
  theme(panel.grid = element_blank(),
        panel.background = element_rect(fill="white",color="black"),
        legend.position = "top",
        axis.text.x = element_text(angle=90)) +
  facet_wrap(~ trial)
```

```{r save-plot-fit-hazard, eval=F}
ggsave("Tutorial_2_Bayesian/figures/M4effects_subject6.png", width = 10, height = 10, dpi = 300)
```

# 3. Plot the logit and complementary log-log (cloglog) link functions.

```{r plot-links}
probability <- (1:99999)/100000
logistic <- function(x) { return( 1/(1+exp(-1*x)) )}
logit    <- function(x) { return( log(x/(1-x)) )}
inverse_cloglog <- function(x) { return( 1-(exp(-1*exp(x))) )}
cloglog         <- function(x) { return( log(-1*log(1-x)) )}

cloglog_prob <- cloglog(probability)
logit_prob <- logit(probability)
dataplot <- cbind(probability,cloglog_prob,logit_prob)

ggplot() +
  geom_hline(yintercept=0, color="white") +
  geom_line(data=dataplot,aes(y=logit_prob,x=probability,colour="logit"),linewidth=1) +
  geom_line(data=dataplot,aes(y=cloglog_prob,x=probability,colour="cloglog"),linewidth=1) +
  scale_color_manual(name = "Link function:", values = c("logit" = "darkblue", "cloglog" = "red")) +
  geom_vline(xintercept = logistic(0), linetype="dotted", linewidth = 0.3) +   
  geom_vline(xintercept = inverse_cloglog(0), linetype="dotted", linewidth = 0.3) +
  annotate("text", x = logistic(0)-.02, y = -6, label = "logistic(0) = 0.5", angle = 90, size=4) +
  annotate("text", x = inverse_cloglog(0)+.02, y = -6, label = "inverse_cloglog(0) = 0.6321", angle=90,size=4) +
  scale_x_continuous(n.breaks=10, limits = c(0,1), ) +
  labs(x = "Probability",
        y = "logit or cloglog scale") +
  theme(panel.grid = element_blank(),
        axis.text=element_text(size=16),
        axis.title=element_text(size=18),
      #  legend.position="top",
        legend.text = element_text(size=18),
      legend.position = "inside",
        legend.position.inside=c(.2,.75),
      legend.background = element_rect(fill="white") )
```

```{r save-plot-links, eval=F}
ggsave("Tutorial_2_Bayesian/figures/linkfunctions.png", width = 8, height = 8, dpi = 300)
```

# 4. Visualize different prior distributions on the logit and cloglog scales.

To gain a sense of what prior logit values would approximate a uniform distribution on the probability (i.e., discrete-time hazard) scale, Solomon Kurz simulated a large number of draws from the Uniform(0,1) distribution, converted those draws to the log-odds metric, and fitted a Student's t model.
Here we do the same for prior cloglog values: simulate a large number of draws from U(0,1), convert them to the cloglog metric, and fit a skew-normal model (due to the asymmetry of the cloglog link function), to gain a sense of what prior cloglog values would approximate a uniform distribution on the probability (i.e., discrete-time hazard) scale.

## Simulate, convert, and fit.

```{r simulate-convert}
set.seed(11)

logit    <- function(x) { return( log(x/(1-x)) )}
cloglog  <- function(x) { return( log(-1*log(1-x)) )}

# generate draws from U(0,1) and convert
dat <- 
  tibble(p = runif(1e6, 0, 1)) %>% 
  mutate(g = logit(p),
         c = cloglog(p)) 
# display
dat %>%   
  ggplot(aes(x = c)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

```{r fit-skew-normal, eval=F}
# fit model
fit_skewN <-
  brm(data = dat,
      family = skew_normal(),
      c ~ 1,
      chains = 4, cores = 4,
      file = "Tutorial_2_Bayesian/models/fit_skewN")
```

```{r}
fit_skewN <- readRDS("Tutorial_2_Bayesian/models/fit_skewN.rds")
summary(fit_skewN) 
```

Now we can reverse the process. We simulate from the skew-Normal distribution based on the posterior means for mu, sigma, and alpha, and then convert the results into the probability (i.e., discrete-time hazard) metric. 

```{r check-results}
set.seed(11)

inverse_cloglog <- function(x) { return( 1-(exp(-1*exp(x))) )}

tibble(c = rskew_normal(1e6, mu=-0.59, sigma = 1.26, alpha = -4.22) ) %>% 
  mutate(p = inverse_cloglog(c)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

## Visualize how seven prior distributions on the logit and/or cloglog scales look on the probability scale.

```{r plot-priors}
logistic <- function(x) { return( 1/(1+exp(-1*x)) )}
logit    <- function(x) { return( log(x/(1-x)) )}
inverse_cloglog <- function(x) { return( 1-(exp(-1*exp(x))) )}
cloglog    <- function(x) { return( log(-1*log(1-x)) )}

set.seed(23)

# A N(0,4) prior on the logit and cloglog scales pushes mass to probabilities of 0 and 1
pr1 <- tibble(prior = rnorm(1e6, mean = 0, sd = 4)) %>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-13, y=60000, label="N(0,4)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'A', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l1 <- tibble(log_odds = rnorm(1e6, mean = 0, sd = 4)) %>%  
  mutate(p = logistic(log_odds)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("logistic(prior)") +
  theme(panel.grid = element_blank())

c1 <- tibble(cloglog_prob = rnorm(1e6, mean = 0, sd = 4)) %>% 
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("inverse_cloglog(prior)")+
  theme(panel.grid = element_blank())

# A N(0,2) prior on the logit and cloglog scales pushes mass to probabilities of 0 and/or 1
pr2 <- tibble(prior = rnorm(1e6, mean = 0, sd = 2)) %>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-6, y=60000, label="N(0,2)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'B', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l2 <- tibble(log_odds = rnorm(1e6, mean = 0, sd = 2)) %>%  
  mutate(p = logistic(log_odds)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

c2 <- tibble(cloglog_prob = rnorm(1e6, mean = 0, sd = 2)) %>% 
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

# A student-t(df=7.61) prior with scale 1.57 on the logit scale approximates a uniform distribution on the probability scale. This might be a good prior to use for the alpha parameters or Intercept in a logit-hazard model.
pr3 <- tibble(prior = rt(1e6, df = 7.61)* 1.57) %>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-12, y=140000, label="t(7.61,0,1.57)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'C', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l3 <- tibble(log_odds = rt(1e6, df = 7.61)* 1.57) %>%  
  mutate(p = logistic(log_odds)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

c3 <- tibble(cloglog_prob = rt(1e6, df = 7.61)* 1.57) %>% 
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

# A Normal(0,1) prior on the logit scale gently regularizes p towards .5. This might be a good prior to use for the beta parameters in a logit-hazard model.
pr4 <- tibble(prior = rnorm(1e6, mean = 0, sd = 1))%>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-3, y=60000, label="N(0,1)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'D', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l4 <- tibble(log_odds = rnorm(1e6, mean = 0, sd = 1))%>%  
  mutate(p = logistic(log_odds)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  geom_vline(xintercept=logistic(0), color="red") +
  theme(panel.grid = element_blank())

c4 <- tibble(cloglog_prob = rnorm(1e6, mean = 0, sd = 1))%>%  
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
 theme(panel.grid = element_blank())

# A skew_Normal(-0.59,1.26,-4.22) prior on the cloglog scale approxiates a uniform distr. on the hazard scale. This uninformative prior might be good for the alpha parameters or Intercept in a cloglog-hazard model.
pr5 <- tibble(prior = rskew_normal(1e6, mu=-0.59, sigma = 1.26, alpha = -4.22)) %>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-5.5, y=55000, label="skew_N(-0.59,1.26,-4.22)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'E', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l5 <- ggplot()

c5 <- tibble(cloglog_prob = rskew_normal(1e6, mu=-0.59, sigma = 1.26, alpha = -4.20)) %>% 
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

# The skew_Normal(-1,1,-2) on the cloglog scale is a weakly informative prior for the alpha parameters or Intercept in a cloglog-hazard model because hazard values below .5 more likely than values above .5 in general.
pr6 <- tibble(prior = rskew_normal(1e6, mu=-1, sigma = 1, alpha = -2)) %>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-5, y=55000, label="skew_N(-1,1,-2)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'F', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l6 <- ggplot()

c6 <- tibble(cloglog_prob = rskew_normal(1e6, mu=-1, sigma = 1, alpha = -2)) %>% 
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

# A skew_Normal(-0.2,0.71,-2.2) on the cloglog scale gently regularizes p towards .6321. This might be a good prior to use for the beta parameters in a cloglog-hazard model.
pr7 <- tibble(prior = rskew_normal(1e6, mu=-0.2, sigma = .71, alpha = -2.2)) %>%  
  ggplot(aes(x = prior)) +
  geom_histogram(bins = 50) +
  scale_y_continuous(NULL, breaks = NULL) +
  annotate(geom="text", x=-3.2, y=55000, label="skew_N(-0.2,.71,-2.2)",
              color="red", size=6) +
  annotate(geom = 'text', label = 'G', x = -Inf, y = Inf, hjust = 0, vjust = 1, size=8)+
  theme(panel.grid = element_blank())

l7 <- ggplot()

c7 <- tibble(cloglog_prob = rskew_normal(1e6, mu=-0.2, sigma = .71, alpha = -2.2)) %>% 
  mutate(p = inverse_cloglog(cloglog_prob)) %>% 
  ggplot(aes(x = p)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept=inverse_cloglog(0), color = "red") +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())

(l1 + pr1 + c1) / (l2 + pr2 + c2) / (l3 + pr3 + c3) / (l4 + pr4 + c4)/ (l5 + pr5 + c5) / (l6 + pr6 + c6) / (l7 + pr7 + c7)
```

```{r save-plot-priors, eval=F}
ggsave("Tutorial_2_Bayesian/figures/plot_of_priors.png", width=14, height=13,dpi=300)
```

