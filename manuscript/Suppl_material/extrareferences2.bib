@article{ABNEY2025,
  title = {Advancing a Temporal Science of Behavior},
  author = {Abney, Drew H. and Fausey, Caitlin M. and {Suarez-Rivera}, Catalina and {Tamis-LeMonda}, Catherine S.},
  year = {2025},
  journal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2025.05.010},
  abstract = {All events unfold over time, and the temporal parameters of events matter for cognition. Yet it is common for scholars across disciplines to summarize events using atemporal statistics. Here, we underscore the urgency of illuminating the temporal structure of behavior streams and testing implications for learning. We review evidence on the importance of timing for cognition, drawing on our expertise in developmental science. We provide a framework for the quantification of single behavior streams, coordination between multiple streams, and the organization of streams across extended and multiple timescales. We highlight opportunities for methodological, analytic, and theoretical innovation to advance a temporal science of behavior. Parameterizing the temporal structure of events will accelerate scientific progress on human, animal, and artificial learning systems.},
  keywords = {behavior,infancy,natural observation,time}
}

@article{allisonDiscreteTimeMethodsAnalysis1982,
  title = {Discrete-{{Time Methods}} for the {{Analysis}} of {{Event Histories}}},
  author = {Allison, Paul D.},
  year = {1982},
  journal = {Sociological Methodology},
  volume = {13},
  eprint = {270718},
  eprinttype = {jstor},
  pages = {61},
  issn = {00811750},
  doi = {10.2307/270718},
  urldate = {2024-06-05}
}

@book{allisonSurvivalAnalysisUsing2010,
  title = {Survival Analysis Using {{SAS}}: A Practical Guide},
  shorttitle = {Survival Analysis Using {{SAS}}},
  author = {Allison, Paul D.},
  year = {2010},
  edition = {2. ed},
  publisher = {SAS Press},
  address = {Cary, NC},
  isbn = {978-1-59994-640-5},
  langid = {english}
}

@article{bakerPowerContoursOptimising2021,
  title = {Power Contours: {{Optimising}} Sample Size and Precision in Experimental Psychology and Human Neuroscience.},
  shorttitle = {Power Contours},
  author = {Baker, Daniel H. and Vilidaite, Greta and Lygo, Freya A. and Smith, Anika K. and Flack, Tessa R. and Gouws, Andr{\'e} D. and Andrews, Timothy J.},
  year = {2021},
  month = jun,
  journal = {Psychological Methods},
  volume = {26},
  number = {3},
  pages = {295--314},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000337},
  urldate = {2024-07-22},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/QRN6ZIB4/Baker et al. - 2021 - Power contours Optimising sample size and precisi.pdf}
}

@article{barackTwoViewsCognitive2021,
  title = {Two Views on the Cognitive Brain},
  author = {Barack, David L. and Krakauer, John W.},
  year = {2021},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {22},
  number = {6},
  pages = {359--371},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-021-00448-6},
  urldate = {2025-06-25},
  langid = {english}
}

@article{barrRandomEffectsStructure2013,
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  journal = {Journal of memory and language},
  volume = {68},
  number = {3},
  pages = {10.1016/j.jml.2012.11.001},
  doi = {10.1016/j.jml.2012.11.001},
  urldate = {2024-10-25},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we ...},
  langid = {english},
  pmid = {24403724},
  file = {/Users/spanis/Zotero/storage/C9VQ3Y75/Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf}
}

@misc{BayesianPowerAnalysisa,
  title = {Bayesian Power Analysis: {{Part I}}. {{Prepare}} to Reject `{\textbackslash}({{H}}\_0{\textbackslash})` with Simulation.},
  shorttitle = {Bayesian Power Analysis},
  journal = {A. Solomon Kurz},
  urldate = {2024-12-08},
  abstract = {If you'd like to learn how to do Bayesian power calculations using **brms**, stick around for this multi-part blog series. Here with part I, we'll set the foundation.},
  howpublished = {https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/IQDE5EIQ/bayesian-power-analysis-part-i.html}
}

@article{bergerComparisonDifferentResponse2021,
  title = {Comparison of {{Different Response Time Outlier Exclusion Methods}}: {{A Simulation Study}}},
  shorttitle = {Comparison of {{Different Response Time Outlier Exclusion Methods}}},
  author = {Berger, Alexander and Kiefer, Markus},
  year = {2021},
  month = jun,
  journal = {Frontiers in Psychology},
  volume = {12},
  pages = {675558},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.675558},
  urldate = {2025-06-25},
  abstract = {In response time (RT) research, RT outliers are typically excluded from statistical analysis to improve the signal-to-noise ratio. Nevertheless, there exist several methods for outlier exclusion. This poses the question, how these methods differ with respect to recovering the uncontaminated RT distribution. In the present simulation study, two RT distributions with a given population difference were simulated in each iteration. RTs were replaced by outliers following two different approaches. The first approach generated outliers at the tails of the distribution, the second one inserted outliers overlapping with the genuine RT distribution. We applied ten different outlier exclusion methods and tested, how many pairs of distributions significantly differed. Outlier exclusion methods were compared in terms of bias. Bias was defined as the deviation of the proportion of significant differences after outlier exclusion from the proportion of significant differences in the uncontaminated samples (before introducing outliers). Our results showed large differences in bias between the exclusion methods. Some methods showed a high rate of Type-I errors and should therefore clearly not be used. Overall, our results showed that applying an exclusion method based on z-scores / standard deviations introduced only small biases, while the absence of outlier exclusion showed the largest absolute bias.},
  file = {/Users/spanis/Zotero/storage/P4S7AGT7/Berger and Kiefer - 2021 - Comparison of Different Response Time Outlier Exclusion Methods A Simulation Study.pdf}
}

@book{blossfeldTechniquesEventHistory2002,
  title = {Techniques of Event History Modeling: {{New}} Approaches to Causal Analysis, 2nd Ed},
  shorttitle = {Techniques of Event History Modeling},
  author = {Blossfeld, Hans-Peter and Rohwer, G{\"o}tz},
  year = {2002},
  series = {Techniques of Event History Modeling: {{New}} Approaches to Causal Analysis, 2nd Ed},
  pages = {x, 310},
  publisher = {Lawrence Erlbaum Associates Publishers},
  address = {Mahwah, NJ, US},
  abstract = {This 2nd edition book gives an updated introductory account of event history modeling techniques. The specific emphasis is still on their usefulness for causal analysis in the social sciences. The book is intended to introduce the reader to the application of continuous-time models. It is both a student textbook and a reference book for research scientists.  The book has 3 main goals: 1. to demonstrate that event history models are an extremely useful approach to uncover causal relationships or to map out a system of causal relations; 2. to introduce the reader to the computer program Transition Data Analysis (G. Rohwer and U. P{\"o}tter, 2000), which estimates the sorts of models most frequently used with longitudinal data, in particular, discrete-time and continuous-time event history data; and 3. to supplement the textbook Event History Analysis (H.-P. Blossfeld et al, 1989) and update the earlier edition of this book. It is a supplement because it substantially extends the practical application of event history analysis, and it is an update because it adds several important new models and concepts that have been developed in an extremely active research area since the late 1980s. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-8058-4090-2 978-0-8058-4091-9},
  keywords = {Biographical Data,Causal Analysis,Computer Software,Experiences (Events),History,Mathematical Modeling},
  file = {/Users/spanis/Zotero/storage/87LREF5D/2001-05108-000.html}
}

@article{bloxomEstimatingResponseTime1984,
  title = {Estimating Response Time Hazard Functions: {{An}} Exposition and Extension},
  shorttitle = {Estimating Response Time Hazard Functions},
  author = {Bloxom, Bruce},
  year = {1984},
  month = dec,
  journal = {Journal of Mathematical Psychology},
  volume = {28},
  number = {4},
  pages = {401--420},
  issn = {00222496},
  doi = {10.1016/0022-2496(84)90008-7},
  urldate = {2025-06-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{bolgerCausalProcessesPsychology2019,
  title = {Causal Processes in Psychology Are Heterogeneous.},
  author = {Bolger, Niall and Zee, Katherine S. and {Rossignac-Milon}, Maya and Hassin, Ran R.},
  year = {2019},
  month = apr,
  journal = {Journal of Experimental Psychology: General},
  volume = {148},
  number = {4},
  pages = {601--618},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000558},
  urldate = {2025-06-25},
  langid = {english}
}

@misc{box-steffensmeierEventHistoryModeling2004,
  title = {Event History Modeling: A Guide for Social Scientists},
  shorttitle = {Event History Modeling},
  author = {{Box-Steffensmeier}, Janet M.},
  year = {2004},
  journal = {Event history modeling : a guide for social scientists},
  publisher = {University Press},
  address = {Cambridge},
  abstract = {Provides a guide to event history analysis for researchers and advanced students in the social sciences. The foundational principles of event history analysis are discussed and ample examples are estimated and interpreted using standard statistical packages, such as STATA and S-Plus. Recent innovations in diagnostics are discussed, including testing the proportional hazards assumption, identifying outliers, and assessing model fit. Points out common problems in the analysis of time-to-event data in the social sciences and makes recommendations regarding the implementation of duration modeling methods.},
  collaborator = {Jones, Bradford S.},
  isbn = {9780521546737},
  langid = {english},
  lccn = {106A592},
  keywords = {data analysis,guide,model,simulation,social sciences,statistical method}
}

@misc{CRANMASSCitation,
  title = {{{CRAN}}: {{MASS}} Citation Info},
  urldate = {2024-10-24},
  howpublished = {https://cran.r-project.org/web/packages/MASS/citation.html}
}

@article{debruineUnderstandingMixedEffectsModels2021,
  title = {Understanding {{Mixed-Effects Models Through Data Simulation}}},
  author = {DeBruine, Lisa M. and Barr, Dale J.},
  year = {2021},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {4},
  number = {1},
  pages = {2515245920965119},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920965119},
  urldate = {2024-12-08},
  abstract = {Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed-effects models. However, much of this research is analyzed using analysis of variance on aggregated responses because researchers are not confident specifying and interpreting mixed-effects models. This Tutorial explains how to simulate data with random-effects structure and analyze the data using linear mixed-effects regression (with the lme4 R package), with a focus on interpreting the output in light of the simulated parameters. Data simulation not only can enhance understanding of how these models work, but also enables researchers to perform power calculations for complex designs. All materials associated with this article can be accessed at https://osf.io/3cz2e/.},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/UBC4QRWY/DeBruine and Barr - 2021 - Understanding Mixed-Effects Models Through Data Si.pdf}
}

@article{elmerModelingCategoricalTimetoevent2023,
  title = {Modeling Categorical Time-to-Event Data: {{The}} Example of Social Interaction Dynamics Captured with Event-Contingent Experience Sampling Methods.},
  shorttitle = {Modeling Categorical Time-to-Event Data},
  author = {Elmer, Timon and Van Duijn, Marijtje A. J. and Ram, Nilam and Bringmann, Laura F.},
  year = {2023},
  month = sep,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000598},
  urldate = {2025-06-25},
  langid = {english}
}

@misc{gelmanBayesianWorkflow2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  number = {arXiv:2011.01808},
  eprint = {2011.01808},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.01808},
  urldate = {2024-12-01},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/spanis/Zotero/storage/YP7U3GBN/Gelman et al. - 2020 - Bayesian Workflow.pdf;/Users/spanis/Zotero/storage/QIBT5FS3/2011.html}
}

@misc{gelmanRegressionOtherStories2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  month = jul,
  journal = {Higher Education from Cambridge University Press},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781139161879},
  urldate = {2024-11-06},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  howpublished = {https://www.cambridge.org/highereducation/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C},
  isbn = {9781139161879},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/AINWU8UF/DD20DD6C9057118581076E54E40C372C.html}
}

@article{gompertzNatureFunctionExpressive1997,
  title = {On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies. {{In}} a Letter to {{Francis Baily}}, {{Esq}}. {{F}}. {{R}}. {{S}}. \&c. {{By Benjamin Gompertz}}, {{Esq}}. {{F}}. {{R}}. {{S}}},
  author = {Gompertz, Benjamin},
  year = {1997},
  month = jan,
  journal = {Abstracts of the Papers Printed in the Philosophical Transactions of the Royal Society of London},
  volume = {2},
  pages = {252--253},
  publisher = {Royal Society},
  doi = {10.1098/rspl.1815.0271},
  urldate = {2024-10-24},
  abstract = {This paper, which professes to be a continuation of former researches on the same subject printed in the Transactions of the Royal Society, is divided into two chapters. In the first the author considers the nature of the law of those numbers in tables of mortality, which express the amount of persons living at the end of ages in regular arithmetical progression. He remarks that for short intervals the law approaches nearly to a decreasing geometrical progression, and that this must be the case whatever be the strict expression for the law of mortality, provided the intervals do not exceed certain limits. But he further remarks, that this property will be found to belong to very extensive portions of tables of mortality, and instances Deparcieux's tables, where from the age of 25 to that of 45, the numbers living at the end of each year decrease very nearly in geometrical progression. Considering however the whole extent of such a table, it will be found that the ratio of this geometrical progression is not the same in all parts of the table. But before he enters on this consideration, the author draws some consequences from the hypothesis of a geometrical progression being the strict law of nature after a certain age. One of these is the equality of value of all life annuities commencing after that age. Another is, that the want of instances in history of persons living to very enormous ages (waving those of the patriarchs,) is no proof that such may not be the law of nature, as he shows by calculation, that out of 3,000,000 persons of 92, not more than one should on this supposition reach 192. This leads him to some general considerations on the causes of death, after which he resumes the consideration of the general law of the tables.},
  file = {/Users/spanis/Zotero/storage/GKGG9PCX/Gompertz - 1997 - On the nature of the function expressive of the la.pdf}
}

@article{halleyVIEstimateDegrees1997,
  title = {{{VI}}. {{An}} Estimate of the Degrees of the Mortality of Mankind; Drawn from Curious Tables of the Births and Funerals at the City of {{Breslaw}}; with an Attempt to Ascertain the Price of Annuities upon Lives},
  author = {Halley, Edmond},
  year = {1997},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London},
  volume = {17},
  number = {196},
  pages = {596--610},
  publisher = {Royal Society},
  doi = {10.1098/rstl.1693.0007},
  urldate = {2024-12-01},
  abstract = {The Contemplation of the Mortality of Mankind, has besides the Moral, its Physical and Political Uses, both which have been some years since most judiciously considered by the curious Sir William Petty, in his Natural and Political Observations on the Bills of Mortality of London, owned by Captain John Graunt.},
  file = {/Users/spanis/Zotero/storage/2HD6FSBU/Halley - 1997 - VI. An estimate of the degrees of the mortality of.pdf}
}

@article{holdenDispersionResponseTimes2009,
  title = {Dispersion of Response Times Reveals Cognitive Dynamics},
  author = {Holden, John G. and Van Orden, Guy C. and Turvey, Michael T.},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {2},
  pages = {318--342},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/a0014849},
  abstract = {Trial-to-trial variation in word-pronunciation times exhibits 1/f scaling. One explanation is that human performances are consequent on multiplicative interactions among interdependent processes---interaction dominant dynamics. This article describes simulated distributions of pronunciation times in a further test for multiplicative interactions and interdependence. Individual participant distributions of {$\approx$}1,100 word-pronunciation times were successfully mimicked for each participant in combinations of lognormal and power-law behavior. Successful hazard function simulations generalized these results to establish interaction dominant dynamics, in contrast with component dominant dynamics, as a likely mechanism for cognitive activity. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Processes,Naming,Pronunciation,Reaction Time,Words (Phonetic Units)},
  file = {/Users/spanis/Zotero/storage/DA3AXND2/Holden et al. - 2009 - Dispersion of response times reveals cognitive dyn.pdf;/Users/spanis/Zotero/storage/H2XLXN8Z/doiLanding.html}
}

@book{hosmerAppliedSurvivalAnalysis2011,
  title = {Applied {{Survival Analysis}}: {{Regression Modeling}} of {{Time}} to {{Event Data}}},
  shorttitle = {Applied {{Survival Analysis}}},
  author = {Hosmer, David W. and Lemeshow, Stanley and May, Susanne},
  year = {2011},
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {2nd ed},
  number = {v.618},
  publisher = {John Wiley \& Sons},
  address = {Hoboken},
  abstract = {THE MOST PRACTICAL, UP-TO-DATE GUIDE TO MODELLING AND ANALYZING TIME-TO-EVENT DATA-NOW IN A VALUABLE NEW EDITION Since publication of the first edition nearly a decade ago, analyses using time-to-event methods have increase considerably in all areas of scientific inquiry mainly as a result of model-building methods available in modern statistical software packages. However, there has been minimal coverage in the available literature to9 guide researchers, practitioners, and students who wish to apply these methods to health-related areas of study. Applied Survival Analysis, Second Edition prov},
  isbn = {978-0-471-75499-2},
  langid = {english}
}

@article{inceBayesianInferencePopulation2021,
  title = {Bayesian Inference of Population Prevalence},
  author = {Ince, Robin Aa and Paton, Angus T. and Kay, Jim W. and Schyns, Philippe G.},
  year = {2021},
  month = oct,
  journal = {eLife},
  volume = {10},
  pages = {e62461},
  issn = {2050-084X},
  doi = {10.7554/eLife.62461},
  abstract = {Within neuroscience, psychology, and neuroimaging, the most frequently used statistical approach is null hypothesis significance testing (NHST) of the population mean. An alternative approach is to perform NHST within individual participants and then infer, from the proportion of participants showing an effect, the prevalence of that effect in the population. We propose a novel Bayesian method to estimate such population prevalence that offers several advantages over population mean NHST. This method provides a population-level inference that is currently missing from study designs with small participant numbers, such as in traditional psychophysics and in precision imaging. Bayesian prevalence delivers a quantitative population estimate with associated uncertainty instead of reducing an experiment to a binary inference. Bayesian prevalence is widely applicable to a broad range of studies in neuroscience, psychology, and neuroimaging. Its emphasis on detecting effects within individual participants can also help address replicability issues in these fields.},
  langid = {english},
  pmcid = {PMC8494477},
  pmid = {34612811},
  keywords = {Bayes Theorem,Biostatistics,Data Interpretation Statistical,generalisation,human,Humans,inference,neuroscience,Neurosciences,prevalence,Psychology,Research Design,statistics},
  file = {/Users/spanis/Zotero/storage/9ZL97L4L/Ince et al. - 2021 - Bayesian inference of population prevalence.pdf}
}

@article{kantowitzInterpretationReactionTime2021,
  title = {The {{Interpretation}} of {{Reaction Time}} in {{Information-Processing Research}} 1},
  author = {Kantowitz, Barry H. and Pachella, Robert G.},
  year = {2021},
  month = oct,
  journal = {Human Information Processing},
  edition = {1},
  pages = {41--82},
  publisher = {Routledge},
  address = {London},
  doi = {10.4324/9781003176688-2},
  urldate = {2024-07-22},
  abstract = {Abstract : The report discusses the use of reaction time measures in modern experimental psychology. Methodological and theoretical issues are raised concerning the logic of experimentation in which reaction time is the major dependent variable and the limitations of interpretation of reaction time in the presence of variable error rates. The relationship between the speed and the accuracy of performance and theoretical models underlying this relation are also discussed.},
  isbn = {9781003176688},
  langid = {english}
}

@article{kelsoOutlineGeneralTheory2013,
  title = {Outline of a General Theory of Behavior and Brain Coordination},
  author = {Kelso, J. A. Scott and Dumas, Guillaume and Tognoli, Emmanuelle},
  year = {2013},
  month = jan,
  journal = {Neural Networks: The Official Journal of the International Neural Network Society},
  volume = {37},
  pages = {120--131},
  issn = {1879-2782},
  doi = {10.1016/j.neunet.2012.09.003},
  abstract = {Much evidence suggests that dynamic laws of neurobehavioral coordination are sui generis: they deal with collective properties that are repeatable from one system to another and emerge from microscopic dynamics but may not (even in principle) be deducible from them. Nevertheless, it is useful to try to understand the relationship between different levels while all the time respecting the autonomy of each. We report a program of research that uses the theoretical concepts of coordination dynamics and quantitative measurements of simple, well-defined experimental model systems to explicitly relate neural and behavioral levels of description in human beings. Our approach is both top-down and bottom-up and aims at ending up in the same place: top-down to derive behavioral patterns from neural fields, and bottom-up to generate neural field patterns from bidirectional coupling between astrocytes and neurons. Much progress can be made by recognizing that the two approaches--reductionism and emergentism--are complementary. A key to understanding is to couch the coordination of very different things--from molecules to thoughts--in the common language of coordination dynamics.},
  langid = {english},
  pmcid = {PMC3914303},
  pmid = {23084845},
  keywords = {Behavior,Brain,Cognition,Humans,Models Neurological,Models Psychological,Neural Pathways,Neuroglia,Neurons,Social Behavior},
  file = {/Users/spanis/Zotero/storage/2JCI6JKC/Kelso et al. - 2013 - Outline of a general theory of behavior and brain .pdf}
}

@article{kruschkeBayesianNewStatistics2018,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {178--206},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  urldate = {2024-10-25},
  abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed ``the New Statistics'' (Cumming 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
  langid = {english},
  keywords = {Bayes factor,Bayesian inference,Confidence interval,Credible interval,Effect size,Equivalence testing,Highest density interval,Meta-analysis,Null hypothesis significance testing,Power analysis,Randomized controlled trial,Region of practical equivalence},
  file = {/Users/spanis/Zotero/storage/Y6E45FMX/Kruschke and Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf}
}

@misc{kurzBayesianPowerAnalysis2019,
  title = {Bayesian Power Analysis: {{Part I}}. {{Prepare}} to Reject `{\textbackslash}({{H}}\_0{\textbackslash})` with Simulation.},
  shorttitle = {Bayesian Power Analysis},
  author = {Kurz, A. Solomon},
  year = {2019},
  month = jul,
  journal = {Bayesian power analysis: Part I.},
  urldate = {2024-12-08},
  abstract = {If you'd like to learn how to do Bayesian power calculations using **brms**, stick around for this multi-part blog series. Here with part I, we'll set the foundation.},
  howpublished = {https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/EMKD2TFV/bayesian-power-analysis-part-i.html}
}

@article{lakensSampleSizeJustification2022,
  title = {Sample {{Size Justification}}},
  author = {Lakens, Dani{\"e}l},
  editor = {van Ravenzwaaij, Don},
  year = {2022},
  month = mar,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {33267},
  issn = {2474-7394},
  doi = {10.1525/collabra.33267},
  urldate = {2024-12-08},
  abstract = {An important step when designing an empirical study is to justify the sample size that will be collected. The key aim of a sample size justification for such studies is to explain how the collected data is expected to provide valuable information given the inferential goals of the researcher. In this overview article six approaches are discussed to justify the sample size in a quantitative empirical study: 1) collecting data from (almost) the entire population, 2) choosing a sample size based on resource constraints, 3) performing an a-priori power analysis, 4) planning for a desired accuracy, 5) using heuristics, or 6) explicitly acknowledging the absence of a justification. An important question to consider when justifying sample sizes is which effect sizes are deemed interesting, and the extent to which the data that is collected informs inferences about these effect sizes. Depending on the sample size justification chosen, researchers could consider 1) what the smallest effect size of interest is, 2) which minimal effect size will be statistically significant, 3) which effect sizes they expect (and what they base these expectations on), 4) which effect sizes would be rejected based on a confidence interval around the effect size, 5) which ranges of effects a study has sufficient power to detect based on a sensitivity power analysis, and 6) which effect sizes are expected in a specific research area. Researchers can use the guidelines presented in this article, for example by using the interactive form in the accompanying online Shiny app, to improve their sample size justification, and hopefully, align the informational value of a study with their inferential goals.},
  file = {/Users/spanis/Zotero/storage/L3FMDQC3/Lakens - 2022 - Sample Size Justification.pdf;/Users/spanis/Zotero/storage/KG6WJ92Z/Sample-Size-Justification.html}
}

@article{landesIntroductionEventHistory2020,
  title = {An Introduction to Event History Analyses for Ecologists},
  author = {Landes, Julie and Engelhardt, Sacha C. and Pelletier, Fanie},
  year = {2020},
  month = oct,
  journal = {Ecosphere},
  volume = {11},
  number = {10},
  pages = {e03238},
  issn = {2150-8925, 2150-8925},
  doi = {10.1002/ecs2.3238},
  urldate = {2024-10-24},
  abstract = {Efforts to understand the emergence of an event require our ability to measure and understand the dynamics between time in a state (e.g., being alive or a behavior) and the outcome of the state. Studying the main drivers that affect changes in state over time allows researchers to better understand population dynamics and evolutionary processes. Event history analyses provide a range of theoretical and empirical tools to explore the emergence of an event. Their use is still restricted in ecology; however, they are commonly used in human demography. Event history analysis is a powerful tool for measuring the probability that an event occurs at time t. Here, we provide an introductory guide for ecologists who are interested in exploring event history analyses in their research. In the first part of this article, we outline key concepts in event history analyses and present a decision tree, statistical techniques, and their applications to ecological questions. To introduce practical applications of event history analyses, we provide four detailed tutorials, stemming from observational and longitudinal records of events in mammalian and avian species, along with relevant R scripts. We then explain how to interpret and present results of such analyses. Our results show that event history analyses are useful to quantify the effect of factors on the emergence of events. We conclude by highlighting additional strengths, pitfalls, and limitations researchers should be aware of when using such methods. We foresee the use of event history analyses for ecological studies.},
  langid = {english}
}

@article{landesIntroductionEventHistory2020a,
  title = {An Introduction to Event History Analyses for Ecologists},
  author = {Landes, Julie and Engelhardt, Sacha C. and Pelletier, Fanie},
  year = {2020},
  journal = {Ecosphere},
  volume = {11},
  number = {10},
  pages = {e03238},
  issn = {2150-8925},
  doi = {10.1002/ecs2.3238},
  urldate = {2024-10-24},
  abstract = {Efforts to understand the emergence of an event require our ability to measure and understand the dynamics between time in a state (e.g., being alive or a behavior) and the outcome of the state. Studying the main drivers that affect changes in state over time allows researchers to better understand population dynamics and evolutionary processes. Event history analyses provide a range of theoretical and empirical tools to explore the emergence of an event. Their use is still restricted in ecology; however, they are commonly used in human demography. Event history analysis is a powerful tool for measuring the probability that an event occurs at time t. Here, we provide an introductory guide for ecologists who are interested in exploring event history analyses in their research. In the first part of this article, we outline key concepts in event history analyses and present a decision tree, statistical techniques, and their applications to ecological questions. To introduce practical applications of event history analyses, we provide four detailed tutorials, stemming from observational and longitudinal records of events in mammalian and avian species, along with relevant R scripts. We then explain how to interpret and present results of such analyses. Our results show that event history analyses are useful to quantify the effect of factors on the emergence of events. We conclude by highlighting additional strengths, pitfalls, and limitations researchers should be aware of when using such methods. We foresee the use of event history analyses for ecological studies.},
  copyright = {{\copyright} 2020 The Authors.},
  langid = {english},
  keywords = {censoring,Cox proportional hazard regression,ecological applications,frailty,Kaplan-Meier estimation,mortality,parametric models,survival},
  file = {/Users/spanis/Zotero/storage/GWLGUSQ9/Landes et al. - 2020 - An introduction to event history analyses for ecol.pdf;/Users/spanis/Zotero/storage/UBHVQB2Q/ecs2.html}
}

@article{lougheedMultilevelSurvivalAnalysis2019,
  title = {Multilevel Survival Analysis: {{Studying}} the Timing of Children's Recurring Behaviors.},
  shorttitle = {Multilevel Survival Analysis},
  author = {Lougheed, Jessica P. and Benson, Lizbeth and Cole, Pamela M. and Ram, Nilam},
  year = {2019},
  month = jan,
  journal = {Developmental Psychology},
  volume = {55},
  number = {1},
  pages = {53--65},
  issn = {1939-0599, 0012-1649},
  doi = {10.1037/dev0000619},
  urldate = {2025-06-25},
  copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/EW2FRLQ5/Lougheed et al. - 2019 - Multilevel survival analysis Studying the timing of children’s recurring behaviors..pdf}
}

@book{luceResponseTimesTheir1991,
  title = {Response Times: Their Role in Inferring Elementary Mental Organization},
  shorttitle = {Response Times},
  author = {Luce, R. Duncan},
  year = {1991},
  series = {Oxford Psychology Series},
  edition = {1. issued as paperback},
  number = {8},
  publisher = {Univ. Press},
  address = {Oxford},
  isbn = {978-0-19-507001-9 978-0-19-503642-8},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2018,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2018},
  month = jan,
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315372495},
  urldate = {2024-07-25},
  isbn = {978-1-315-37249-5},
  langid = {english}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  address = {New York},
  doi = {10.1201/9780429029608},
  abstract = {Winner of the 2024 De Groot Prize awarded by the International Society for Bayesian Analysis (ISBA) Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work. The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding. The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses. Features Integrates working code into the main text. Illustrates concepts through worked data analysis examples. Emphasizes understanding assumptions and how assumptions are reflected in code. Offers more detailed explanations of the mathematics in optional sections. Presents examples of using the dagitty R package to analyze causal graphs. Provides the rethinking R package on the author's website and on GitHub.},
  isbn = {978-0-429-02960-8}
}

@article{meyerModernMentalChronometry1988,
  title = {Modern Mental Chronometry},
  author = {Meyer, David E. and Osman, Allen M. and Irwin, David E. and Yantis, Steven},
  year = {1988},
  journal = {Biological Psychology},
  volume = {26},
  number = {1-3},
  pages = {3--67},
  publisher = {Elsevier Science},
  address = {Netherlands},
  issn = {1873-6246},
  doi = {10.1016/0301-0511(88)90013-0},
  abstract = {Asserts that mental chronometry (M. I. Posner, 1978), in which conclusions about human information processing are reached through measures of Ss' reaction time, (RT), has contributed substantially to studies of cognition and action. During the evolution of the chronometric paradigm, issues that have emerged include (a) the existence of separable processing stages, (b) the degree to which various stages of processing produce partial outputs before they are completed, and (c) the discrete vs continuous form of the outputs. To obtain added temporal resolution, new RT procedures have been developed, including special response-priming and speed-accuracy decomposition techniques that focus on quantitative patterns of RT distributions and error rates. The authors summarize these developments, starting with a historical review of chronometric research and present a survey of recent empirical and theoretical innovations. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Evoked Potentials,Experimental Design,Human Information Storage,Reaction Time},
  file = {/Users/spanis/Zotero/storage/JSJIF2NV/1989-25060-001.html}
}

@book{millsIntroducingSurvivalEvent2011,
  title = {Introducing {{Survival}} and {{Event History Analysis}}},
  author = {Mills, Melinda},
  year = {2011},
  publisher = {SAGE Publications Ltd},
  address = {1 Oliver's Yard,~55 City Road,~London~EC1Y 1SP~United Kingdom},
  doi = {10.4135/9781446268360},
  urldate = {2025-06-25},
  isbn = {978-1-84860-102-4 978-1-4462-6836-0}
}

@article{nelderStatisticsStatisticalScience1999,
  title = {From {{Statistics}} to {{Statistical Science}}},
  author = {Nelder, John A.},
  year = {1999},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {48},
  number = {2},
  eprint = {2681191},
  eprinttype = {jstor},
  pages = {257--269},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0039-0526},
  urldate = {2024-12-09},
  abstract = {It is asserted that statistics must be relevant to making inferences in science and technology. The subject should be renamed statistical science and be focused on the experimental cycle, design-execute-analyse-predict. Its part in each component of the cycle is discussed. The P-value culture is claimed to be the main prop of non-scientific statistics, leading to the cult of the single study and the proliferation of multiple-comparison tests. The malign influence of P-values on protocols for the analysis of groups of experiments is discussed, and also the consequences of the formation of inferentially uninteresting linear models. Suggestions for action by statisticians include the sorting out of modes of inference, the removal of non-scientific procedures, the offering of help to editors, the promotion of good software and teaching methods built round the experimental cycle.},
  file = {/Users/spanis/Zotero/storage/C7LAR4UJ/Nelder - 1999 - From Statistics to Statistical Science.pdf}
}

@article{panisAnalyzingResponseTimes2020,
  title = {Analyzing {{Response Times}} and {{Other Types}} of {{Time-to-Event Data Using Event History Analysis}}: {{A Tool}} for {{Mental Chronometry}} and {{Cognitive Psychophysiology}}},
  shorttitle = {Analyzing {{Response Times}} and {{Other Types}} of {{Time-to-Event Data Using Event History Analysis}}},
  author = {Panis, Sven and Schmidt, Filipp and Wolkersdorfer, Maximilian P. and Schmidt, Thomas},
  year = {2020},
  month = nov,
  journal = {i-Perception},
  volume = {11},
  number = {6},
  pages = {2041669520978673},
  publisher = {SAGE Publications},
  issn = {2041-6695},
  doi = {10.1177/2041669520978673},
  urldate = {2024-06-05},
  abstract = {In this Methods article, we discuss and illustrate a unifying, principled way to analyze response time data from psychological experiments---and all other types of time-to-event data. We advocate the general application of discrete-time event history analysis (EHA) which is a well-established, intuitive longitudinal approach to statistically describe and model the shape of time-to-event distributions. After discussing the theoretical background behind the so-called hazard function of event occurrence in both continuous and discrete time units, we illustrate how to calculate and interpret the descriptive statistics provided by discrete-time EHA using two example data sets (masked priming, visual search). In case of discrimination data, the hazard analysis of response occurrence can be extended with a microlevel speed-accuracy trade-off analysis. We then discuss different approaches for obtaining inferential statistics. We consider the advantages and disadvantages of a principled use of discrete-time EHA for time-to-event data compared to (a) comparing means with analysis of variance, (b) other distributional methods available in the literature such as delta plots and continuous-time EHA methods, and (c) only fitting parametric distributions or computational models to empirical data. We conclude that statistically controlling for the passage of time during data analysis is equally important as experimental control during the design of an experiment, to understand human behavior in our experimental paradigms.},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/N4RBP9Z5/Panis et al. - 2020 - Analyzing Response Times and Other Types of Time-t.pdf}
}

@article{panisHowCanWe2020,
  title = {How Can We Learn What Attention Is? {{Response}} Gating via Multiple Direct Routes Kept in Check by Inhibitory Control Processes},
  shorttitle = {How Can We Learn What Attention Is?},
  author = {Panis, Sven},
  year = {2020},
  month = jan,
  journal = {Open Psychology},
  volume = {2},
  number = {1},
  pages = {238--279},
  publisher = {De Gruyter Open Access},
  issn = {2543-8883},
  doi = {10.1515/psych-2020-0107},
  urldate = {2024-06-05},
  abstract = {To explore the time course of space- and object-based attentional selection processes I analysed the shapes of the response time (RT) and accuracy distributions of left/right arrow identification responses in the two-rectangle paradigm. After cueing one of the four ends of two horizontally or vertically oriented rectangles the arrow typically appears at the cued location (valid), or sometimes at an uncued location in the same (invalid-same) or other rectangle (invalid-different). The data point to a multiple-route model in which (a) an informative cue generates response channel activation before arrow signals emerge, (b) the task-irrelevant arrow location is represented in multiple egocentric and allocentric reference frames around 150 ms after target onset, with the former including a reference frame centered on the currently attended location, (c) the task-irrelevant spatial codes activate premature response tendencies that are actively inhibited to allow gating of arrow direction signals, (d) after an invalid cue the onset of the arrow triggers an ``attention shift'' -- acting between 150 and 240 ms after target onset -- that strongly interferes with task performance in certain conditions (invalid-same cueing with horizontal rectangles, and invalid-different cueing with vertical rectangles), and (e) participants differ in which task-irrelevant codes they preferentially inhibit. These results pave the way for future confirmatory studies to temporally characterize and disentangle the contributions of different types of response channel activation processes, from those of reactive cognitive control processes including active and selective response suppression.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {event history analysis,hazard function,inter-individual differences,object-based attention,response times,Simon effect,space-based attention,spatial compatibility effects,speed-accuracy tradeoff},
  file = {/Users/spanis/Zotero/storage/XNJG9372/Panis - 2020 - How can we learn what attention is Response gatin.pdf}
}

@article{panisNeuropsychologicalEvidenceTemporal2017,
  title = {Neuropsychological Evidence for the Temporal Dynamics of Category-Specific Naming},
  author = {Panis, Sven and Torfs, Katrien and Gillebert, Celine R. and Wagemans, Johan and Humphreys, Glyn W.},
  year = {2017},
  month = mar,
  journal = {Visual Cognition},
  volume = {25},
  number = {1-3},
  pages = {79--99},
  publisher = {Routledge},
  issn = {1350-6285},
  doi = {10.1080/13506285.2017.1330790},
  urldate = {2024-06-05},
  abstract = {Multiple accounts have been proposed to explain category-specific recognition impairments. Some suggest that category-specific deficits may be caused by a deficit in recurrent processing between the levels of a hierarchically organized visual object recognition system. Here, we tested predictions of interactive processing theories on the emergence of category-selective naming deficits in neurologically intact observers and in patient GA, a single case showing a category-specific impairment for natural objects after a herpes simplex encephalitis infection. Fragmented object outlines were repeatedly presented until correct naming occurred (maximum 10 times), and the fragments increased in length with every repetition. We studied how shape complexity, object category, and fragment curvature influence the timing of correct object identification. The results of a survival analysis are consistent with the idea that deficits in recurrent processing between low- and high-level visual object representations can cause category-selective impairments.},
  pmid = {29238759},
  keywords = {category-specific impairments,event history analysis,Object recognition},
  file = {/Users/spanis/Zotero/storage/UWLVWKQX/Panis et al. - 2017 - Neuropsychological evidence for the temporal dynam.pdf}
}

@article{panisStudyingDynamicsVisual2020,
  title = {Studying the Dynamics of Visual Search Behavior Using {{RT}} Hazard and Micro-Level Speed--Accuracy Tradeoff Functions: {{A}} Role for Recurrent Object Recognition and Cognitive Control Processes},
  shorttitle = {Studying the Dynamics of Visual Search Behavior Using {{RT}} Hazard and Micro-Level Speed--Accuracy Tradeoff Functions},
  author = {Panis, Sven and Moran, Rani and Wolkersdorfer, Maximilian P. and Schmidt, Thomas},
  year = {2020},
  month = feb,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {2},
  pages = {689--714},
  issn = {1943-393X},
  doi = {10.3758/s13414-019-01897-z},
  urldate = {2024-06-05},
  abstract = {Thanks to the work of Anne Treisman and many others, the visual search paradigm has become one of the most popular paradigms in the study of visual attention. However, statistics like mean correct response time (RT) and percent error do not usually suffice to decide between the different search models that have been developed. Recently, to move beyond mean performance measures in visual search, RT histograms have been plotted, theoretical waiting time distributions have been fitted, and whole RT and error distributions have been simulated. Here we promote and illustrate the general application of discrete-time hazard analysis to response times, and of micro-level speed--accuracy tradeoff analysis to timed response accuracies. An exploratory analysis of published benchmark search data from feature, conjunction, and spatial configuration search tasks reveals new features of visual search behavior, such as a relatively flat hazard function in the right tail of the RT distributions for all tasks, a clear effect of set size on the shape of the RT distribution for the feature search task, and individual differences in the presence of a systematic pattern of early errors. Our findings suggest that the temporal dynamics of visual search behavior results from a decision process that is temporally modulated by concurrently active recurrent object recognition, learning, and cognitive control processes, next to attentional selection processes.},
  langid = {english},
  keywords = {Discrete-time hazard analysis,Event history analysis,Individual differences,Response times,Speed-accuracy tradeoff,Visual search},
  file = {/Users/spanis/Zotero/storage/3TMIFC8R/Panis et al. - 2020 - Studying the dynamics of visual search behavior us.pdf}
}

@article{panisTimecourseContingenciesPerceptual2009,
  title = {Time-Course Contingencies in Perceptual Organization and Identification of Fragmented Object Outlines},
  author = {Panis, Sven and Wagemans, Johan},
  year = {2009},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {35},
  number = {3},
  pages = {661--687},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1277},
  doi = {10.1037/a0013547},
  abstract = {To study the dynamic interplay between different component processes involved in the identification of fragmented object outlines, the authors used a discrete-identification paradigm in which the masked presentation duration of fragmented object outlines was repeatedly increased until correct naming occurred. Survival analysis was used to investigate whether and when different types of information---such as contour integration cues (proximity, collinearity, and fragment density), fragment properties (low vs. high curvature), stimulus complexity (global symmetry, number and saliency of the parts), and memory factors (natural vs. artifactual)---influenced the timing of identification. The results show that the importance of these different types of information can change over the time course of object identification, indicating so-called time-course contingencies. Most important, the straight segments of a contour played a larger role for complex outlines with high part saliency during early (bottom-up) grouping processes, whereas the curved segments of object outlines were more important during later (top-down) matching processes for simpler outlines with lower part saliency. This new insight can explain why different studies on shape-based object identification have produced seemingly contradictory results. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Form and Shape Perception,Object Recognition,Perceptual Organization,Visual Perception},
  file = {/Users/spanis/Zotero/storage/MWBIAJMW/Panis and Wagemans - 2009 - Time-course contingencies in perceptual organizati.pdf;/Users/spanis/Zotero/storage/CJUMDH7Z/doiLanding.html}
}

@article{panisWhatShapingRT2016,
  title = {What {{Is Shaping RT}} and {{Accuracy Distributions}}? {{Active}} and {{Selective Response Inhibition Causes}} the {{Negative Compatibility Effect}}},
  shorttitle = {What {{Is Shaping RT}} and {{Accuracy Distributions}}?},
  author = {Panis, Sven and Schmidt, Thomas},
  year = {2016},
  month = nov,
  journal = {Journal of Cognitive Neuroscience},
  volume = {28},
  number = {11},
  pages = {1651--1671},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00998},
  urldate = {2024-06-05},
  abstract = {Abstract             Inhibitory control such as active selective response inhibition is currently a major topic in cognitive neuroscience. Here we analyze the shape of behavioral RT and accuracy distributions in a visual masked priming paradigm. We employ discrete time hazard functions of response occurrence and conditional accuracy functions to study what causes the negative compatibility effect (NCE)---faster responses and less errors in inconsistent than in consistent prime target conditions---during the time course of a trial. Experiment 1 compares different mask types to find out whether response-relevant mask features are necessary for the NCE. After ruling out this explanation, Experiment 2 manipulates prime mask and mask target intervals to find out whether the NCE is time-locked to the prime or to the mask. We find that (a) response conflicts in inconsistent prime target conditions are locked to target onset, (b) positive priming effects are locked to prime onset whereas the NCE is locked to mask onset, (c) active response inhibition is selective for the primed response, and (d) the type of mask has only modulating effects. We conclude that the NCE is neither caused by automatic self-inhibition of the primed response due to backward masking nor by updating response-relevant features of the mask, but by active mask-triggered selective inhibition of the primed response. We discuss our results in light of a recent computational model of the role of the BG in response gating and executive control.},
  langid = {english}
}

@article{panisWhenDoesInhibition2022,
  title = {When Does ``Inhibition of Return'' Occur in Spatial Cueing Tasks? {{Temporally}} Disentangling Multiple Cue-Triggered Effects Using Response History and Conditional Accuracy Analyses},
  shorttitle = {When Does ``Inhibition of Return'' Occur in Spatial Cueing Tasks?},
  author = {Panis, Sven and Schmidt, Thomas},
  year = {2022},
  month = jan,
  journal = {Open Psychology},
  volume = {4},
  number = {1},
  pages = {84--114},
  publisher = {De Gruyter Open Access},
  issn = {2543-8883},
  doi = {10.1515/psych-2022-0005},
  urldate = {2024-06-05},
  abstract = {Research on spatial cueing has shown that uninformative cues often facilitate mean response time (RT) performance in valid- compared to invalid-cueing conditions at short cue-target stimulus-onset-asynchronies (SOAs), and robustly generate a reversed or inhibitory cueing effect at longer SOAs that is widely known as inhibition-of-return (IOR). To study the within-trial time course of the IOR and facilitation effects we employ discrete-time hazard and conditional accuracy analyses to analyze the shapes of the RT and accuracy distributions measured in two experimental tasks. Our distributional analyses show that (a) IOR is present only from {\textasciitilde}160 ms to {\textasciitilde}280 ms after target onset for cue-target SOAs above {\textasciitilde}200 ms, (b) facilitation does not precede IOR, but co-occurs with it, (c) the cue-triggered motor response activation is selectively and actively inhibited before target onset, (d) the IOR effect consists of a facilitatory and an inhibitory component when compared to central cueing, (e) the addition of an extra central cue causes a temporary negative cueing effect in the conditional accuracy functions, and (f) the within-trial time course of IOR is not affected much by the task employed (detection or localization). We conclude that the traditional mean performance measures conceal crucial information on behavioral dynamics in spatial cueing paradigms.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {attention,distributional analysis,event history analysis,inhibition of return,response inhibition,spatial cueing},
  file = {/Users/spanis/Zotero/storage/STVG3M2R/Panis and Schmidt - 2022 - When does “inhibition of return” occur in spatial .pdf}
}

@article{pargentTutorialTailoredSimulationBased2024,
  title = {A {{Tutorial}} on {{Tailored Simulation-Based Sample-Size Planning}} for {{Experimental Designs With Generalized Linear Mixed Models}}},
  author = {Pargent, Florian and Koch, Timo K. and Kleine, Anne-Kathrin and Lermer, Eva and Gaube, Susanne},
  year = {2024},
  month = oct,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {4},
  pages = {25152459241287132},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459241287132},
  urldate = {2024-12-16},
  abstract = {When planning experimental research, determining an appropriate sample size and using suitable statistical models are crucial for robust and informative results. The recent replication crisis underlines the need for more rigorous statistical methodology and adequately powered designs. Generalized linear mixed models (GLMMs) offer a flexible statistical framework to analyze experimental data with complex (e.g., dependent and hierarchical) data structures. However, available methods and software for a priori sample-size planning for GLMMs are often limited to specific designs. Tailored data-simulation approaches offer a more flexible alternative. Based on a practical case study in which we focus on a binomial GLMM with two random intercepts and discrete predictor variables, the current tutorial equips researchers with a step-by-step guide and corresponding code for conducting tailored a priori sample-size planning with GLMMs. We not only focus on power analysis but also explain how to use the precision of parameter estimates to determine appropriate sample sizes. We conclude with an outlook on the increasing importance of simulation-based sample-size planning.},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/TG42XYIF/Pargent et al. - 2024 - A Tutorial on Tailored Simulation-Based Sample-Siz.pdf}
}

@misc{ripleyMASSSupportFunctions2024,
  title = {{{MASS}}: {{Support Functions}} and {{Datasets}} for {{Venables}} and {{Ripley}}'s {{MASS}}},
  shorttitle = {{{MASS}}},
  author = {Ripley, Brian and Venables, Bill and Bates, Douglas M. and port {ca 1998)}, Kurt Hornik (partial and port {ca 1998)}, Albrecht Gebhardt (partial and functions for {polr)}, David Firth (support},
  year = {2024},
  month = jun,
  urldate = {2024-10-24},
  abstract = {Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S" (4th edition, 2002).},
  copyright = {GPL-2 {\textbar} GPL-3},
  keywords = {Distributions,Econometrics,Environmetrics,MixedModels,NumericalMathematics,Psychometrics,Robust,TeachingStatistics}
}

@article{schmidtResponseInhibitionNegative2022,
  title = {Response Inhibition in the {{Negative Compatibility Effect}} in the Absence of Inhibitory Stimulus Features},
  author = {Schmidt, Thomas and Panis, Sven and Wolkersdorfer, Maximilian P. and Vorberg, Dirk},
  year = {2022},
  month = jan,
  journal = {Open Psychology},
  volume = {4},
  number = {1},
  pages = {219--230},
  publisher = {De Gruyter Open Access},
  issn = {2543-8883},
  doi = {10.1515/psych-2022-0012},
  urldate = {2024-06-05},
  abstract = {The Negative Compatibility Effect (NCE) is a reversal in priming effects that can occur when a masked arrow prime is followed by an arrow target at a long stimulus-onset asynchrony (SOA). To test the explanation that the NCE is actually a positive priming effect elicited by mask features associated with the prime-opposed response, we devise masks that always point in the same direction as the prime, eliminating all antiprime features. We find large positive priming effects for arrow primes without masks and for arrow masks without primes. When a neutral mask is introduced, priming effects turn negative at long SOAs. In the critical case where the mask is an arrow in the same direction as the prime, the prime does not add to the positive priming effect from the mask shape, but instead strongly diminishes it and induces response errors even though all stimuli point in the same direction. No such feature-free inhibition is seen when arrows are replaced by color stimuli. We conclude that even though response activation by stimulus features plays a role in the NCE, there is a strong inhibitory component (though perhaps not in all feature domains) that is not based on visual features.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {Negative Compatibility Effect Response Inhibition Response Priming},
  file = {/Users/spanis/Zotero/storage/TU4TBFVB/Schmidt et al. - 2022 - Response inhibition in the Negative Compatibility .pdf}
}

@book{schonerDynamicThinkingPrimer2016,
  title = {Dynamic Thinking: A Primer on Dynamic Field Theory},
  shorttitle = {Dynamic Thinking},
  author = {Sch{\"o}ner, Gregor and Spencer, John P.},
  year = {2016},
  series = {Oxford Series in Developmental Cognitive Neuroscience},
  publisher = {Oxford University Press},
  address = {New York, NY},
  doi = {10.1093/acprof:oso/9780199300563.001.0001},
  abstract = {Dynamic Field Theory (DFT) is a theoretical framework of embodied cognition that is grounded in neurophysiology. Dynamic fields formalize how neural populations represent the continuous dimensions that characterize perceptual features, movements, and cognitive decisions. This book introduces the reader to a new approach to understanding cognitive and neural dynamics using the concepts of DFT},
  isbn = {978-0-19-930056-3 978-0-19-029902-6},
  langid = {english}
}

@book{singerAppliedLongitudinalData2003,
  title = {Applied {{Longitudinal Data Analysis}}: {{Modeling Change}} and {{Event Occurrence}}},
  shorttitle = {Applied {{Longitudinal Data Analysis}}},
  author = {Singer, Judith D. and Willett, John B.},
  year = {2003},
  month = may,
  publisher = {Oxford University Press},
  address = {Oxford, New York},
  abstract = {Change is constant in everyday life. Infants crawl and then walk, children learn to read and write, teenagers mature in myriad ways, the elderly become frail and forgetful. In addition to these natural changes, targeted interventions may cause change: cholesterol levels may decline as a result of a new medication, exam grades may rise following completion of a coaching class. By measuring and charting changes like these - both naturalistic and experimentally induced - researchers uncover the temporal nature of development. The investigation of change has fascinated empirical researchers for generations, and to do it well, they must have longitudinal data.  Applied Longitudinal Data Analysis is a much-needed professional book that will instruct readers in the many new methodologies now at their disposal to make the best use of longitudinal data, including both individual growth modelling and survival analysis. Throughout the chapters, the authors employ many cases and examples from a variety of disciplines, covering multilevel models, curvilinear and discontinuous change, in addition to discrete-time hazard models, continuous-time event occurrence, and Cox regression models. Applied Longitudinal Data Analysis is a unique contribution to the literature on research methods and will be useful to a wide range of behavioural and social science researchers.                                                        ,                Change is constant in everyday life. Infants crawl and then walk, children learn to read and write, teenagers mature in myriad ways, the elderly become frail and forgetful. In addition to these natural changes, targeted interventions may cause change: cholesterol levels may decline as a result of a new medication, exam grades may rise following completion of a coaching class. By measuring and charting changes like these - both naturalistic and experimentally induced - researchers uncover the temporal nature of development. The investigation of change has fascinated empirical researchers for generations, and to do it well, they must have longitudinal data.  Applied Longitudinal Data Analysis is a much-needed professional book that will instruct readers in the many new methodologies now at their disposal to make the best use of longitudinal data, including both individual growth modelling and survival analysis. Throughout the chapters, the authors employ many cases and examples from a variety of disciplines, covering multilevel models, curvilinear and discontinuous change, in addition to discrete-time hazard models, continuous-time event occurrence, and Cox regression models. Applied Longitudinal Data Analysis is a unique contribution to the literature on research methods and will be useful to a wide range of behavioural and social science researchers.},
  isbn = {978-0-19-515296-8},
  file = {/Users/spanis/Zotero/storage/REX3C3UL/applied-longitudinal-data-analysis-9780195152968.html}
}

@article{smithSmallBeautifulDefense2018,
  title = {Small Is Beautiful: {{In}} Defense of the Small-{{N}} Design},
  shorttitle = {Small Is Beautiful},
  author = {Smith, Philip L. and Little, Daniel R.},
  year = {2018},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {6},
  pages = {2083--2101},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1451-8},
  urldate = {2024-07-22},
  abstract = {The dominant paradigm for inference in psychology is a null-hypothesis significance testing one. Recently, the foundations of this paradigm have been shaken by several notable replication failures. One recommendation to remedy the replication crisis is to collect larger samples of participants. We argue that this recommendation misses a critical point, which is that increasing sample size will not remedy psychology's lack of strong measurement, lack of strong theories and models, and lack of effective experimental control over error variance. In contrast, there is a long history of research in psychology employing small-N designs that treats the individual participant as the replication unit, which addresses each of these failings, and which produces results that are robust and readily replicated. We illustrate the properties of small-N and large-N designs using a simulated paradigm investigating the stage structure of response times. Our simulations highlight the high power and inferential validity of the small-N design, in contrast to the lower power and inferential indeterminacy of the large-N design. We argue that, if psychology is to be a mature quantitative science, then its primary theoretical aim should be to investigate systematic, functional relationships as they are manifested at the individual participant level and that, wherever possible, it should use methods that are optimized to identify relationships of this kind.},
  langid = {english},
  keywords = {Inference,Mathematical psychology,Methodology,Replication},
  file = {/Users/spanis/Zotero/storage/SWTZNRR3/Smith and Little - 2018 - Small is beautiful In defense of the small-N desi.pdf}
}

@misc{StatisticsLinguistsIntroduction,
  title = {Statistics for {{Linguists}}: {{An Introduction Using R}}},
  shorttitle = {Statistics for {{Linguists}}},
  journal = {Routledge \& CRC Press},
  urldate = {2024-11-06},
  abstract = {Statistics for Linguists: An Introduction Using R is the first statistics textbook on linear models for linguistics. The book covers simple uses of linear models through generalized models to more advanced approaches, maintaining its focus on conceptual issues and avoiding excessive mathematical details. It contains many applied examples using the R statistical programming environment. Written in an accessible tone and style, this text is the ideal main resource for graduate and advanced underg},
  howpublished = {https://www.routledge.com/Statistics-for-Linguists-An-Introduction-Using-R/Winter/p/book/9781138056091},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/2U2VNZGJ/9781138056091.html}
}

@article{steeleGeneralMultilevelMultistate2004,
  title = {A General Multilevel Multistate Competing Risks Model for Event History Data,                 with an Application to a Study of Contraceptive Use Dynamics},
  author = {Steele, Fiona and Goldstein, Harvey and Browne, William},
  year = {2004},
  month = jul,
  journal = {Statistical Modelling},
  volume = {4},
  number = {2},
  pages = {145--159},
  publisher = {SAGE Publications India},
  issn = {1471-082X},
  doi = {10.1191/1471082X04st069oa},
  urldate = {2024-10-24},
  abstract = {We propose a general discrete time model for multilevel event history data. The model is developed for the analysis of longitudinal repeated episodes within individuals where there are multiple states and multiple types of event (competing risks) which may vary across states. The different transitions are modelled jointly to allow for correlation across transitions in unobserved individual risk factors. Implementation of the methodology using existing multilevel models for discrete response data is described. The model is applied in an analysis of contraceptive use dynamics in Indonesia where transitions from two states, contraceptive use and nonuse, are of interest. A distinction is made between two ways in which an episode of contraceptive use may end: a transition to nonuse or a switch to another method. Before adjusting for covariate effects, there is a strong negative residual correlation between the hazards of a transition from use to nonuse and from nonuse to use; this correlation is due to a tendency for short periods of nonuse after a birth to be followed by long periods of using the same contraceptive method.},
  langid = {english}
}

@book{stoolmillerIntroductionUsingMultivariate2015,
  title = {An {{Introduction}} to {{Using Multivariate Multilevel Survival Analysis}} to {{Study Coercive Family Process}}},
  author = {Stoolmiller, Mike},
  editor = {Dishion, Thomas J. and Snyder, James},
  year = {2015},
  month = jul,
  volume = {1},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780199324552.013.27},
  urldate = {2025-06-25},
  abstract = {Multivariate multilevel survival analysis is introduced for studying hazard rates of observed emotional behavior relevant for coercion theory. Finite time sampling reliability (FTSR) and short-term retest reliability (STRR) across two occasions (sessions) of observation during structured problem-solving tasks several weeks apart were determined for hazard rates of emotional behaviors for parent--child dyads. While FTSR was high (.80--.96), STRR was low (.16--.65), suggesting that emotional behaviors in the context of parent--child social interaction are not very stable over a period of several weeks. Using latent variable structural equation models that corrected for the low STRR, two hazard rates were predictive of change in child antisocial behavior over a 3-year period (kindergarten to third grade) net of initial child antisocial behavior. Low levels of parent positive emotion and increases from session 1 to 2 of child neutral behavior both accounted for unique variance in third grade antisocial behavior.},
  langid = {english}
}

@article{stoolmillerModelingHeterogeneitySocial2006,
  title = {Modeling Heterogeneity in Social Interaction Processes Using Multilevel Survival Analysis.},
  author = {Stoolmiller, Mike and Snyder, James},
  year = {2006},
  journal = {Psychological Methods},
  volume = {11},
  number = {2},
  pages = {164--177},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.2.164},
  urldate = {2025-06-25},
  langid = {english}
}

@article{teachmanAnalyzingSocialProcesses1983,
  title = {Analyzing Social Processes: {{Life}} Tables and Proportional Hazards Models},
  shorttitle = {Analyzing Social Processes},
  author = {Teachman, Jay D.},
  year = {1983},
  month = sep,
  journal = {Social Science Research},
  volume = {12},
  number = {3},
  pages = {263--301},
  issn = {0049-089X},
  doi = {10.1016/0049-089X(83)90015-7},
  urldate = {2024-10-24},
  abstract = {A number of techniques useful in describing and modeling social processes are detailed. Emphasis is placed on the analysis of changes in categorical variables as they occur in continuous time. The procedures considered are mainly non-parametric, although parametric alternatives do exist when the appropriate distributional assumptions are met. Specifically, life tables and proportional hazards models are discussed and illustrated through an analysis of first live birth intervals for a sample of white American women. The generality of proportional hazards models is outlined by considering extensions of the basic model to include competing risks, time-dependent covariates, and repeatable events.},
  file = {/Users/spanis/Zotero/storage/2JPYTB7G/0049089X83900157.html}
}

@article{tongStatisticalInferenceEnables2019,
  title = {Statistical {{Inference Enables Bad Science}}; {{Statistical Thinking Enables Good Science}}},
  author = {Tong, Christopher},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {246--261},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1518264},
  urldate = {2024-12-09},
  langid = {english},
  file = {/Users/spanis/Zotero/storage/6UXPZKQY/Tong - 2019 - Statistical Inference Enables Bad Science; Statist.pdf}
}

@article{townsendTruthConsequencesOrdinal1990,
  title = {Truth and Consequences of Ordinal Differences in Statistical Distributions: {{Toward}} a Theory of Hierarchical Inference},
  shorttitle = {Truth and Consequences of Ordinal Differences in Statistical Distributions},
  author = {Townsend, James T.},
  year = {1990},
  journal = {Psychological Bulletin},
  volume = {108},
  number = {3},
  pages = {551--567},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.108.3.551},
  abstract = {A theory is presented that establishes a dominance hierarchy of potential distinctions (order relations) between 2 distributions. It is proposed that it is worthwhile for researchers to ascertain the strongest possible distinction, because all weaker distinctions are logically implied. Implications of the theory for hypothesis testing, theory construction, and scales of measurement are considered. Open problems for future research are outlined. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  keywords = {Consequence,Frequency Distribution,Statistical Analysis},
  file = {/Users/spanis/Zotero/storage/6QEU7RXR/1991-06332-001.html}
}

@article{vanzandtHowFitResponse2000,
  title = {How to Fit a Response Time Distribution},
  author = {Van Zandt, Trisha},
  year = {2000},
  month = sep,
  journal = {Psychonomic Bulletin \& Review},
  volume = {7},
  number = {3},
  pages = {424--465},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03214357},
  urldate = {2025-06-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}

@article{wickelgrenSpeedaccuracyTradeoffInformation1977,
  title = {Speed-Accuracy Tradeoff and Information Processing Dynamics},
  author = {Wickelgren, Wayne A.},
  year = {1977},
  month = feb,
  journal = {Acta Psychologica},
  volume = {41},
  number = {1},
  pages = {67--85},
  issn = {0001-6918},
  doi = {10.1016/0001-6918(77)90012-9},
  urldate = {2024-07-22},
  abstract = {For a long time, it has been known that one can tradeoff accuracy for speed in (presumably) any task. The range over which one can obtain substantial speed-accuracy tradeoff varies from 150 msec in some very simple perceptual tasks to 1,000 msec in some recognition memory tasks and presumably even longer in more complex cognitive tasks. Obtaining an entire speed-accuracy tradeoff function provides much greater knowledge concerning information processing dynamics than is obtained by a reaction- time experiment, which yields the equivalent of a single point on this function. For this and other reasons, speed-accuracy tradeoff studies are often preferable to reaction-time studies of the dynamics of perceptual, memory, and cognitive processes. Methods of obtaining speed-accuracy tradeoff functions include: instructions, payoffs, deadlines, bands, response signals (with blocked and mixed designs), and partitioning of reaction time. A combination of the mixed-design signal method supplemented by partitioning of reaction times appears to be the optimal method.},
  file = {/Users/spanis/Zotero/storage/KAYRT7MS/0001691877900129.html}
}

@book{wickhamDataScienceImport2023,
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  shorttitle = {R for Data Science},
  author = {Wickham, Hadley and {\c C}etinkaya-Rundel, Mine and Grolemund, Garrett},
  year = {2023},
  edition = {2nd edition},
  publisher = {O'Reilly},
  address = {Beijing Boston Farnham Sebastopol Tokyo},
  abstract = {Use R to turn data into insight, knowledge, and understanding. With this practical book, aspiring data scientists will learn how to do data science with R and RStudio, along with the tidyverse---a collection of R packages designed to work together to make data science fast, fluent, and fun. Even if you have no programming experience, this updated edition will have you doing data science quickly. You'll learn how to import, transform, and visualize your data and communicate the results. And you'll get a complete, big-picture understanding of the data science cycle and the basic tools you need to manage the details. Updated for the latest tidyverse features and best practices, new chapters show you how to get data from spreadsheets, databases, and websites. Exercises help you practice what you've learned along the way},
  isbn = {978-1-4920-9740-2},
  langid = {english}
}

@book{williammatthewmakehamLawMortalityConstruction1860,
  title = {On the {{Law}} of {{Mortality}} and the {{Construction}} of {{Annuity Tables}}},
  author = {{William Matthew Makeham}},
  year = {1860},
  month = jan,
  publisher = {{The Assurance Magazine, and Journal of the Institute of Actuaries}},
  urldate = {2024-10-24},
  abstract = {"On the Law of Mortality and the Construction of Annuity Tables" is an article from The Assurance Magazine, and Journal of the Institute of Actuaries, Volume 8.  View more articles from The Assurance Magazine, and Journal of the Institute of Actuaries. View this article on JSTOR. View this article's JSTOR metadata. You may also retrieve all of this items metadata in JSON at the following URL: https://archive.org/metadata/jstor-41134925},
  collaborator = {{JSTOR}},
  langid = {english}
}

@book{winterStatisticsLinguistsIntroduction2019,
  title = {Statistics for {{Linguists}}: {{An Introduction Using R}}},
  shorttitle = {Statistics for {{Linguists}}},
  author = {Winter, Bodo},
  year = {2019},
  month = nov,
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9781315165547},
  abstract = {Statistics for Linguists: An Introduction Using R is the first statistics textbook on linear models for linguistics. The book covers simple uses of linear models through generalized models to more advanced approaches, maintaining its focus on conceptual issues and avoiding excessive mathematical details. It contains many applied examples using the R statistical programming environment. Written in an accessible tone and style, this text is the ideal main resource for graduate and advanced undergraduate students of Linguistics statistics courses as well as those in other fields, including Psychology, Cognitive Science, and Data Science.},
  isbn = {978-1-315-16554-7},
  file = {/Users/spanis/Zotero/storage/DIA8R2LQ/Winter - 2019 - Statistics for Linguists An Introduction Using R.pdf}
}

@article{wolkersdorferTemporalDynamicsSequential2020,
  title = {Temporal Dynamics of Sequential Motor Activation in a Dual-Prime Paradigm: {{Insights}} from Conditional Accuracy and Hazard Functions},
  shorttitle = {Temporal Dynamics of Sequential Motor Activation in a Dual-Prime Paradigm},
  author = {Wolkersdorfer, Maximilian P. and Panis, Sven and Schmidt, Thomas},
  year = {2020},
  month = jul,
  journal = {Attention, Perception, \& Psychophysics},
  volume = {82},
  number = {5},
  pages = {2581--2602},
  issn = {1943-393X},
  doi = {10.3758/s13414-020-02010-5},
  urldate = {2024-10-27},
  abstract = {In response priming experiments, a participant has to respond as quickly and as accurately as possible to a target stimulus preceded by a prime. The prime and the target can either be mapped to the same response (consistent trial) or to different responses (inconsistent trial). Here, we investigate the effects of two sequential primes (each one either consistent or inconsistent) followed by one target in a response priming experiment. We employ discrete-time hazard functions of response occurrence and conditional accuracy functions to explore the temporal dynamics of sequential motor activation. In two experiments (small-N design, 12 participants, 100 trials per cell and subject), we find that (1) the earliest responses are controlled exclusively by the first prime if primes are presented in quick succession, (2) intermediate responses reflect competition between primes, with the second prime increasingly dominating the response as its time of onset is moved forward, and (3) only the slowest responses are clearly controlled by the target. The current study provides evidence that sequential primes meet strict criteria for sequential response activation. Moreover, it suggests that primes can influence responses out of a memory buffer when they are presented so early that participants are forced to delay their responses.},
  langid = {english},
  keywords = {Event history analysis,Feedforward sweep,Reaction- time analysis,Response priming,Visuomotor},
  file = {/Users/spanis/Zotero/storage/Z55J7NUA/Wolkersdorfer et al. - 2020 - Temporal dynamics of sequential motor activation i.pdf}
}

@book{kurzAppliedLongitudinalDataAnalysis2023,
  title = {Applied longitudinal data analysis in brms and the tidyverse},
  author = {Kurz, A. Solomon},
  year = {2023},
  month = {6},
  edition = {version 0.0.3},
  url = {https://bookdown.org/content/4253/}
}

@book{kurzStatisticalRethinkingSecondEd2023,
  title = {Statistical rethinking with brms, ggplot2, and the tidyverse: {{Second}} edition},
  author = {Kurz, A. Solomon},
  year = {2023},
  month = {jan},
  edition = {version 0.4.0},
  url = {https://bookdown.org/content/4857/}
}

@article{SteeleF2004,
author = {Fiona Steele and Harvey Goldstein and William Browne},
title ={A general multilevel multistate competing risks model for event history data, with an application to a study of contraceptive use dynamics},
journal = {Statistical Modelling},
volume = {4},
number = {2},
pages = {145-159},
year = {2004},
doi = {10.1191/1471082X04st069oa},
URL = {https://doi.org/10.1191/1471082X04st069oa},
eprint = {https://doi.org/10.1191/1471082X04st069oa},
abstract = { We propose a general discrete time model for multilevel event history data. The model is developed for the analysis of longitudinal repeated episodes within individuals where there are multiple states and multiple types of event (competing risks) which may vary across states. The different transitions are modelled jointly to allow for correlation across transitions in unobserved individual risk factors. Implementation of the methodology using existing multilevel models for discrete response data is described. The model is applied in an analysis of contraceptive use dynamics in Indonesia where transitions from two states, contraceptive use and nonuse, are of interest. A distinction is made between two ways in which an episode of contraceptive use may end: a transition to nonuse or a switch to another method. Before adjusting for covariate effects, there is a strong negative residual correlation between the hazards of a transition from use to nonuse and from nonuse to use; this correlation is due to a tendency for short periods of nonuse after a birth to be followed by long periods of using the same contraceptive method. }
}

@book{williammatthewmakehamLawMortalityConstruction1860,
  title = {On the {{Law}} of {{Mortality}} and the {{Construction}} of {{Annuity Tables}}},
  author = {Makeham, William M.},
  year = {1860},
  month = jan,
  publisher = {{The Assurance Magazine, and Journal of the Institute of Actuaries}},
  urldate = {2024-10-24},
  abstract = {"On the Law of Mortality and the Construction of Annuity Tables" is an article from The Assurance Magazine, and Journal of the Institute of Actuaries, Volume 8.  View more articles from The Assurance Magazine, and Journal of the Institute of Actuaries. View this article on JSTOR. View this article's JSTOR metadata. You may also retrieve all of this items metadata in JSON at the following URL: https://archive.org/metadata/jstor-41134925},
  collaborator = {{JSTOR}},
  langid = {english}
}


@Manual{pap2024,
  title = {{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}},
  author = {Frederik Aust and Marius Barth},
  year = {2024},
  note = {R package version 0.1.3},
  url = {https://github.com/crsh/papaja},
  doi = {10.32614/CRAN.package.papaja},
}

@online{heiss2021,
  author = {Heiss, Andrew},
  title = {A Guide to Correctly Calculating Posterior Predictions and
    Average Marginal Effects with Multilievel {Bayesian} Models},
  date = {2021-11-10},
  url = {https://www.andrewheiss.com/blog/2021/11/10/ame-bayes-re-guide/},
  doi = {10.59350/wbn93-edb02},
  langid = {en}
}


@article{Halley1693,
author = {Halley, Edmond },
title = {VI. An estimate of the degrees of the mortality of mankind; drawn from curious tables of the births and funerals at the city of Breslaw; with an attempt to ascertain the price of annuities upon lives},
journal = {Philosophical Transactions of the Royal Society of London},
volume = {17},
number = {196},
pages = {596-610},
year = {1693},
doi = {10.1098/rstl.1693.0007},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstl.1693.0007},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstl.1693.0007}
,
    abstract = { The Contemplation of the Mortality of Mankind, has besides the Moral, its Physical and Political Uses, both which have been some years since most judiciously considered by the curious Sir William Petty, in his Natural and Political Observations on the Bills of Mortality of London, owned by Captain John Graunt. }
}
