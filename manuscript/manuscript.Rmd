---
title             : "A tutorial on Bayesian and Frequentist Event History Analyses for psychological time-to-event data"
shorttitle        : "Tutorial on hazard analysis"
author: 
  - name          : "Sven Panis"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "ETH GLC, room G16.2, Gloriastrasse 37/39, 8006 Zürich"
    email         : "sven.panis@hest.ethz.ch"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Richard Ramsey"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"
affiliation:
  - id            : "1"
    institution   : "ETH Zürich"
authornote: |
  Neural Control of Movement lab, Department of Health Sciences and Technology (D-HEST).
  <!-- Enter author note here. -->
abstract: |
  Time-to-event data such as response times, saccade latencies, and fixation durations are ubiquitous in experimental psychology. The orthodox method for analysing such data -- comparing means with analysis-of-variance -- is actually hiding a lot of information about psychological effects, such as their onset time and duration, and whether they are time-locked to stimuli. Such information can change key conclusions about psychological processes and can be revealed by using distributional measures.  
  Here we provide a set of tutorials on how to implement one particular distributional method known as discrete-time event history analysis, a.k.a. hazard analysis, duration analysis, failure-time analysis, survival analysis, and transition analysis. We illustrate how one can calculate the descriptive statistics, and how one can implement Bayesian and frequentist regression models, using the R packages tidyverse, brms, and lme4. The R code is publicly available on Github and OSF, and can easily be adapted for other data sets. We  discuss possible link functions and prior distributions, how to manage inter-individual differences, implications for experimental design, and how to select among various options when analysing time-to-event data using discrete-time hazard analysis.  
  
  <!-- https://tinyurl.com/ybremelq -->
keywords          : "response times, event history analysis, Bayesian regression models"
wordcount         : "X"
bibliography      : ["r-references.bib", "extrareferences.bib"]
floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_pdf
# output            : papaja::apa6_word
editor_options: 
  chunk_output_type: console
header-includes:
  - \raggedbottom
---


```{r setup, include = FALSE}
# Tutorials 1 and 4
pkg1 <- c("papaja", "citr", "tidyverse", "RColorBrewer", "patchwork")

lapply(pkg1, library, character.only = TRUE)

# Tutorial 2
pkg2 <- c("cmdstanr", "standist", "tidyverse", "RColorBrewer", "patchwork", 
         "brms", "tidybayes", "bayesplot", "future", "parallel")

lapply(pkg2, library, character.only = TRUE)

options(brms.backend = "cmdstanr",
        mc.cores = parallel::detectCores(),
        future.fork.enable = TRUE,
        future.rng.onMisuse = "ignore") ## automatically set in RStudio

supportsMulticore()
detectCores()

# Tutorial 3
pkg3 <- c("lme4", "nlme")

lapply(pkg3, library, character.only = TRUE)

# R references
r_refs(file = "r-references.bib")
my_r_citation <- cite_r(file = "r-references.bib", footnote = TRUE)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r plot-settings}
## theme settings for ggplot
theme_set(
  theme_bw() +
    theme(text = element_text(size = 18), 
          title = element_text(size = 18),
          legend.position = "bottom")
)

## Set the amount of dodge in figures
pd <- position_dodge(0.7)
pd2 <- position_dodge(1)
```

```{r global-chunk-settings}
## set the figure options
knitr::opts_chunk$set(fig.pos='H', out.extra = '', out.height = "67%",
                      fig.align = "center") 
```

# Introduction

## Means versus distributional shapes

In experimental psychology, it is still standard practice to analyse response times (RTs), saccade latencies, and fixation durations by calculating average performance across a series of trials. However, differences in means conceal when an experimental effect starts, how long it lasts, how it evolves over increasing waiting time, and whether its onset is time-locked to other events. Such information is useful not only for interpretation, but also for cognitive psychophysiology and computational model selection [@panisAnalyzingResponseTimes2020].
As a simple illustration, Figure 1 shows three examples of how an observed difference in mean response times (RTs) between two experimental conditions conceals differences in the shapes of the underlying RT distributions.

(ref:descr-fig1-caption) Means versus distributional shapes.

```{r plot1, fig.cap = "(ref:descr-fig1-caption)", out.width="80%"}
knitr::include_graphics("../Figures/rt_haz_combi_2.jpeg")
```

In each example, the mean RT is lower in condition 2 compared to condition 1. However, the distributions in the first example show that the effect starts around 200 ms and is gone by 600 ms. In the second example, the distributional effect starts around 400 ms and is gone by 800 ms. And in the third example, the distributional effect reverses around 550 ms.

## Outline of the paper

In this paper we focus on a distributional method known as discrete-time event history analysis, a.k.a. hazard analysis, duration analysis, failure-time analysis, survival analysis, and transition analysis.
We first provide a brief overview of hazard analysis to orient the reader to the basic concepts and ideas that we will use throughout the paper. However, this will remain relatively short, and for detailed treatment, see @singerAppliedLongitudinalData2003, @allisonDiscreteTimeMethodsAnalysis1982, and @allisonSurvivalAnalysisUsing2010. 

We then provide four different tutorials, each of which is written in R code and publicly available on Github and the Open Science Framework (OSF). The tutorials provide hands-on, concrete examples of key parts of the analytical process, so that others can apply the analyses to their own time-to-event data sets. In Tutorial 1 we illustrate how to calculate the descriptive statistics for a published data set when there is one independent variable. The descriptive statistics are plotted, and we comment on their interpretation. In Tutorial 2 we illustrate how one can fit Bayesian hazard models to the data. After selecting the best of four models, we plot the model-based effects and the model fits for a few subjects. In Tutorial 3 we illustrate how to fit hazard models in a frequentist framework. We compare the model-based effects between Bayesian and frequentist approaches. In Tutorial 4 we illustrate how to calculate the descriptive statistics when there are two independent variables.  

# Overview of hazard analysis

To apply event history analysis (EHA), one must be able to define the event of interest (any qualitative change that can be situated in time, e.g., a button press, a saccade onset, a fixation offset, etc.), time point zero (e.g., target stimulus onset, fixation onset), and measure the passage of time between time point zero and event occurrence in discrete or continuous time units. Both the definition of hazard and the type of models employed depend on whether one is using continuous or discrete time units.

The shape of a distribution of waiting times can be described in multiple ways [@luceResponseTimesTheir1991]. Let RT be a continous random variable denoting a particular person's response time in a particular experimental condition. Because waiting times can only increase, continuous-time EHA does not focus on the cumulative distribution function F(t) = P(RT $\leq$ t) and its derivative, the probability density function f(t) = F(t)', but on the survivor function S(t) = P(RT $>$ t) and the hazard rate function $\lambda$(t) = f(t)/S(t). The hazard rate function gives you the instantaneous rate of event occurrence at time point t, given that the event has not occurred yet. 

Similarly, after dividing time in discrete, contiguous time bins indexed by t, let RT be a discrete random variable denoting the rank of the time bin in which a particular person's response occurs in a particular experimental condition. Discrete-time EHA focuses on the discrete-time hazard function h(t) = P(RT = t| RT $\geq$ t) and the discrete-time survivor function S(t) = P(RT $>$ t) = [1-h(t)].[1-h(t-1)].[1-h(t-2)]...[1-h(1)], and not on the probability mass function p(t) = h(t).S(t-1) and the cumulative distribution function F(t) = 1-S(t). The discrete-time hazard probability function gives you the probability that the event occurs (sometime) in bin t, given that the event has not occurred yet in previous bins. Unlike the discrete-time hazard function, which assesses the unique risk associated with each time bin, the discrete-time survivor function cumulates the bin-by-bin risks of event *non*occurrence.

For two-choice RT data, the discrete-time hazard function can be extended with the discrete-time conditional accuracy function ca(t) = P(correct | RT = t), which gives you the probability that a response is correct given that it has been emitted in time bin t [@kantowitzInterpretationReactionTime2021; @wickelgrenSpeedaccuracyTradeoffInformation1977; @allisonSurvivalAnalysisUsing2010]. This latter function is also known as the micro-level speed-accuracy tradeoff function.

Statisticians and mathematical psychologists recommend focusing on the hazard function when analyzing time-to-event data for various reasons. First, as discussed by @holdenDispersionResponseTimes2009, “probability density functions can appear nearly identical, both statistically and to the naked eye, and yet are clearly different on the basis of their hazard functions (but not vice versa). Hazard functions are thus more diagnostic than density functions” (p. 331).
Second, because RT distributions may differ from one another in multiple ways, @townsendTruthConsequencesOrdinal1990 developed a dominance hierarchy of statistical differences between two arbitrary distributions A and B. For example, if F~A~(t) > F~B~(t) for all t, then both cumulative distribution functions are said to show a complete ordering. Townsend (1990) showed that a complete ordering on the hazard functions —$\lambda$~A~(t) > $\lambda$~B~(t) for all t— implies a complete ordering on both the cumulative distribution and survivor functions —F~A~(t) > F~B~(t) and S~A~(t) < S~B~(t)— which in turn implies an ordering on the mean latencies —mean A < mean B. In contrast, an ordering on two means does not imply a complete ordering on the corresponding F(t) and S(t) functions, and a complete ordering on these latter functions does not imply a complete ordering on the corresponding hazard functions. This means that stronger conclusions can be drawn from data when comparing the hazard functions using EHA. For example, when mean A < mean B, the hazard functions might show a complete ordering (i.e., for all t), a partial ordering (e.g., only for t > 300 ms, or only for t < 500 ms), or they may cross each other one or more times.
Third, EHA does not discard right-censored observations when estimating hazard functions, that is, trials for which we do not observe a response during the data collection period so that we only know that the RT must be larger than some value. This is important because although a few right-censored observations are inevitable in most RT tasks, a lot of right-censored observations are expected in experiments on masking, the attentional blink, and so forth.
Fourth, hazard modeling allows incorporating time-varying explanatory covariates such as heart rate, electroencephalogram (EEG) signal amplitude, gaze location, etc. (Allison, 2010) which is useful for cognitive psychophysiology [@meyerModernMentalChronometry1988].
Finally, as explained by @kelsoOutlineGeneralTheory2013, it is crucial to first have a precise description of the macroscopic behavior of a system (here: h(t) and ca(t) functions) in order to know what to derive on the microscopic level. For example, fitting parametric functions or computational models to data without studying the shape of the discrete-time h(t) and ca(t) functions can miss important features in the data [@panisStudyingDynamicsVisual2020; @panisWhatShapingRT2016].

We focus on factorial within-subject designs in which a large number of observations are made on a relatively small number of participants (small-*N* designs). This  approach  emphasizes  the  precision  and reproducibility of data patterns at the individual participant level to increase the inferential validity of the design [@bakerPowerContoursOptimising2021; @smithSmallBeautifulDefense2018]. In contrast to the large-*N* design that averages across many participants without being able to scrutinize individual data patterns,  small-*N*  designs  retain  crucial  information  about  the  data  patterns  of  individual  observers.  This  is  of  great  advantage whenever participants differ systematically in their strategies or in the time-courses of their effects, so that blindly averaging them would lead to misleading data patterns. Indeed, @smithSmallBeautifulDefense2018 argue that, “if psychology is to be a mature quantitative science, then its primary theoretical aim should be to investigate systematic functional relationships as they are manifested at the individual participant level” (p. 2083). Note that because statistical power derives both from the number of participants and from the number of repeated measures per participant and condition, small-*N* designs can have excellent power [@bakerPowerContoursOptimising2021; @smithSmallBeautifulDefense2018].

We used `r my_r_citation$r` for all reported analyses. Web links are printed in bold. The content of the tutorials is mainly based on @allisonSurvivalAnalysisUsing2010, @singerAppliedLongitudinalData2003, @mcelreathStatisticalRethinkingBayesian2018, @kurzAppliedLongitudinalDataAnalysis2023, and @kurzStatisticalRethinkingSecondEd2023.

`r my_r_citation$pkgs` <!-- This creates the footnote -->

# Tutorial 1: Calculating descriptive statistics using a life table

To illustrate how to quickly set up life tables for calculating the descriptive statistics (functions of discrete time), we use a published data set on masked response priming from @panisWhatShapingRT2016, available on [**ResearchGate**](https://www.researchgate.net/publication/304069212_What_Is_Shaping_RT_and_Accuracy_Distributions_Active_and_Selective_Response_Inhibition_Causes_the_Negative_Compatibility_Effect).
In their first experiment, @panisWhatShapingRT2016 presented a double arrow for 94 ms that pointed left or right as the target stimulus with an onset at time point zero in each trial. Participants had to indicate the direction in which the double arrow pointed using their corresponding index finger, within 800 ms after target onset. Response time and accuracy were recorded on each trial. Prime type (blank, congruent, incongruent) and mask type were manipulated. Here we focus on the subset of trials in which no mask was presented. The 13-ms prime stimulus was a double arrow with onset at -187 ms for the congruent (same direction as target) and incongruent (opposite direction as target) prime conditions.

After loading in the data file, one has to (a) supply required column names, and (b) specify the factor condition with the correct levels and labels.
The required column names are as follows:

* "pid", indicating unique participant IDs;
* "trial", indicating each unique trial per participant;
* "condition", a factor indicating the levels of the independent variable (1, 2, ...) and the corresponding labels;
* "rt", indicating the response times in ms;
* "acc", inicating the accuracies (1/0).

In the code of Tutorial 1, this is accomplished as follows.

\scriptsize
```{r setup-data-tut1, echo=TRUE}
data_wr <- read_csv("../Tutorial_1_descriptive_stats/data/DataExp1_6subjects_wrangled.csv")
colnames(data_wr) <- c("pid","bl","tr","condition","resp","acc","rt","trial") 
data_wr <- data_wr %>% 
  mutate(condition = condition + 1, # original levels were 0, 1, 2.
         condition = factor(condition, levels=c(1,2,3), labels=c("blank","congruent","incongruent")))
```
\normalsize

```{r load-functions, echo=F}
# function censor creates censored observations and discrete response times (drt), given a user-defined censoring time (timeout) and bin width in ms
censor <- function(df, timeout, bin_width){
  if(!(timeout %% bin_width == 0)){
    return("The censoring time must be a multiple of the bin width!")
  }
  if(timeout < 0 | bin_width < 0){
    return("Both timeout and bin_width must be larger than 0!")
  }
  df %>% mutate(right_censored = 0,
                rtc = ifelse(rt > timeout, timeout, rt) %>% round(digits=2), # censored response times
                right_censored = ifelse(rtc == timeout,1,right_censored), # 1 = right censored observation, 0 = observed rt
                drt = ceiling(rtc/bin_width), # drt = discrete response time
                cens_time = timeout, bin_width = bin_width) # save both user-defined parameters for plotting
}

# function ptb creates a person-trial-bin oriented data set
ptb <- function(df){
  df %>% uncount(weights = drt) %>% # create drt rows per trial
         group_by(trial) %>% 
         mutate(period = 1:n()) %>% # create time bin (or time period) ranks within each trial
         mutate(event = if_else(period == max(period) & right_censored == 0, 1, 0)) %>% # event = 1 indicates response occurrence
         ungroup()
}

# function setup_lt sets up a life table for each level of condition (1 independent variable)
setup_lt <- function(ptb){
  ptb %>% mutate(event = str_c("event", event)) %>%
          group_by(condition,period) %>% 
          count(event) %>% 
          ungroup() %>% 
          pivot_wider(names_from = event,
                      values_from = n) %>% 
          mutate(event0 = ifelse(is.na(event0),0,event0), # replace NA with 0
                 event1 = ifelse(is.na(event1),0,event1),
                 risk_set = event0 + event1) %>% # define the risk set
          mutate(hazard = (event1 / risk_set) %>% round(digits = 3)) %>% # calculate hazard estimate
          mutate(se_haz = sqrt((hazard * (1 - hazard)) / risk_set) %>% round(digits = 4)) %>% # standard error for hazard
          group_by(condition) %>%
          mutate(survival = (cumprod(1-hazard)) %>% round(digits = 4), # calculate survival estimate
                 term     = (cumsum(hazard / (risk_set * (1 - hazard)))) %>% round(digits = 7), # intermediate calculation
                 se_surv  = (survival * sqrt(term)) %>% round(digits = 5)  ) %>% # Greenwood's (1926) approximation
          ungroup() 
}

# function calc_ca calculates the conditional accuracies 
calc_ca <- function(df){
  df %>% filter(right_censored==0) %>%
         group_by(condition,drt,cens_time,bin_width) %>%
         summarize(ca = mean(acc) %>% round(digits = 2),
                   n = n(),
                   .groups = 'drop') %>%
         ungroup() %>%
         mutate(period = drt,
                se_ca = sqrt((ca * (1-ca)) / n) %>% round(digits = 3)) %>%
         select(-drt)
}

# function join_lt_ca joins the conditional accuracies to the life tables
join_lt_ca <- function(df1,df2){df1 %>% left_join(df2, join_by(condition,period))}

# function extract_median is used to extract the S(t).50 quantiles (i.e., the estimated median RTs)
extract_median <- function(df){
  above_pct50 <- df %>% 
      group_by(condition) %>%
      filter(survival > .5) %>% 
      slice(n()) # take last row
  below_pct50 <- df %>% 
      group_by(condition) %>%
      filter(survival < .5) %>% 
      slice(1) # take first row
  # pull period above
  period_above <- pull(above_pct50, period)
  # pull survivor function values
  survival_above <- pull(above_pct50, survival)
  survival_below <- pull(below_pct50, survival)
  # estimate median by interpolation
  median_period <- period_above+((survival_above-.5)/(survival_above-survival_below))*((period_above+1)-period_above)
}

# function plot_eha plots the hazard, survivor, and ca(t) functions, when there is one independent variable
plot_eha <- function(df,subj,haz_yaxis){ # set the upper limit of the y-axis for the hazard functions to haz_yaxis == 1 when plotting the data of individual subjects (see line 244)
  library(patchwork)
  cutoff <- df %>% pull(cens_time) %>% max(na.rm=T)
  binsize <- df %>% pull(bin_width) %>% max(na.rm=T)
  median_period <- extract_median(df)
  n_conditions <- nlevels(df$condition)
  data_median <- c()
  for(i in 1:n_conditions){
    data_median <- append(data_median, c(median_period[i], median_period[i]))
  }
  
  data_medians <- tibble(period= data_median,
                         survival = rep(c(.5, 0),n_conditions),
                         condition = rep(1:n_conditions, each=2))
# plot the hazard functions
p1 <- df %>% ggplot(aes(x=period, color=condition, group=condition)) +
  geom_line(aes(y=hazard)) +
  geom_point(aes(y=hazard), size=1) + labs(color="Condition") +
  geom_linerange(aes(ymin=hazard-se_haz, ymax=hazard+se_haz), show.legend = F) +
  scale_x_continuous(breaks = c(0,1:(cutoff/binsize)), labels=c(0,1:(cutoff/binsize)*binsize),
                     limits = c(0,cutoff/binsize)) +
  scale_y_continuous(limits = c(0,haz_yaxis)) +
  labs(x="", y="h(t)", title = paste("Subject ", subj)) +
  theme(legend.background = element_rect(fill = "transparent"),
        panel.grid = element_blank(),
        legend.position = "top")
# plot the survivor functions
p2 <-df %>%
  ggplot(aes(x=period, color=condition, group=condition)) +
  geom_line(aes(y=survival), show.legend = F) +
  geom_point(aes(y=survival), size=1, show.legend = F) +
  geom_linerange(aes(ymin=survival-se_surv, ymax=survival+se_surv), show.legend = F) +
  # add vertical lines at the median RTs in the plot of the survivor functions using geom_path(). Make sure you apply the same levels and labels for the factor condition as above on line 83!
  geom_path(aes(x=period, y=survival, color=factor(condition, levels =c(1,2,3),labels=c("blank","congruent","incongruent"))),
            data = data_medians, 
            linetype = 3, show.legend = F) +
  
  scale_x_continuous(breaks = c(0,1:(cutoff/binsize)), labels=c(0,1:(cutoff/binsize)*binsize),
                     limits=c(0,cutoff/binsize)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(x="", y="S(t)",
       colour="Condition") +
  theme(panel.grid = element_blank())
# plot the conditional accuracy functions
p3 <-df %>%
  ggplot(aes(x=period, color=condition, group=condition)) +
  geom_line(aes(y=ca), show.legend = F) +
  geom_point(aes(y=ca), size=1, show.legend = F) +
  geom_linerange(aes(ymin=ca-se_ca, ymax=ca+se_ca), show.legend = F) +
  scale_x_continuous(breaks = c(0,1:(cutoff/binsize)), labels=c(0,1:(cutoff/binsize)*binsize),
                     limits=c(0,cutoff/binsize)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(x="Time bin t's endpoint (ms)", y="ca(t)",
       colour="Condition") +
  theme(panel.grid = element_blank())

p1/p2/p3
}
```

To set up the life tables and plots of the discrete-time functions h(t), S(t) and ca(t) using functional programming, one has to nest the data within participants using the group_nest() function, and supply a user-defined censoring time and bin width to our function "censor()", as follows.

\scriptsize
```{r nest-apply-functions-tut1, echo = TRUE}
data_nested <- data_wr %>% group_nest(pid)
data_final <- data_nested %>% 
  mutate(censored  = map(data, censor, 600, 40)) %>%   # ! user input: censoring time, and bin width
  mutate(ptb_data  = map(censored, ptb)) %>%           # create person-trial-bin dataset
  mutate(lifetable = map(ptb_data, setup_lt)) %>%      # create life tables without ca(t)
  mutate(condacc   = map(censored, calc_ca)) %>%       # calculate ca(t)
  mutate(lifetable_ca = map2(lifetable, condacc, join_lt_ca)) %>%    # create life tables with ca(t)
  mutate(plot      = map2(.x = lifetable_ca, .y = pid, plot_eha,1))  # create plots 
```
\normalsize

Note that the censoring time should be a multiple of the bin width (both in ms). The censoring time should be a time point after which no informative responses are expected anymore. In experiments that implement a response deadline in each trial the censoring time can equal that deadline time point. Trials with a RT larger than the censoring time, or trials in which no response is emitted during the data collection period, are treated as right-censored observations in EHA. In other words, these trials are not discarded, because they contain the information that the event did not occur before the censoring time. Removing such trials before calculating the mean event time can introduce a sampling bias. The person-trial-bin oriented dataset has one row for each time bin of each trial that is at risk for event occurrence. The variable "event" in the person-trial-bin oriented data set indicates whether a response occurs (1) or not (0) for each bin.
When creating the plots using our function plot_eha(), some warning messages will likely be generated, like these:

* Removed 2 rows containing missing values or values outside the scale range (`geom_line()`). 
* Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). 
* Removed 2 rows containing missing values or values outside the scale range (`geom_segment()`).

The warning messages are generated because some bins have no hazard and ca(t) estimates, and no error bars. They can thus safely be ignored.
One can now inspect different aspects, including the life table for a particular condition of a particular subject, and a plot of the different functions for a particular participant.

Table 1 shows the life table for condition "blank" (no prime stimulus presented) - compare to Figure 1. A life table includes for each time bin, the risk set (number of trials that are event-free at the start of the bin), the number of observed events, and the estimates of h(t), S(t), ca(t) and their estimated standard errors (se). At time point zero, no events can occur and therefore h(t) and ca(t) are undefined.

Figure 1 displays the discrete-time hazard, survivor, and conditional accuracy functions for each prime condition for participant 6. By using discrete-time h(t) functions of event occurrence - in combination with ca(t) functions for two-choice tasks - one can provide an unbiased, time-varying, and probabilistic description of the latency and accuracy of responses based on all trials of any data set. 

For example, for participant 6, the estimated hazard values in bin (240,280] are `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(hazard) %>% slice(7) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(hazard) %>% slice(7) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(hazard) %>% slice(7) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively. In other words, when the waiting time has increased until *240 ms* after target onset, then the conditional probability of response occurrence in the next 40 ms is more than five times larger for both prime-present conditions, compared to the blank prime condition. 

Furthermore, the estimated conditional accuracy values in bin (240,280] are `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(ca) %>% slice(7) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(ca) %>% slice(7) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(ca) %>% slice(7) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively. In other words, if a response is emitted in bin (240,280], then the probability that it is correct is estimated to be `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(ca) %>% slice(7) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(ca) %>% slice(7) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(ca) %>% slice(7) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively.

(ref:life-table-caption) The life table for the blank prime condition of participant 6.

(ref:life-note-caption) The column named "bin" indicates the endpoint of each time bin (in ms), and includes time point zero. For example the first bin is (0,40] with the starting point excluded and the endpoint included. se = standard error. ca = conditional accuracy.

```{r life-table}
life_tab <- read_csv("../Tutorial_1_descriptive_stats/tables/lifetable_neutral_s6.csv")

apa_table(
  life_tab,
  caption = "(ref:life-table-caption)",
  note = "(ref:life-note-caption)",
  placement = "H"
)
```

(ref:descr-fig-caption) Estimated discrete-time hazard, survivor, and conditional accuracy functions for participant 6, as a function of the passage of discrete waiting time.

```{r eha-plot, fig.cap = "(ref:descr-fig-caption)"}
knitr::include_graphics("../Tutorial_1_descriptive_stats/figures/Plot_for_subject6_PanisSchmidt.png")
```

   However, when the waiting time has increased until *400 ms* after target onset, then the conditional probability of response occurrence in the next 40 ms is estimated to be `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(hazard) %>% slice(11) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(hazard) %>% slice(11) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(hazard) %>% slice(11) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively. And when a response does occur in bin (400,440], then the probability that it is correct is estimated to be `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(ca) %>% slice(11) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(ca) %>% slice(11) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(ca) %>% slice(11) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively.
 
  These results show that this participant is initially responding to the prime even though (s)he was instructed to only respond to the target, that response competition emerges in the incongruent prime condition around 300 ms, and that only later response are fully controlled by the target stimulus. Qualitatively similar results were obtained for the other five participants. Also, in their second Experiment, @panisWhatShapingRT2016 showed that the negative compatibility effect in the mask-present conditions is time-locked to mask onset. This example shows that a simple difference between two means fails to reveal the dynamic behavior people display in many experimental paradigms [@panisHowCanWe2020; @panisNeuropsychologicalEvidenceTemporal2017; @panisStudyingDynamicsVisual2020; @panisTimecourseContingenciesPerceptual2009; @panisWhenDoesInhibition2022; @schmidtResponseInhibitionNegative2022]. In other words, statistically controlling for the passage of time during data analysis is equally important as experimental control during the design of an experiment, to better understand human behavior in experimental paradigms. As we will show in Tutorials 2 and 3, statistical models for h(t) can be implemented as generalized linear mixed regression models predicting event occurrence (1/0) in each bin of a selected time range. 

# Tutorial 2: Fitting Bayesian hazard models

When you want to study how hazard depends on various predictors, you can fit regression models to the data [@singerAppliedLongitudinalData2003]. There are two analytic decisions one has to make. First, one has to select an analysis time range, i.e., a contiguous set of bins for which there is enough data for each participant. Second, one can choose the logit link function which transforms a (hazard) probability into the log of the odds ratio, or the complementary log-log (cloglog) link function, which yields the logarithm of the negated logarithm of the probability of event *non*occurrence. An important difference between these two link functions is that cloglog provides a discrete-time hazard model that has a built-in proportional hazards assumption, while logit provides a proportional odds assumption (see below). The cloglog link is preferred over the logit link when events can occur in principle at any time point within a bin, which is the case for RT data [@singerAppliedLongitudinalData2003]. Third, one has to choose a specification of the effect of discrete TIME (i.e., the time bin index t). One can choose a general specification (one intercept per bin) or a functional specification, such as a polynomial one.

An example discrete-time hazard model with three predictors (TIME, X1, X2) and the cloglog link function can be written as follows:

cloglog[h(t)] = ln(-ln[1-h(t)]) =  [$\alpha$~1~ONE + $\alpha$~2~(TIME-1) + $\alpha$~3~(TIME-1)$^2$] +                                    [$\beta$~1~X~1~ + $\beta$~2~X~2~ + $\beta$~3~X~2~(TIME-1)].

The main predictor variable TIME is the time bin index t that is centered on value 1 in this example. The first set of terms within brackets, the alpha parameters multiplied by their polynomial specifications of (centered) time, represents the shape of the baseline cloglog-hazard function (i.e., when all predictors X~i~ take on a value of zero). The second set of terms (the beta parameters) represents the vertical shift in the baseline cloglog-hazard for a 1 unit increase in the respective predictor. Predictors can be discrete, continuous, and time-varying or time-invariant. For example, the effect of a 1 unit increase in X~1~ is to vertically shift the whole baseline cloglog-hazard function by $\beta$~1~ cloglog-hazard units. However, if the predictor interacts linearly with time (see X~2~ in the example), then the effect of a 1 unit increase in X~2~ is to vertically shift the predicted cloglog-hazard in bin 1 by $\beta$~2~ cloglog-hazard units (when TIME-1 = 0), in bin 2 by $\beta$~2~ + $\beta$~3~ cloglog-hazard units (when TIME-1 = 1), and so forth. To interpret the effects of the predictors, the parameter estimates are exponentiated, resulting in a hazard ratio (due to the use of the cloglog link).

In the case of a large-*N* design without repeated measurements, the parameters of a discrete-time hazard model can be estimated using standard logistic regression software (after expanding the typical person-trial-oriented data set into a person-trial-bin-oriented data set (see Tutorial 1); @allisonSurvivalAnalysisUsing2010). When there is clustering in the data, as in the case of a small-*N* design with repeated measurements, the parameters of a discrete-time hazard model can be estimated using population-averaged methods (e.g., Generalized Estimating Equations), and Bayesian or frequentist generalized linear mixed models [@allisonSurvivalAnalysisUsing2010]. 

In this second tutorial we illustrate how to fit a Bayesian hazard regression model for the masked response priming data set used in the first tutorial. 
In general, there are three assumptions one can make or relax when adding experimental predictor variables: The linearity assumption for continuous predictors (the effect of a 1 unit change is the same anywhere on the scale), the additivity assumption (predictors do not interact), and the proportionality assumption (predictors do not interact with TIME).

First, we select the analysis range (200,600] and the cloglog link, and use a polynomial to specify the effect of TIME in the "blank" prime condition. Second, based on previous work [@panisHowCanWe2020; @panisNeuropsychologicalEvidenceTemporal2017; @panisStudyingDynamicsVisual2020; @panisTimecourseContingenciesPerceptual2009; @panisWhenDoesInhibition2022] and because cognition is likely the behavior of a non-linear dynamical system [ref], we relax all three assumptions, as follows:

\scriptsize
```{r prepare-data, echo=T}
# load data
ptb_data <- read_csv("../Tutorial_1_descriptive_stats/data/inputfile_hazard_modeling.csv")
# select analysis time range: (200,600] with 10 bins (time bin ranks 6 to 15)
ptb_data <- ptb_data %>% filter(period > 5)
# create factor condition, with "blank" as the reference level
ptb_data <- ptb_data %>% mutate(condition = factor(condition, labels = c("blank", "congruent","incongruent")))
# center discrete TIME (period) on bin 9, and trial on trial 1000
ptb_data <- ptb_data %>% mutate(period_9 = period - 9,
                                trial_c = (trial - 1000)/1000)
# remove unnecessary columns before fitting a model
ptb_data <- ptb_data %>% select(-c(bl,tr,trial,period)) # 12840 obs. of 5 variables
```

```{r set-priors, echo=T}
priors <- c(
  set_prior("normal(0, 1)", class = "b"), # for beta parameters 
  set_prior("student_t(7.61, 0, 1.57)", class = "b", coef = "Intercept"), # flat prior for intercept on hazard scale
  set_prior("normal(0, 1)", class = "sd"), # for standard deviation of RE
  set_prior("lkj(2)", class = "cor") # for correlations between RE
)
```

```{r fit-model, echo=T}
#plan(multicore)
#model_full_RE <-
#   brm(data = ptb_data,
#       family = binomial(link="cloglog"),
#       event | trials(1) ~ 0 + Intercept + 
#                           condition*period_9*trial_c + 
#                           condition*I(period_9^2) + 
#                           condition*I(period_9^3) +
#                           (1 + condition*period_9*trial_c +
#                           condition*I(period_9^2) +
#                           condition*I(period_9^3) | pid),
#       prior = priors,
#       chains = 4, cores = 4, iter = 3000, warmup = 1000,
#       control = list(adapt_delta = 0.999, step_size = 0.04, max_treedepth = 12),
#       seed = 12, init = "0",
#       file = "../Tutorial_2_Bayesian/models/model_full_RE")
```
\normalsize

To test whether (centered) trial number affects behavior, we fit a model without the variable trial_c.

Use WAIC to compare models.

Plot the effects of congruent and incongruent for each time bin for the selected model.

Plot the model-based hazard and survivor functions.



# Tutorial 3: Fitting Frequentist hazard models

In this third tutorial we illustrate how to fit a frequentist hazard regression model for the data set used in the first tutorial. 


# Tutorial 4: Calculating descriptive statistics when there are two independent variables

In this final tutorial we illustrate how to calculate and plot the descriptive statistics for the full data set of Experiment 1 of @panisWhatShapingRT2016, which includes two independent variables: mask type and prime type.



# Discussion

We noticed that many researchers are still reluctant to abandon analysis-of-variance and switch to event history analysis when analysing time-to-event data. By providing this tutorial, we hope that researchers will start using hazard analysis more often.
While we focused on within-subject factorial small-*N* designs, event history analysis has been developed to also encompass other designs.

## Advantages of hazard functions



## Individual differences

- role of response deadlines, low-level vs. higher-level processes, overlap between distributions
- clustering algorithms based on h(t) and ca(t) data
- 

## Cognitive psychophysiology and computational model selection

## Power analysis

- example repo on github

## Preregistration

- example preregistration for knot data

# Conclusions




\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
