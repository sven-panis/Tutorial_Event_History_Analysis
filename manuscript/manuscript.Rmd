---
title             : "A tutorial on Bayesian and Frequentist Event History Analyses for psychological time-to-event data"
shorttitle        : "Tutorial on hazard analysis"

author: 
  - name          : "Sven Panis"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "ETH GLC, room G16.2, Gloriastrasse 37/39, 8006 Zürich"
    email         : "sven.panis@hest.ethz.ch"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Richard Ramsey"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "ETH Zürich"

authornote: |
  Neural Control of Movement lab, Department of Health Sciences and Technology (D-HEST).

  <!-- Enter author note here. -->

abstract: |
  Time-to-event data such as response times, saccade latencies, and fixation durations are ubiquitous in experimental psychology. To move beyond mean performance measures, various distributional analyses have been proposed. Here we focus on one particular distributional analysis known as discrete-time event history analysis, a.k.a. hazard analysis, duration analysis, failure-time analysis, survival analysis, and transition analysis. Across four tutorials that we make publicly available on Github and OSF, we illustrate how to calculate and interpret descriptive statistics, and how to implement Bayesian and frequentist regression models, using the R packages tidyverse, brms, and lme4. Along the way, we discuss how to manage inter-individual differences, implications for experimental design, and how to select among various options when analysing time-to-event data using discrete-time survival analysis.  
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "response times, event history analysis, Bayesian regression models"
wordcount         : "X"

bibliography      : ["r-references.bib", "extrareferences.bib"]

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
# output            : papaja::apa6_word
editor_options: 
  chunk_output_type: console
header-includes:
  - \raggedbottom
---


```{r setup, include = FALSE}
pkg <- c("papaja", "citr", "tidyverse", "RColorBrewer", "patchwork")

lapply(pkg, library, character.only = TRUE)

r_refs(file = "r-references.bib")
my_r_citation <- cite_r(file = "r-references.bib", footnote = TRUE)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r plot-settings}
## theme settings for ggplot
theme_set(
  theme_bw() +
    theme(text = element_text(size = 18), 
          title = element_text(size = 18),
          legend.position = "bottom")
)

## Set the amount of dodge in figures
pd <- position_dodge(0.7)
pd2 <- position_dodge(1)
```

```{r global-chunk-settings}
## set the figure options
knitr::opts_chunk$set(fig.pos='H', out.extra = '', out.height = "67%",
                      fig.align = "center") 
```

# Introduction

In experimental psychology, it is still standard practice to analyse response times (RTs), saccade latencies, and fixation durations using analysis-of-variance. However, differences in means conceal when an experimental effect starts, how long it lasts, and whether its onset is time-locked to other events. Such information is useful not only for interpretation, but also for cognitive psychophysiology and computational model selection [@panisAnalyzingResponseTimes2020].
In this tutorial we focus on a distributional method for analyzing time-to-event data that is known as discrete-time event history analysis (EHA), a.k.a. survival, hazard, duration, failure-time, and transition analysis [@singerAppliedLongitudinalData2003; @allisonDiscreteTimeMethodsAnalysis1982; @allisonSurvivalAnalysisUsing2010]. Across four tutorials that we make publicly available on Github and OSF, we illustrate how to calculate and interpret descriptive statistics, and how to implement Bayesian and frequentist regression models, using the R packages tidyverse, brms, and lme4.

To apply EHA, one must be able to define the event of interest (any qualitative change that can be situated in time, e.g., a button press, saccade onset, fixation offset), time point zero (e.g., target stimulus onset, fixation onset), and measure the passage of time between time point zero and event occurrence in discrete or continuous time units. 

The shape of a distribution of waiting times can be described in multiple ways [@luceResponseTimesTheir1991]. Let RT be a continous random variable denoting a particular person's response time in a particular experimental condition. Because waiting times can only increase, continuous-time EHA does not focus on the cumulative distribution function F(t) = P(RT $\leq$ t) and its derivative, the probability density function f(t) = F(t)', but on the survivor function S(t) = P(RT $>$ t) and the hazard rate function $\lambda$(t) = f(t)/S(t). The hazard rate function gives you the instantaneous rate of event occurrence at time point t, given that the event has not occurred yet. 

Similarly, after dividing time in discrete, contiguous time bins indexed by t, let RT be a discrete random variable denoting the rank of the time bin in which a particular person's response occurs in a particular experimental condition. Discrete-time EHA focuses on the discrete-time survivor function S(t) = P(RT $>$ t) and the discrete-time hazard function h(t) = P(RT = t| RT $\geq$ t), and not on the probability mass function and the cumulative distribution function. The discrete-time hazard probability function gives you the probability that the event occurs (sometime) in bin t, given that the event has not occurred yet in previous bins. For two-choice RT data, the discrete-time hazard function can be extended with the conditional accuracy function ca(t) = P(correct | RT = t), which gives you the probability that a response is correct given that it has been emitted in time bin t [@kantowitzInterpretationReactionTime2021; @wickelgrenSpeedaccuracyTradeoffInformation1977; @allisonSurvivalAnalysisUsing2010]. This latter function is also known as the micro-level speed-accuracy tradeoff function.

Statisticians and mathematical psychologists recommend focusing on the hazard function when analyzing time-to-event data for various reasons. First, as discussed by @holdenDispersionResponseTimes2009, “probability density functions can appear nearly identical, both statistically and to the naked eye, and yet are clearly different on the basis of their hazard functions (but not vice versa). Hazard functions are thus more diagnostic than density functions” (p. 331).
Second, because RT distributions may differ from one another in multiple ways, @townsendTruthConsequencesOrdinal1990 developed a dominance hierarchy of statistical differences between two arbitrary distributions A and B. For example, if F~A~(t) > F~B~(t) for all t, then both cumulative distribution functions are said to show a complete ordering. Townsend (1990) showed that a complete ordering on the hazard functions —$\lambda$~A~(t) > $\lambda$~B~(t) for all t— implies a complete ordering on both the cumulative distribution and survivor functions —F~A~(t) > F~B~(t) and S~A~(t) < S~B~(t)— which in turn implies an ordering on the mean latencies —mean A < mean B. In contrast, an ordering on two means does not imply a complete ordering on the corresponding F(t) and S(t) functions, and a complete ordering on these latter functions does not imply a complete ordering on the corresponding hazard functions. This means that stronger conclusions can be drawn from data when comparing the hazard functions using EHA. For example, when mean A < mean B, the hazard functions might show a complete ordering (i.e., for all t), a partial ordering (e.g., only for t > 300 ms, or only for t < 500 ms), or they may cross each other one or more times.
Third, EHA does not discard right-censored observations when estimating hazard functions, that is, trials for which we do not observe a response during the data collection period so that we only know that the RT must be larger than some value. This is important because although a few right-censored observations are inevitable in most RT tasks, a lot of right-censored observations are expected in experiments on masking, the attentional blink, and so forth.
Fourth, hazard modeling allows incorporating time-varying explanatory covariates such as heart rate, electroencephalogram (EEG) signal amplitude, gaze location, etc. (Allison, 2010) which is useful for cognitive psychophysiology [@meyerModernMentalChronometry1988].
Finally, as explained by @kelsoOutlineGeneralTheory2013, it is crucial to first have a precise description of the macroscopic behavior of a system (here: h(t) and ca(t) functions) in order to know what to derive on the microscopic level. For example, fitting parametric functions or computational models to data without studying the shape of the h(t) and ca(t) functions can miss important features in the data [@panisStudyingDynamicsVisual2020; @panisWhatShapingRT2016].

We focus on factorial within-subject designs in which a large number of observations are made on a relatively small number of participants (small-*N* designs). This  approach  emphasizes  the  precision  and reproducibility of data patterns at the individual participant level to increase the inferential validity of the design [@bakerPowerContoursOptimising2021; @smithSmallBeautifulDefense2018]. In contrast to the large-*N* design that averages across many participants without being able to scrutinize individual data patterns,  small-*N*  designs  retain  crucial  information  about  the  data  patterns  of  individual  observers.  This  is  of  great  advantage whenever participants differ systematically in their strategies or in the time-courses of their effects, so that blindly averaging them would lead to misleading data patterns. Indeed, @smithSmallBeautifulDefense2018 argue that, “if psychology is to be a mature quantitative science, then its primary theoretical aim should be to investigate systematic functional relationships as they are manifested at the individual participant level” (p. 2083). Note that because statistical power derives both from the number of participants and from the number of repeated measures per participant and condition, small-*N* designs can have excellent power [@bakerPowerContoursOptimising2021; @smithSmallBeautifulDefense2018].

We used `r my_r_citation$r` for all reported analyses. Web links are printed in bold.

`r my_r_citation$pkgs` <!-- This creates the footnote -->

# Tutorial 1: Calculating descriptive statistics using a life table

To illustrate how to quickly set up life tables for calculating the descriptive statistics (functions of discrete time), we use a published data set on masked response priming from @panisWhatShapingRT2016, available on [**ResearchGate**](https://www.researchgate.net/publication/304069212_What_Is_Shaping_RT_and_Accuracy_Distributions_Active_and_Selective_Response_Inhibition_Causes_the_Negative_Compatibility_Effect).
In their first experiment, @panisWhatShapingRT2016 presented a double arrow for 94 ms that pointed left or right as the target stimulus with an onset at time point zero in each trial. Participants had to indicate the direction in which the double arrow pointed using their corresponding index finger, within 800 ms after target onset. Response time and accuracy were recorded on each trial. Prime type (blank, congruent, incongruent) and mask type were manipulated. Here we focus on the subset of trials in which no mask was presented. The 13-ms prime stimulus was a double arrow with onset at -187 ms for the congruent (same direction as target) and incongruent (opposite direction as target) prime conditions.

After loading in the data file, one has to (a) supply required column names, and (b) specify the factor condition with the correct levels and labels.
The required column names are as follows:

* "pid", indicating unique participant IDs;
* "trial", indicating each unique trial per participant;
* "condition", a factor indicating the levels of the independent variable (1, 2, ...) and the corresponding labels;
* "rt", indicating the response times in ms;
* "acc", inicating the accuracies (1/0).

In the code of Tutorial 1, this is accomplished as follows.

\scriptsize
```{r setup-data-tut1, echo=TRUE}
data_wr <- read_csv("../Tutorial_1_descriptive_stats/data/DataExp1_6subjects_wrangled.csv")
colnames(data_wr) <- c("pid","bl","tr","condition","resp","acc","rt","trial") 
data_wr <- data_wr %>% 
  mutate(condition = condition + 1, # original levels were 0, 1, 2.
         condition = factor(condition, levels=c(1,2,3), labels=c("blank","congruent","incongruent")))
```
\normalsize

```{r load-functions, echo=F}
# function censor creates censored observations and discrete response times (drt), given a user-defined censoring time (timeout) and bin width in ms
censor <- function(df, timeout, bin_width){
  if(!(timeout %% bin_width == 0)){
    return("The censoring time must be a multiple of the bin width!")
  }
  if(timeout < 0 | bin_width < 0){
    return("Both timeout and bin_width must be larger than 0!")
  }
  df %>% mutate(right_censored = 0,
                rtc = ifelse(rt > timeout, timeout, rt) %>% round(digits=2), # censored response times
                right_censored = ifelse(rtc == timeout,1,right_censored), # 1 = right censored observation, 0 = observed rt
                drt = ceiling(rtc/bin_width), # drt = discrete response time
                cens_time = timeout, bin_width = bin_width) # save both user-defined parameters for plotting
}

# function ptb creates a person-trial-bin oriented data set
ptb <- function(df){
  df %>% uncount(weights = drt) %>% # create drt rows per trial
         group_by(trial) %>% 
         mutate(period = 1:n()) %>% # create time bin (or time period) ranks within each trial
         mutate(event = if_else(period == max(period) & right_censored == 0, 1, 0)) %>% # event = 1 indicates response occurrence
         ungroup()
}

# function setup_lt sets up a life table for each level of condition (1 independent variable)
setup_lt <- function(ptb){
  ptb %>% mutate(event = str_c("event", event)) %>%
          group_by(condition,period) %>% 
          count(event) %>% 
          ungroup() %>% 
          pivot_wider(names_from = event,
                      values_from = n) %>% 
          mutate(event0 = ifelse(is.na(event0),0,event0), # replace NA with 0
                 event1 = ifelse(is.na(event1),0,event1),
                 risk_set = event0 + event1) %>% # define the risk set
          mutate(hazard = (event1 / risk_set) %>% round(digits = 3)) %>% # calculate hazard estimate
          mutate(se_haz = sqrt((hazard * (1 - hazard)) / risk_set) %>% round(digits = 4)) %>% # standard error for hazard
          group_by(condition) %>%
          mutate(survival = (cumprod(1-hazard)) %>% round(digits = 4), # calculate survival estimate
                 term     = (cumsum(hazard / (risk_set * (1 - hazard)))) %>% round(digits = 7), # intermediate calculation
                 se_surv  = (survival * sqrt(term)) %>% round(digits = 5)  ) %>% # Greenwood's (1926) approximation
          ungroup() 
}

# function calc_ca calculates the conditional accuracies 
calc_ca <- function(df){
  df %>% filter(right_censored==0) %>%
         group_by(condition,drt,cens_time,bin_width) %>%
         summarize(ca = mean(acc) %>% round(digits = 2),
                   n = n(),
                   .groups = 'drop') %>%
         ungroup() %>%
         mutate(period = drt,
                se_ca = sqrt((ca * (1-ca)) / n) %>% round(digits = 3)) %>%
         select(-drt)
}

# function join_lt_ca joins the conditional accuracies to the life tables
join_lt_ca <- function(df1,df2){df1 %>% left_join(df2, join_by(condition,period))}

# function extract_median is used to extract the S(t).50 quantiles (i.e., the estimated median RTs)
extract_median <- function(df){
  above_pct50 <- df %>% 
      group_by(condition) %>%
      filter(survival > .5) %>% 
      slice(n()) # take last row
  below_pct50 <- df %>% 
      group_by(condition) %>%
      filter(survival < .5) %>% 
      slice(1) # take first row
  # pull period above
  period_above <- pull(above_pct50, period)
  # pull survivor function values
  survival_above <- pull(above_pct50, survival)
  survival_below <- pull(below_pct50, survival)
  # estimate median by interpolation
  median_period <- period_above+((survival_above-.5)/(survival_above-survival_below))*((period_above+1)-period_above)
}

# function plot_eha plots the hazard, survivor, and ca(t) functions, when there is one independent variable
plot_eha <- function(df,subj,haz_yaxis){ # set the upper limit of the y-axis for the hazard functions to haz_yaxis == 1 when plotting the data of individual subjects (see line 244)
  library(patchwork)
  cutoff <- df %>% pull(cens_time) %>% max(na.rm=T)
  binsize <- df %>% pull(bin_width) %>% max(na.rm=T)
  median_period <- extract_median(df)
  n_conditions <- nlevels(df$condition)
  data_median <- c()
  for(i in 1:n_conditions){
    data_median <- append(data_median, c(median_period[i], median_period[i]))
  }
  
  data_medians <- tibble(period= data_median,
                         survival = rep(c(.5, 0),n_conditions),
                         condition = rep(1:n_conditions, each=2))
# plot the hazard functions
p1 <- df %>% ggplot(aes(x=period, color=condition, group=condition)) +
  geom_line(aes(y=hazard)) +
  geom_point(aes(y=hazard), size=1) + labs(color="Condition") +
  geom_linerange(aes(ymin=hazard-se_haz, ymax=hazard+se_haz), show.legend = F) +
  scale_x_continuous(breaks = c(0,1:(cutoff/binsize)), labels=c(0,1:(cutoff/binsize)*binsize),
                     limits = c(0,cutoff/binsize)) +
  scale_y_continuous(limits = c(0,haz_yaxis)) +
  labs(x="", y="h(t)", title = paste("Subject ", subj)) +
  theme(legend.background = element_rect(fill = "transparent"),
        panel.grid = element_blank(),
        legend.position = "top")
# plot the survivor functions
p2 <-df %>%
  ggplot(aes(x=period, color=condition, group=condition)) +
  geom_line(aes(y=survival), show.legend = F) +
  geom_point(aes(y=survival), size=1, show.legend = F) +
  geom_linerange(aes(ymin=survival-se_surv, ymax=survival+se_surv), show.legend = F) +
  # add vertical lines at the median RTs in the plot of the survivor functions using geom_path(). Make sure you apply the same levels and labels for the factor condition as above on line 83!
  geom_path(aes(x=period, y=survival, color=factor(condition, levels =c(1,2,3),labels=c("blank","congruent","incongruent"))),
            data = data_medians, 
            linetype = 3, show.legend = F) +
  
  scale_x_continuous(breaks = c(0,1:(cutoff/binsize)), labels=c(0,1:(cutoff/binsize)*binsize),
                     limits=c(0,cutoff/binsize)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(x="", y="S(t)",
       colour="Condition") +
  theme(panel.grid = element_blank())
# plot the conditional accuracy functions
p3 <-df %>%
  ggplot(aes(x=period, color=condition, group=condition)) +
  geom_line(aes(y=ca), show.legend = F) +
  geom_point(aes(y=ca), size=1, show.legend = F) +
  geom_linerange(aes(ymin=ca-se_ca, ymax=ca+se_ca), show.legend = F) +
  scale_x_continuous(breaks = c(0,1:(cutoff/binsize)), labels=c(0,1:(cutoff/binsize)*binsize),
                     limits=c(0,cutoff/binsize)) +
  scale_y_continuous(limits = c(0,1)) +
  labs(x="Time bin t's endpoint (ms)", y="ca(t)",
       colour="Condition") +
  theme(panel.grid = element_blank())

p1/p2/p3
}
```

To set up the life tables and plots of the discrete-time functions h(t), S(t) and ca(t) using functional programming, one has to nest the data within participants using the group_nest() function, and supply a user-defined censoring time and bin width to our function "censor()", as follows.

\scriptsize
```{r nest-apply-functions-tut1, echo = TRUE}
data_nested <- data_wr %>% group_nest(pid)
data_final <- data_nested %>% 
  mutate(censored  = map(data, censor, 600, 40)) %>%   # ! user input: censoring time, and bin width
  mutate(ptb_data  = map(censored, ptb)) %>%           # create person-trial-bin dataset
  mutate(lifetable = map(ptb_data, setup_lt)) %>%      # create life tables without ca(t)
  mutate(condacc   = map(censored, calc_ca)) %>%       # calculate ca(t)
  mutate(lifetable_ca = map2(lifetable, condacc, join_lt_ca)) %>%    # create life tables with ca(t)
  mutate(plot      = map2(.x = lifetable_ca, .y = pid, plot_eha,1))  # create plots 
```
\normalsize

Note that the censoring time should be a multiple of the bin width (both in ms). The censoring time should be a time point after which no informative responses are expected anymore. In experiments that implement a response deadline in each trial the censoring time can equal that deadline time point. Trials with a RT larger than the censoring time, or trials in which no response is emitted during the data collection period, are treated as right-censored observations in EHA. In other words, these trials are not discarded, because they contain the information that the event did not occur before the censoring time. Removing such trials before calculating the mean event time can introduce a sampling bias. The person-trial-bin oriented dataset has one row for each time bin of each trial that is at risk for event occurrence. The variable "event" in the person-trial-bin oriented data set indicates whether a response occurs (1) or not (0) for each bin.
When creating the plots using our function plot_eha(), some warning messages will likely be generated, like these:

* Removed 2 rows containing missing values or values outside the scale range (`geom_line()`). 
* Removed 2 rows containing missing values or values outside the scale range (`geom_point()`). 
* Removed 2 rows containing missing values or values outside the scale range (`geom_segment()`).

The warning messages are generated because some bins have no hazard and ca(t) estimates, and no error bars. They can thus safely be ignored.
One can now inspect different aspects, including the life table for a particular condition of a particular subject, and a plot of the different functions for a particular participant.

Table 1 shows the life table for condition "blank" (no prime stimulus presented) - compare to Figure 1. A life table includes for each time bin, the risk set (number of trials that are event-free at the start of the bin), the number of observed events, and the estimates of h(t), S(t), ca(t) and their estimated standard errors (se). At time point zero, no events can occur and therefore h(t) and ca(t) are undefined.

Figure 1 displays the discrete-time hazard, survivor, and conditional accuracy functions for each prime condition for participant 6. By using discrete-time h(t) functions of event occurrence - in combination with ca(t) functions for two-choice tasks - one can provide an unbiased, time-varying, and probabilistic description of the latency and accuracy of responses based on all trials of any data set. 

For example, for participant 6, the estimated hazard values in bin (240,280] are `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(hazard) %>% slice(7) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(hazard) %>% slice(7) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(hazard) %>% slice(7) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively. In other words, when the waiting time has increased until *240 ms* after target onset, then the conditional probability of response occurrence in the next 40 ms is more than five times larger for both prime-present conditions, compared to the blank prime condition. 

Furthermore, the estimated conditional accuracy values in bin (240,280] are `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(ca) %>% slice(7) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(ca) %>% slice(7) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(ca) %>% slice(7) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively. In other words, if a response is emitted in bin (240,280], then the probability that it is correct is estimated to be `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(ca) %>% slice(7) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(ca) %>% slice(7) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(ca) %>% slice(7) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively.

  However, when the waiting time has increased until *400 ms* after target onset, then the conditional probability of response occurrence in the next 40 ms is estimated to be `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(hazard) %>% slice(11) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(hazard) %>% slice(11) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(hazard) %>% slice(11) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively. And when a response does occur in bin (400,440], then the probability that it is correct is estimated to be `r data_final[[7]][[6]] %>% filter(condition=="blank") %>% select(ca) %>% slice(11) %>% pull()`, `r data_final[[7]][[6]] %>% filter(condition=="congruent") %>% select(ca) %>% slice(11) %>% pull()`, and `r data_final[[7]][[6]] %>% filter(condition=="incongruent") %>% select(ca) %>% slice(11) %>% pull()` for the blank, congruent, and incongruent prime conditions, respectively.
 

(ref:life-table-caption) The life table for the blank prime condition of participant 6.

(ref:life-note-caption) The column named "bin" indicates the endpoint of each time bin (in ms), and includes time point zero. For example the first bin is (0,40] with the starting point excluded and the endpoint included. se = standard error. ca = conditional accuracy.

```{r life-table}
life_tab <- read_csv("../Tutorial_1_descriptive_stats/tables/lifetable_neutral_s6.csv")

apa_table(
  life_tab,
  caption = "(ref:life-table-caption)",
  note = "(ref:life-note-caption)",
  placement = "H"
)
```

(ref:descr-fig-caption) Estimated discrete-time hazard, survivor, and conditional accuracy functions for participant 6, as a function of the passage of discrete waiting time.

```{r eha-plot, fig.cap = "(ref:descr-fig-caption)"}
knitr::include_graphics("../Tutorial_1_descriptive_stats/figures/Plot_for_subject6_PanisSchmidt.png")
```

 
  These results show that this participant is initially responding to the prime even though (s)he was instructed to only respond to the target, that response competition emerges in the incongruent prime condition around 300 ms, and that only later response are fully controlled by the target stimulus. Qualitatively similar results were obtained for the other five participants. Also, in their second Experiment, @panisWhatShapingRT2016 showed that the negative compatibility effect in the mask-present conditions is time-locked to mask onset. This example shows that a simple difference between two means fails to reveal the dynamic behavior people display in many experimental paradigms [@panisHowCanWe2020; @panisNeuropsychologicalEvidenceTemporal2017; @panisStudyingDynamicsVisual2020; @panisTimecourseContingenciesPerceptual2009; @panisWhenDoesInhibition2022; @schmidtResponseInhibitionNegative2022]. In other words, statistically controlling for the passage of time during data analysis is equally important as experimental control during the design of an experiment, to better understand human behavior in experimental paradigms. As we will show in Tutorials 2 and 3, statistical models for h(t) and ca(t) can each be implemented as generalized linear mixed regression models predicting event occurrence (1/0) and response accuracy (1/0) in each bin of a selected time range, respectively. 

# Tutorial 2: Fitting Bayesian hazard models

In this second tutorial we illustrate how to fit a Bayesian hazard regression model for the data set used in the first tutorial. 

model formula 
analytic decisions: specification of the effect of TIME, 

assumptions:
- linearity: continuous variables (TIME, trial number) 
- additivity:
- proportionality: 




# Tutorial 3: Fitting Frequentist hazard models

In this third tutorial we illustrate how to fit a frequentist hazard regression model for the data set used in the first tutorial. 


# Tutorial 4: Calculating descriptive statistics when there are two independent variables

In this final tutorial we illustrate how to calculate and plot the descriptive statistics for the full data set of Experiment 1 of @panisWhatShapingRT2016. 


# Discussion

## Individual differences

- role of response deadlines, low-level vs. higher-level processes, 
- clustering algorithms based on h(t) and ca(t) data
- 

## Cognitive psychophysiology and computational model selection

## Power analysis

- example repo on github

## Preregistration

- example preregistration for knot data

# Conclusions




\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
